{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "3650a1fe",
            "metadata": {},
            "source": [
                "# Spread Simulation with XAI Annotations (TFT + SEIR)\n",
                "This notebook implements a Temporal Fusion Transformer (TFT) model trained on COVID-19 data (OWID) enriched with SEIR model states and external factors.\n",
                "\n",
                "## Objectives\n",
                "1. **Data Preprocessing**: Load OWID data, clean, and engineer features.\n",
                "2. **SEIR Modeling**: Fit an Enhanced SEIR model to each country's data to generate features.\n",
                "3. **TFT Training**: Train using specific data splits (Primary, Rising 3rd Wave) and target variables (Active Cases or New Cases)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1dfdfe5a",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install pytorch-forecasting pytorch-lightning pandas numpy scipy matplotlib scikit-learn seaborn optuna"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "278ec26d",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import warnings\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from scipy.integrate import odeint\n",
                "from scipy.optimize import minimize\n",
                "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
                "import torch\n",
                "import pytorch_lightning as pl\n",
                "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
                "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline, GroupNormalizer\n",
                "from pytorch_forecasting.data import GroupNormalizer\n",
                "from pytorch_forecasting.metrics import MSE, QuantileLoss, MAE, SMAPE\n",
                "import optuna\n",
                "from optuna.integration import PyTorchLightningPruningCallback\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bf9dcfc8",
            "metadata": {},
            "source": [
                "## 1. Data Loading (Steps 2-4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8d802ec5",
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_and_preprocess_data():\n",
                "    paths = [\n",
                "        \"/content/drive/MyDrive/SEIR/Dataset/cleaned_owid-covid-data.csv\",\n",
                "        \"owid-covid-data.csv\",\n",
                "        \"https://covid.ourworldindata.org/data/owid-covid-data.csv\"\n",
                "    ]\n",
                "    df = None\n",
                "    for path in paths:\n",
                "        if os.path.exists(path) or path.startswith(\"http\"):\n",
                "            try:\n",
                "                print(f\"Attempting to load data from: {path}\")\n",
                "                df = pd.read_csv(path)\n",
                "                print(\"Success!\")\n",
                "                break\n",
                "            except Exception as e:\n",
                "                print(f\"Failed to load {path}: {e}\")\n",
                "    \n",
                "    if df is None:\n",
                "        raise FileNotFoundError(\"Could not load OWID dataset from any source.\")\n",
                "    \n",
                "    # Step 2: Column Selection\n",
                "    cols = ['location', 'date', 'new_cases', 'new_deaths', 'new_tests', \n",
                "            'positive_rate', 'stringency_index', \n",
                "            'aged_65_older', 'aged_70_older', 'diabetes_prevalence', 'cardiovasc_death_rate', \n",
                "            'population']\n",
                "    \n",
                "    # Filter to available columns\n",
                "    df = df[[c for c in cols if c in df.columns]].copy()\n",
                "    df['date'] = pd.to_datetime(df['date'])\n",
                "    \n",
                "    # Sort (Pre-requisite for rolling)\n",
                "    df = df.sort_values(['location', 'date']).reset_index(drop=True)\n",
                "    \n",
                "    # Step 3: SEIR State Estimation\n",
                "    # Use filled new_cases for calculation to handle NaNs safely during rolling\n",
                "    df['new_cases_filled'] = df.groupby('location')['new_cases'].apply(lambda x: x.fillna(0))\n",
                "    \n",
                "    # I: Rolling sum 14d / pop\n",
                "    df['rolling_14_cases'] = df.groupby('location')['new_cases_filled'].rolling(14, min_periods=1).sum().reset_index(0, drop=True)\n",
                "    df['I'] = df['rolling_14_cases'] / df['population']\n",
                "    \n",
                "    # R: (CumCases - 14d_sum) / pop\n",
                "    # Use cumsum of the filled new_cases\n",
                "    df['cumulative_cases'] = df.groupby('location')['new_cases_filled'].cumsum()\n",
                "    df['R'] = (df['cumulative_cases'] - df['rolling_14_cases']) / df['population']\n",
                "    \n",
                "    # S: 1 - R - I\n",
                "    df['S'] = 1 - df['R'] - df['I']\n",
                "    df['S'] = df['S'].clip(lower=0)\n",
                "    \n",
                "    # E: Rolling sum 5d * 2.5 / pop\n",
                "    df['rolling_5_cases'] = df.groupby('location')['new_cases_filled'].rolling(5, min_periods=1).sum().reset_index(0, drop=True)\n",
                "    df['E'] = (df['rolling_5_cases'] * 2.5) / df['population']\n",
                "    \n",
                "    # Verify SEIR Sum\n",
                "    seir_sum = df['S'] + df['E'] + df['I'] + df['R']\n",
                "    if (seir_sum - 1.0).abs().max() > 0.05:\n",
                "        print(\"WARNING: SEIR sum deviates by > 0.05 in some rows\")\n",
                "    \n",
                "    # Step 4: Feature Engineering\n",
                "    df['day_of_week'] = df['date'].dt.dayofweek\n",
                "    df['day_of_year'] = df['date'].dt.dayofyear\n",
                "    \n",
                "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
                "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
                "    df['day_of_year_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
                "    df['day_of_year_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
                "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
                "    \n",
                "    # Log transforms (log1p)\n",
                "    df['new_cases_log'] = np.log1p(df['new_cases'])\n",
                "    df['new_deaths_log'] = np.log1p(df['new_deaths'])\n",
                "    df['new_tests_log'] = np.log1p(df['new_tests'])\n",
                "    \n",
                "    # Cleanup temp columns\n",
                "    df.drop(columns=['new_cases_filled', 'rolling_14_cases', 'rolling_5_cases', 'cumulative_cases', 'day_of_week', 'day_of_year'], inplace=True)\n",
                "    \n",
                "    return df\n",
                "\n",
                "raw_df = load_and_preprocess_data()\n",
                "print(f\"Data Loaded & Feature Engineered: {raw_df.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eda_intro",
            "metadata": {},
            "source": [
                "## 1.5 Exploratory Data Analysis (EDA)\n",
                "Analyizing distributions, missing values, and trends before strict cleaning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eda_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Top 5 Rows:\")\n",
                "print(raw_df.head())\n",
                "\n",
                "# 1. Overview\n",
                "print(\"\\nDataset Info:\")\n",
                "print(raw_df.info())\n",
                "print(\"\\nSummary Statistics:\")\n",
                "print(raw_df.describe())\n",
                "\n",
                "# 2. Missing Values\n",
                "plt.figure(figsize=(12, 6))\n",
                "sns.heatmap(raw_df.isnull(), cbar=False, cmap='viridis')\n",
                "plt.title(\"Missing Values Heatmap\")\n",
                "plt.show()\n",
                "\n",
                "# 3. Distributions\n",
                "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
                "sns.histplot(raw_df['new_cases'], bins=50, ax=axes[0, 0], log_scale=(False, True))\n",
                "axes[0, 0].set_title(\"New Cases Distribution (Log Scale Y)\")\n",
                "\n",
                "if 'new_deaths' in raw_df.columns:\n",
                "    sns.histplot(raw_df['new_deaths'], bins=50, ax=axes[0, 1], log_scale=(False, True))\n",
                "    axes[0, 1].set_title(\"New Deaths Distribution (Log Scale Y)\")\n",
                "\n",
                "if 'positive_rate' in raw_df.columns:\n",
                "    sns.boxplot(x=raw_df['positive_rate'], ax=axes[1, 0])\n",
                "    axes[1, 0].set_title(\"Positive Rate Boxplot\")\n",
                "\n",
                "if 'stringency_index' in raw_df.columns:\n",
                "    sns.histplot(raw_df['stringency_index'], bins=50, ax=axes[1, 1])\n",
                "    axes[1, 1].set_title(\"Stringency Index Distribution\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# 4. Temporal Trends (Top 5 Locations)\n",
                "top_locs = raw_df.groupby('location')['new_cases'].sum().nlargest(5).index\n",
                "plt.figure(figsize=(15, 6))\n",
                "sns.lineplot(data=raw_df[raw_df['location'].isin(top_locs)], x='date', y='new_cases', hue='location')\n",
                "plt.title(\"New Cases over Time (Top 5 Locations)\")\n",
                "plt.show()\n",
                "\n",
                "# 5. Correlation Matrix\n",
                "numeric_df = raw_df.select_dtypes(include=[np.number])\n",
                "plt.figure(figsize=(12, 10))\n",
                "sns.heatmap(numeric_df.corr(), annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
                "plt.title(\"Feature Correlation Matrix\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "994626e2",
            "metadata": {},
            "source": [
                "## 2. Cleaning & Scaling (Steps 5-7)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "12d8ca28",
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_and_scale_data(df):\n",
                "    # Step 5: Data Cleaning\n",
                "    # 1. Sort\n",
                "    df = df.sort_values(['location', 'date']).reset_index(drop=True)\n",
                "    \n",
                "    # 2. Fill (Forward then Backward)\n",
                "    fill_cols = ['stringency_index', 'positive_rate', 'new_tests', 'new_deaths']\n",
                "    for col in fill_cols:\n",
                "        if col in df.columns:\n",
                "            df[col] = df.groupby('location')[col].ffill().bfill()\n",
                "            \n",
                "    # Fill new_cases NaNs with 0 (keep genuine 0s)\n",
                "    df['new_cases'] = df['new_cases'].fillna(0)\n",
                "    \n",
                "    # Re-compute logs after fill to be consistent\n",
                "    df['new_cases_log'] = np.log1p(df['new_cases'])\n",
                "    if 'new_deaths' in df.columns: df['new_deaths_log'] = np.log1p(df['new_deaths'])\n",
                "    if 'new_tests' in df.columns: df['new_tests_log'] = np.log1p(df['new_tests'])\n",
                "    \n",
                "    # 3. Drop locations < 60 rows\n",
                "    counts = df['location'].value_counts()\n",
                "    valid_locs = counts[counts >= 60].index\n",
                "    df = df[df['location'].isin(valid_locs)].copy()\n",
                "    \n",
                "    # 4. Drop null population\n",
                "    df = df.dropna(subset=['population'])\n",
                "    \n",
                "    # 5. Drop rows where new_cases_log is null\n",
                "    df = df.dropna(subset=['new_cases_log'])\n",
                "    \n",
                "    # 6. Clip positive_rate\n",
                "    if 'positive_rate' in df.columns:\n",
                "        df['positive_rate'] = df['positive_rate'].clip(0, 1)\n",
                "        \n",
                "    # 7. Clip SEIR\n",
                "    for col in ['S', 'E', 'I', 'R']:\n",
                "        df[col] = df[col].clip(0, 1)\n",
                "        \n",
                "    # Step 6: Static Covariate Encoding\n",
                "    static_vars = ['aged_65_older', 'aged_70_older', 'diabetes_prevalence', 'cardiovasc_death_rate']\n",
                "    # Extract first non-null\n",
                "    static_df = df.groupby('location')[static_vars].first().reset_index()\n",
                "    # Impute Global Median\n",
                "    for col in static_vars:\n",
                "        med = static_df[col].median()\n",
                "        static_df[col] = static_df[col].fillna(med)\n",
                "        \n",
                "    # Label Encode Location\n",
                "    le = LabelEncoder()\n",
                "    static_df['location_idx'] = le.fit_transform(static_df['location'])\n",
                "    location_map = dict(zip(le.classes_, le.transform(le.classes_)))\n",
                "    \n",
                "    # Scale Static (StandardScaler)\n",
                "    static_scaler = StandardScaler()\n",
                "    static_df[static_vars] = static_scaler.fit_transform(static_df[static_vars])\n",
                "    \n",
                "    # Merge back\n",
                "    df = df.drop(columns=static_vars)\n",
                "    df = df.merge(static_df, on='location', how='left')\n",
                "    \n",
                "    # Step 7: Temporal Scaling\n",
                "    temporal_vars = ['S', 'E', 'I', 'R', 'new_cases_log', 'new_deaths_log', 'new_tests_log', 'positive_rate', 'stringency_index']\n",
                "    temporal_scalers = {}\n",
                "    \n",
                "    # Training cutoff for fitting scalers (Rising 3rd Wave End)\n",
                "    TRAIN_CUTOFF = \"2021-12-31\"\n",
                "    train_mask = df['date'] <= pd.to_datetime(TRAIN_CUTOFF)\n",
                "    \n",
                "    for col in temporal_vars:\n",
                "        if col in df.columns:\n",
                "            scaler = StandardScaler()\n",
                "            train_subset = df.loc[train_mask, col].values.reshape(-1, 1)\n",
                "            scaler.fit(train_subset)\n",
                "            df[col] = scaler.transform(df[col].values.reshape(-1, 1)).flatten()\n",
                "            temporal_scalers[col] = scaler\n",
                "            \n",
                "    # Add time_idx for TFT\n",
                "    df['time_idx'] = df.groupby('location').cumcount()\n",
                "    \n",
                "    return df, temporal_scalers, static_scaler, location_map\n",
                "\n",
                "processed_df, temporal_scalers, static_scaler, loc_map = clean_and_scale_data(raw_df)\n",
                "print(f\"Processed Data Shape: {processed_df.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ef809abe",
            "metadata": {},
            "source": [
                "## 3. Creating TimeSeriesDataSet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7912ae66",
            "metadata": {},
            "outputs": [],
            "source": [
                "SPLIT_CONFIGS = {\n",
                "    \"Primary\": {\n",
                "        \"train_end\": \"2021-11-29\",\n",
                "        \"val_start\": \"2021-11-30\", \"val_end\": \"2021-12-14\",\n",
                "        \"test_start\": \"2021-12-15\", \"test_end\": \"2021-12-29\"\n",
                "    },\n",
                "    \"Rising 3rd Wave\": {\n",
                "        \"train_end\": \"2021-12-31\",\n",
                "        \"val_start\": \"2022-01-01\", \"val_end\": \"2022-01-15\",\n",
                "        \"test_start\": \"2022-01-16\", \"test_end\": \"2022-01-30\"\n",
                "    },\n",
                "    \"Falling 3rd Wave\": {\n",
                "        \"train_end\": \"2022-01-31\",\n",
                "        \"val_start\": \"2022-02-01\", \"val_end\": \"2022-02-15\",\n",
                "        \"test_start\": \"2022-02-16\", \"test_end\": \"2022-03-02\"\n",
                "    },\n",
                "    \"Post 3rd Wave\": {\n",
                "        \"train_end\": \"2022-02-28\",\n",
                "        \"val_start\": \"2022-03-01\", \"val_end\": \"2022-03-15\",\n",
                "        \"test_start\": \"2022-03-16\", \"test_end\": \"2022-03-30\"\n",
                "    }\n",
                "}\n",
                "\n",
                "def create_tft_dataset(df, split_name=\"Rising 3rd Wave\", target_variable=\"new_cases_log\", max_prediction_length=7, max_encoder_length=14):\n",
                "    print(f\"Creating Dataset for split: {split_name}\")\n",
                "    config = SPLIT_CONFIGS[split_name]\n",
                "    \n",
                "    train_end = pd.to_datetime(config['train_end'])\n",
                "    test_end = pd.to_datetime(config['test_end'])\n",
                "    \n",
                "    df_filtered = df[df['date'] <= test_end].copy()\n",
                "    \n",
                "    target_col = \"new_cases_log\"\n",
                "    print(f\"Target Variable: {target_col}\")\n",
                "\n",
                "    training_data = df_filtered[df_filtered['date'] <= train_end]\n",
                "    training_cutoff = training_data['time_idx'].max()\n",
                "    \n",
                "    print(f\"Training Cutoff Index: {training_cutoff} (Date: {train_end})\")\n",
                "\n",
                "    training = TimeSeriesDataSet(\n",
                "        df_filtered[lambda x: x.time_idx <= training_cutoff],\n",
                "        time_idx=\"time_idx\",\n",
                "        target=target_col,\n",
                "        group_ids=[\"location_idx\" if \"location_idx\" in df.columns else \"location\"],\n",
                "        min_encoder_length=max_encoder_length // 2,\n",
                "        max_encoder_length=max_encoder_length,\n",
                "        min_prediction_length=1,\n",
                "        max_prediction_length=max_prediction_length,\n",
                "        static_reals=[\"aged_65_older\", \"aged_70_older\", \"diabetes_prevalence\", \"cardiovasc_death_rate\"],\n",
                "        time_varying_known_reals=[\"time_idx\", \"day_of_week_sin\", \"day_of_week_cos\", \"day_of_year_sin\", \"day_of_year_cos\", \"is_weekend\", \"stringency_index\"],\n",
                "        time_varying_unknown_reals=[\n",
                "            target_col, \n",
                "            \"S\", \"E\", \"I\", \"R\", \"positive_rate\", \"new_deaths_log\", \"new_tests_log\"\n",
                "        ],\n",
                "        target_normalizer=None, # Already scaled\n",
                "        add_relative_time_idx=True,\n",
                "        add_target_scales=True,\n",
                "        add_encoder_length=True,\n",
                "    )\n",
                "\n",
                "    validation = TimeSeriesDataSet.from_dataset(\n",
                "        training, df_filtered, predict=True, stop_randomization=True\n",
                "    )\n",
                "    \n",
                "    batch_size = 64\n",
                "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
                "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 2, num_workers=0)\n",
                "    \n",
                "    return training, train_dataloader, val_dataloader\n",
                "\n",
                "training_dataset, train_loader, val_loader = create_tft_dataset(\n",
                "    processed_df, \n",
                "    split_name=\"Rising 3rd Wave\", \n",
                "    target_variable=\"new_cases_log\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "optuna_intro",
            "metadata": {},
            "source": [
                "## 4. Hyperparameter Tuning with Optuna\n",
                "Optimizing TFT hyperparameters (Hidden Size, Dropout, Attention Heads, Learning Rate) for both MSE and Quantile Loss."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "optuna_impl",
            "metadata": {},
            "outputs": [],
            "source": [
                "import optuna\n",
                "from optuna.integration import PyTorchLightningPruningCallback\n",
                "\n",
                "def optimize_tft(trial, loss_metric, train_dl, val_dl, max_epochs=10):\n",
                "    # Hyperparameters\n",
                "    gradient_clip_val = trial.suggest_float(\"gradient_clip_val\", 0.01, 1.0)\n",
                "    hidden_size = trial.suggest_categorical(\"hidden_size\", [16, 32, 64, 128])\n",
                "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
                "    hidden_continuous_size = trial.suggest_categorical(\"hidden_continuous_size\", [8, 16, 32])\n",
                "    attention_head_size = trial.suggest_categorical(\"attention_head_size\", [1, 2, 4])\n",
                "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
                "    \n",
                "    # Model\n",
                "    tft = TemporalFusionTransformer.from_dataset(\n",
                "        training_dataset,\n",
                "        learning_rate=learning_rate,\n",
                "        hidden_size=hidden_size,\n",
                "        attention_head_size=attention_head_size,\n",
                "        dropout=dropout,\n",
                "        hidden_continuous_size=hidden_continuous_size,\n",
                "        loss=loss_metric,\n",
                "        log_interval=10,\n",
                "        reduce_on_plateau_patience=4,\n",
                "    )\n",
                "    \n",
                "    # Trainer\n",
                "    trainer = pl.Trainer(\n",
                "        max_epochs=max_epochs,\n",
                "        accelerator=\"auto\",\n",
                "        gradient_clip_val=gradient_clip_val,\n",
                "        callbacks=[\n",
                "            EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=3, verbose=False, mode=\"min\"),\n",
                "            PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")\n",
                "        ],\n",
                "        enable_checkpointing=False, # Disable for tuning\n",
                "        logger=False\n",
                "    )\n",
                "    \n",
                "    trainer.fit(\n",
                "        tft,\n",
                "        train_dataloaders=train_dl,\n",
                "        val_dataloaders=val_dl,\n",
                "    )\n",
                "    \n",
                "    return trainer.callback_metrics[\"val_loss\"].item()\n",
                "\n",
                "# --- Study 1: MSE ---\n",
                "print(\"Starting Optuna Study for MSE Loss...\")\n",
                "study_mse = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
                "study_mse.optimize(lambda trial: optimize_tft(trial, MSE(), train_loader, val_loader), n_trials=10)\n",
                "\n",
                "print(\"Best MSE Params:\", study_mse.best_params)\n",
                "\n",
                "# Train Final MSE Model with Best Params\n",
                "best_params_mse = study_mse.best_params\n",
                "tft_mse = TemporalFusionTransformer.from_dataset(\n",
                "    training_dataset,\n",
                "    loss=MSE(),\n",
                "    **best_params_mse\n",
                ")\n",
                "trainer_mse = pl.Trainer(max_epochs=15, accelerator=\"auto\", gradient_clip_val=best_params_mse[\"gradient_clip_val\"])\n",
                "trainer_mse.fit(tft_mse, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
                "\n",
                "\n",
                "# --- Study 2: Quantile ---\n",
                "print(\"Starting Optuna Study for Quantile Loss...\")\n",
                "study_quantile = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
                "study_quantile.optimize(lambda trial: optimize_tft(trial, QuantileLoss([0.1, 0.5, 0.9]), train_loader, val_loader), n_trials=10)\n",
                "\n",
                "print(\"Best Quantile Params:\", study_quantile.best_params)\n",
                "\n",
                "# Train Final Quantile Model with Best Params\n",
                "best_params_quantile = study_quantile.best_params\n",
                "tft_quantile = TemporalFusionTransformer.from_dataset(\n",
                "    training_dataset,\n",
                "    loss=QuantileLoss([0.1, 0.5, 0.9]),\n",
                "    **best_params_quantile\n",
                ")\n",
                "trainer_quantile = pl.Trainer(max_epochs=15, accelerator=\"auto\", gradient_clip_val=best_params_quantile[\"gradient_clip_val\"])\n",
                "trainer_quantile.fit(tft_quantile, train_dataloaders=train_loader, val_dataloaders=val_loader)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "504a8486",
            "metadata": {},
            "outputs": [],
            "source": [
                "raw_predictions, x = tft_quantile.predict(val_loader, mode=\"raw\", return_x=True)\n",
                "for idx in range(3): \n",
                "    tft_quantile.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True)\n",
                "    plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}