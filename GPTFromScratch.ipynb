{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashish-Abraham/LLM-Boilerplate/blob/main/GPTFromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iie5bzjR4nGf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikKTs_JYtG2n"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3Mmcpske_K3v"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, attention_dim, bias=False, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.w_key = nn.Linear(embed_dim, attention_dim, bias=bias)\n",
        "        self.w_query = nn.Linear(embed_dim, attention_dim, bias=bias)\n",
        "        self.w_value = nn.Linear(embed_dim, attention_dim, bias=bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, _ = x.size()\n",
        "        \"\"\"\n",
        "        [\n",
        "            [write vector],\n",
        "            [me vector],\n",
        "            [a vector],\n",
        "            [poem vector],\n",
        "        ]\n",
        "        \"\"\"\n",
        "\n",
        "        k = self.w_key(x)   # (B, T, A)\n",
        "        q = self.w_query(x) # (B, T, A)\n",
        "        v = self.w_value(x) # (B, T, A)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = (q @ k.transpose(-2, -1)) / (k.size(-1) ** 0.5)  # (B, T, T)\n",
        "\n",
        "        # Causal mask (future positions masked)\n",
        "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
        "        scores = scores.masked_fill(mask, float('-1e10'))\n",
        "\n",
        "        attn = scores.softmax(dim=-1)  # (B, T, T)\n",
        "\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        return attn @ v  # (B, T, A)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iffYMgdXNc7j"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, embed_dim, attention_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.head_size = attention_dim//num_heads\n",
        "        self.heads = nn.ModuleList()\n",
        "        for i in range(num_heads):\n",
        "            self.heads.append(SelfAttention(embed_dim=embed_dim, attention_dim=self.head_size,dropout=dropout))\n",
        "\n",
        "    def forward(self,x):\n",
        "        head_outputs = []\n",
        "        for head in self.heads:\n",
        "            head_outputs.append(head(x)) #B x T x A//num_heads\n",
        "        concatenated = torch.cat(head_outputs, dim = 2)\n",
        "        return concatenated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9_jw1ZvcNWdn"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self,attention_dim):\n",
        "        super().__init__()\n",
        "        self.up = nn.Linear(attention_dim,attention_dim*4)\n",
        "        self.relu = nn.GELU()\n",
        "        self.down = nn.Linear(attention_dim*4,attention_dim)\n",
        "    def forward(self,x):\n",
        "        return self.down(self.relu(self.up(x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wZ5E25wAtEFA"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,num_heads,embed_dim,attention_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.masked_multihead = MultiHeadAttention(num_heads, embed_dim, attention_dim, dropout)\n",
        "        self.feed_forward = FeedForward(attention_dim)\n",
        "        self.n1 = nn.LayerNorm(attention_dim)\n",
        "        self.n2 = nn.LayerNorm(attention_dim)\n",
        "    def forward(self,x):\n",
        "        e = self.masked_multihead(self.n1(x))\n",
        "        e =  e + x\n",
        "        e = self.feed_forward(self.n2(e))\n",
        "        return e\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "K5Fz5TAVCCf6"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, num_heads, vocab_size, embed_dim, attention_dim, num_blocks, context_length, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, attention_dim)\n",
        "        self.positional_embedding = nn.Embedding(context_length, attention_dim)\n",
        "\n",
        "        self.decoders = nn.ModuleList([\n",
        "            Decoder(num_heads, attention_dim, attention_dim, dropout_rate) for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "        self.exit_norm = nn.LayerNorm(attention_dim)\n",
        "        self.linear = nn.Linear(attention_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context):\n",
        "        embeddings = self.embedding(context)\n",
        "        context_len = context.shape[1]\n",
        "        position = torch.arange(context_len, device=context.device).unsqueeze(0)\n",
        "        position_embeddings = self.positional_embedding(position)\n",
        "\n",
        "        e = embeddings + position_embeddings\n",
        "\n",
        "        for decoder in self.decoders:\n",
        "            e = decoder(e)\n",
        "\n",
        "        return self.linear(self.exit_norm(e))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1giUMFWXE70d"
      },
      "outputs": [],
      "source": [
        "int_to_char = {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '$', 5: '%', 6: '&', 7: \"'\", 8: '(', 9: ')', 10: '*', 11: '+', 12: ',', 13: '-', 14: '.', 15: '/', 16: '0', 17: '1', 18: '2', 19: '3', 20: '4', 21: '5', 22: '6', 23: '7', 24: '8', 25: '9', 26: ':', 27: ';', 28: '?', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '[', 56: ']', 57: '_', 58: 'a', 59: 'b', 60: 'c', 61: 'd', 62: 'e', 63: 'f', 64: 'g', 65: 'h', 66: 'i', 67: 'j', 68: 'k', 69: 'l', 70: 'm', 71: 'n', 72: 'o', 73: 'p', 74: 'q', 75: 'r', 76: 's', 77: 't', 78: 'u', 79: 'v', 80: 'w', 81: 'x', 82: 'y', 83: 'z', 84: '{', 85: '|', 86: '}', 87: 'à', 88: 'á', 89: 'è', 90: 'é', 91: 'ë', 92: 'ñ', 93: 'ó', 94: 'ú', 95: '\\u2005', 96: '–', 97: '—', 98: '‘', 99: '’', 100: '“', 101: '”', 102: '…', 103: '\\u205f'}\n",
        "\n",
        "def top_k_logits(logits, k):\n",
        "    v, ix = torch.topk(logits, k)\n",
        "    out = logits.clone()\n",
        "    out[out < v[:, [-1]]] = float('-inf')\n",
        "    return out\n",
        "\n",
        "\n",
        "def generate_text(model, new_chars, context, context_length, int_to_char, temperature=1.0, top_k=None):\n",
        "    res = []\n",
        "    for _ in range(new_chars):\n",
        "        if context.shape[1] > context_length:\n",
        "            context = context[:, -context_length:]\n",
        "\n",
        "        logits = model(context)  # [B, T, V]\n",
        "        logits = logits[:, -1, :]  # [B, V]\n",
        "        logits = logits / max(temperature, 1e-3)\n",
        "\n",
        "        if top_k is not None:\n",
        "            logits = top_k_logits(logits, top_k)\n",
        "\n",
        "        if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
        "            raise ValueError(\"Logits contain NaN or Inf\")\n",
        "\n",
        "        probabilities = nn.functional.softmax(logits, dim=-1)\n",
        "        probabilities = torch.clamp(probabilities, min=1e-9, max=1.0)\n",
        "\n",
        "        next_token = torch.multinomial(probabilities, 1)  # [B, 1]\n",
        "        context = torch.cat((context, next_token), dim=1)\n",
        "        res.append(int_to_char[next_token.item()])\n",
        "\n",
        "    return ''.join(res)\n",
        "\n",
        "def generate(model, max_new_tokens, context, context_length, temperature=1.0, top_k=None):\n",
        "    res = []\n",
        "    for _ in range(max_new_tokens):\n",
        "        if context.shape[1] > context_length:\n",
        "            context = context[:, -context_length:]\n",
        "\n",
        "        logits = model(context)  # [B, T, V]\n",
        "        logits = logits[:, -1, :]  # [B, V]\n",
        "        logits = logits / max(temperature, 1e-3)\n",
        "\n",
        "        if top_k is not None:\n",
        "            logits = top_k_logits(logits, top_k)\n",
        "\n",
        "        if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
        "            raise ValueError(\"Logits contain NaN or Inf\")\n",
        "\n",
        "        probabilities = nn.functional.softmax(logits, dim=-1)\n",
        "        probabilities = torch.clamp(probabilities, min=1e-9, max=1.0)\n",
        "\n",
        "        next_token = torch.multinomial(probabilities, 1)  # [B, 1]\n",
        "        context = torch.cat((context, next_token), dim=1)\n",
        "\n",
        "\n",
        "    return context\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-9nUyMbuY02D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT2-124M\n",
        "\n",
        "| Parameter      | Value   | Description                 |\n",
        "|----------------|---------|-----------------------------|\n",
        "| `vocab_size`   | 50257   | Vocabulary size             |\n",
        "| `context_length`| 1024    | Context length              |\n",
        "| `emb_dim`      | 768     | Embedding dimension         |\n",
        "| `n_heads`      | 12      | Number of attention heads   |\n",
        "| `n_layers`     | 12      | Number of layers            |\n",
        "| `drop_rate`    | 0.1     | Dropout rate                |\n",
        "| `qkv_bias`     | False   | Query-Key-Value bias        |"
      ],
      "metadata": {
        "id": "DShjAfTWa2OI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28KmT9qhQ1xs"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD7LpRatLZYN",
        "outputId": "cd86a848-ad51-4bf3-bc57-6e6bcc516041"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (embedding): Embedding(104, 256)\n",
              "  (positional_embedding): Embedding(128, 256)\n",
              "  (decoders): ModuleList(\n",
              "    (0-5): 6 x Decoder(\n",
              "      (masked_multihead): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x SelfAttention(\n",
              "            (w_key): Linear(in_features=256, out_features=64, bias=False)\n",
              "            (w_query): Linear(in_features=256, out_features=64, bias=False)\n",
              "            (w_value): Linear(in_features=256, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (up): Linear(in_features=256, out_features=1024, bias=True)\n",
              "        (relu): GELU(approximate='none')\n",
              "        (down): Linear(in_features=1024, out_features=256, bias=True)\n",
              "      )\n",
              "      (n1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (n2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (exit_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "  (linear): Linear(in_features=256, out_features=104, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "vocab_size = 104             # Number of tokens in your vocabulary\n",
        "context_length = 128         # Maximum sequence length (tokens)\n",
        "embed_dim = 256              # Token embedding size\n",
        "attention_dim = 256          # Attention projection dimension (keep same as embed_dim)\n",
        "num_heads = 4                # Number of attention heads (must divide attention_dim)\n",
        "num_blocks = 6               # Number of decoder blocks (layers)\n",
        "num_words = 5000\n",
        "dropout_rate=0.1           # Number of new tokens to generate (optional tuning)\n",
        "\n",
        "# Initial context (starting token or BOS)\n",
        "context = torch.zeros(1, 1, dtype=torch.int64).to(device)\n",
        "\n",
        "model = GPT(num_heads,vocab_size,embed_dim,attention_dim,num_blocks,context_length,dropout_rate).to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HoKQt7J-MS79"
      },
      "outputs": [],
      "source": [
        "# generate_text(model,num_words,context,context_length,int_to_char)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgkjmgHPYKyG"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Nx4FwjcNZMfF",
        "outputId": "f5e19867-7d10-4460-bc44-5b919b0a8e03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vU2jpH6JYMTl",
        "outputId": "d5312443-b251-4bb0-eb54-45a6f58411f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  100k  100  100k    0     0   112k      0 --:--:-- --:--:-- --:--:--  112k\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   row_idx                                                row truncated_cells\n",
              "0        0  {'text': 'Ive been reading books of old\n",
              "The le...              []\n",
              "1        1  {'text': 'Come up to meet you, tell you Im sor...              []\n",
              "2        2  {'text': 'I used to rule the world\n",
              "Seas would ...              []\n",
              "3        3  {'text': 'When you try your best, but you dont...              []\n",
              "4        4  {'text': 'Look at the stars\n",
              "Look how they shin...              []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e617745b-9e99-4c8b-aabd-e0da7fb13ea6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_idx</th>\n",
              "      <th>row</th>\n",
              "      <th>truncated_cells</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>{'text': 'Ive been reading books of old\n",
              "The le...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>{'text': 'Come up to meet you, tell you Im sor...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>{'text': 'I used to rule the world\n",
              "Seas would ...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>{'text': 'When you try your best, but you dont...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>{'text': 'Look at the stars\n",
              "Look how they shin...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e617745b-9e99-4c8b-aabd-e0da7fb13ea6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e617745b-9e99-4c8b-aabd-e0da7fb13ea6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e617745b-9e99-4c8b-aabd-e0da7fb13ea6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-31ad64f6-be9d-41d3-a0a1-cfbdc017ef08\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-31ad64f6-be9d-41d3-a0a1-cfbdc017ef08')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-31ad64f6-be9d-41d3-a0a1-cfbdc017ef08 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"row_idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"row\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"truncated_cells\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!curl -X GET \"https://datasets-server.huggingface.co/rows?dataset=huggingartists%2Fcoldplay&config=default&split=train&offset=0&length=100\" -o coldplay_data.json\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open('coldplay_data.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "df = pd.DataFrame(data['rows'])\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['truncated_cells'])\n",
        "df['text'] = df['row'].apply(lambda x: x['text'])\n",
        "df = df.drop(columns=['row'])\n",
        "display(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "A-zKpLgpcOFR",
        "outputId": "bf0a221d-948e-4237-df57-ec85b57fadf5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   row_idx                                               text\n",
              "0        0  Ive been reading books of old\\nThe legends and...\n",
              "1        1  Come up to meet you, tell you Im sorry\\nYou do...\n",
              "2        2  I used to rule the world\\nSeas would rise when...\n",
              "3        3  When you try your best, but you dont succeed\\n...\n",
              "4        4  Look at the stars\\nLook how they shine for you..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-83036dd5-c08f-456a-a116-4b6aef392926\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_idx</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Ive been reading books of old\\nThe legends and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Come up to meet you, tell you Im sorry\\nYou do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>I used to rule the world\\nSeas would rise when...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>When you try your best, but you dont succeed\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Look at the stars\\nLook how they shine for you...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83036dd5-c08f-456a-a116-4b6aef392926')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-83036dd5-c08f-456a-a116-4b6aef392926 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-83036dd5-c08f-456a-a116-4b6aef392926');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3409476c-34b9-435d-9850-0871ab6b8431\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3409476c-34b9-435d-9850-0871ab6b8431')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3409476c-34b9-435d-9850-0871ab6b8431 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"row_idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Come up to meet you, tell you Im sorry\\nYou dont know how lovely you are\\nI had to find you, tell you I need you\\nTell you I set you apart\\nTell me your secrets, and ask me your questions\\nOh, lets go back to the start\\nRunning in circles, coming up tails\\nHeads on a science apart\\nNobody said it was easy\\nIts such a shame for us to part\\nNobody said it was easy\\nNo one ever said it would be this hard\\nOh, take me back to the start\\nI was just guessing at numbers and figures\\nPulling the puzzles apart\\nQuestions of science, science and progress\\nDo not speak as loud as my heart\\nTell me you love me, come back and haunt me\\nOh, and I rush to the start\\nRunning in circles, chasing our tails\\nComing back as we are\\nNobody said it was easy\\nOh, its such a shame for us to part\\nNobody said it was easy\\nNo one ever said it would be so hard\\nIm going back to the start\\nOh-ooh ooh-ooh-ooh-ooh\\nAh-ooh ooh-ooh-ooh-ooh\\nOh-ooh ooh-ooh-ooh-ooh\\nOh-ooh ooh-ooh-ooh-ooh\",\n          \"Look at the stars\\nLook how they shine for you\\nAnd everything you do\\nYeah, they were all yellow\\nI came along\\nI wrote a song for you\\nAnd all the things you do\\nAnd it was called Yellow\\nSo then I took my turn\\nOh, what a thing to have done\\nAnd it was all yellow\\n Your skin, oh yeah, your skin and bones\\n Turn into something beautiful\\n You know, you know I love you so\\nYou know I love you so\\nI swam across\\nI jumped across for you\\nOh, what a thing to do\\nCause you were all yellow\\nI drew a line\\nI drew a line for you\\nOh, what a thing to do\\nAnd it was all yellow\\n Your skin, oh yeah, your skin and bones\\n Turn into something beautiful\\n And you know\\nFor you, Id bleed myself dry\\nFor you, Id bleed myself dry\\nIts true, look how they shine for you\\nLook how they shine for you\\nLook how they shine for\\nLook how they shine for you\\nLook how they shine for you\\nLook how they shine\\nLook at the stars\\nLook how they shine for you\\nAnd all the things that you do\",\n          \"I used to rule the world\\nSeas would rise when I gave the word\\nNow in the morning, I sleep alone\\nSweep the streets I used to own\\nI used to roll the dice\\nFeel the fear in my enemys eyes\\nListen as the crowd would sing\\nNow the old King is dead, long live the King\\nOne minute I held the key\\nNext the walls were closed on me\\nAnd I discovered that my castles stand\\nUpon pillars of salt and pillars of sand\\nI hear Jerusalem bells are ringing\\nRoman Cavalry choirs are singing\\nBe my mirror, my sword, and shield\\nMy missionaries in a foreign field\\nFor some reason I cant explain\\nOnce youd gone, there was never\\nNever an honest word\\nAnd that was when I ruled the world\\nIt was the wicked and wild wind\\nBlew down the doors to let me in\\nShattered windows and the sound of drums\\nPeople couldnt believe what Id become\\nRevolutionaries wait\\nFor my head on a silver plate\\nJust a puppet on a lonely string\\nAw, who would ever want to be king?\\nI hear Jerusalem bells are ringing\\nRoman Cavalry choirs are singing\\nBe my mirror, my sword, and shield\\nMy missionaries in a foreign field\\nFor some reason I cant explain\\nI know Saint Peter wont call my name\\nNever an honest word\\nBut that was when I ruled the world\\nOh-oh-woah, oh-oh, oh\\nOh-oh-woah, oh-oh, oh\\nOh-oh-woah, oh-oh, oh\\nOh-oh-woah, oh-oh, oh\\nI hear Jerusalem bells are ringing\\nRoman Cavalry choirs are singing\\nBe my mirror, my sword and shield\\nMy missionaries in a foreign field\\nFor some reason I cant explain\\nI know Saint Peter wont call my name\\nNever an honest word\\nBut that was when I ruled the world\\nMmm, mmm, mmm, mmm\\nMmm, mmm, mmm, mmm\\nMmm, mmm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "y14KQe6heAOb"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "# Select subsets\n",
        "train_subset = df.iloc[:90]\n",
        "test_subset = df.iloc[90:]\n",
        "\n",
        "# Extract and clean lyrics\n",
        "def keep_english_only(text):\n",
        "    return re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
        "\n",
        "# Process training lyrics\n",
        "train_lyrics = [keep_english_only(row[\"text\"]) for index, row in train_subset.iterrows()]\n",
        "joined_train_lyrics = '\\n'.join(train_lyrics)\n",
        "\n",
        "# Process test lyrics\n",
        "test_lyrics = [keep_english_only(row[\"text\"]) for index, row in test_subset.iterrows()]\n",
        "joined_test_lyrics = '\\n'.join(test_lyrics)\n",
        "\n",
        "full_lyrics = joined_train_lyrics + '\\n' + joined_test_lyrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_lyrics[:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "UQKV01oDjE4W",
        "outputId": "b31d2f4c-1bf6-4147-fb4c-b05aaa82f9ca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ive been reading books of old\\nThe legends and the myths\\nAchilles and his gold\\nHercules and his gifts\\nSpider-Mans control\\nAnd Batman with his fists\\nAnd clearly I dont see myself upon that list\\nBut she '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "j04d2k5nbYIe",
        "outputId": "6494699d-cffd-45d2-9ac3-941c91646766",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "characters = list(set(full_lyrics))\n",
        "len(characters)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Tokenizer"
      ],
      "metadata": {
        "id": "vyN8Ick5dNcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "# Alternatively:\n",
        "# from llms_from_scratch.ch04 import generate_text_simple\n",
        "\n",
        "def text_to_token_ids(text, tokenizer, device):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0).to(device) # add batch dimension and move to device\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "\n",
        "start_context = \"I want something\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "FfXxF-M0eC7h"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.n_vocab           # GPT-2 tokenizer vocab size\n",
        "context_length = 256                     # Reduced context length for faster training\n",
        "embed_dim = 256                          # Smaller embedding dimension\n",
        "attention_dim = 256                      # Keep same as embed_dim\n",
        "num_heads = 4                           # Divisible by attention_dim\n",
        "num_blocks = 4                          # Fewer blocks for smaller model\n",
        "dropout_rate = 0.25\n",
        "\n",
        "# Initial context (starting token or BOS)\n",
        "context = torch.zeros(1, 1, dtype=torch.int64).to(device)\n",
        "\n",
        "model = GPT(num_heads,vocab_size,embed_dim,attention_dim,num_blocks,context_length, dropout_rate).to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJ0RN5ojd5yb",
        "outputId": "07467403-b53f-45e5-fe97-992b728e2225"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (embedding): Embedding(50257, 256)\n",
              "  (positional_embedding): Embedding(256, 256)\n",
              "  (decoders): ModuleList(\n",
              "    (0-3): 4 x Decoder(\n",
              "      (masked_multihead): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x SelfAttention(\n",
              "            (w_key): Linear(in_features=256, out_features=64, bias=False)\n",
              "            (w_query): Linear(in_features=256, out_features=64, bias=False)\n",
              "            (w_value): Linear(in_features=256, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (up): Linear(in_features=256, out_features=1024, bias=True)\n",
              "        (relu): GELU(approximate='none')\n",
              "        (down): Linear(in_features=1024, out_features=256, bias=True)\n",
              "      )\n",
              "      (n1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (n2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (exit_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "  (linear): Linear(in_features=256, out_features=50257, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = generate(\n",
        "    model=model,\n",
        "    context=text_to_token_ids(start_context, tokenizer, device),\n",
        "    max_new_tokens=10,\n",
        "    context_length=context_length\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGMUD_qDdRCD",
        "outputId": "f2d263ba-1075-442a-c3ab-6ce5986d03ad"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " I want something introduceウ coaches Kard Judaism trendsCommerce rotating infiltration approach\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretraining"
      ],
      "metadata": {
        "id": "e2cBhls87O5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import re\n",
        "\n",
        "# Load dataset\n",
        "ds = load_dataset(\"stanfordnlp/imdb\")\n",
        "\n",
        "# Function to keep only English (ASCII) characters\n",
        "def keep_english_only(text):\n",
        "    return re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
        "\n",
        "# Function to clean and combine a list of texts\n",
        "def combine_and_clean(text_list):\n",
        "    # Keep only English\n",
        "    cleaned_list = [keep_english_only(t) for t in text_list]\n",
        "    # Combine into one string\n",
        "    combined = \" \".join(cleaned_list)\n",
        "    # Remove extra spaces/newlines\n",
        "    combined = re.sub(r'\\s+', ' ', combined).strip()\n",
        "    return combined\n",
        "\n",
        "# Create separate combined strings\n",
        "train_text_data = combine_and_clean(ds['train']['text'])\n",
        "test_text_data = combine_and_clean(ds['test']['text'])\n",
        "\n",
        "print(f\"Train text length: {len(train_text_data)} characters\")\n",
        "print(f\"Test text length: {len(test_text_data)} characters\")\n",
        "\n",
        "# Preview first 300 chars from each\n",
        "print(\"\\nTrain preview:\", train_text_data[:300])\n",
        "print(\"\\nTest preview:\", test_text_data[:300])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "V5iQMULW7Sab",
        "outputId": "ccaee834-ccfe-4d84-b2de-868c1047d391"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-365053562.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Create separate combined strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtrain_text_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_and_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtest_text_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_and_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train text length: {len(train_text_data)} characters\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-365053562.py\u001b[0m in \u001b[0;36mcombine_and_clean\u001b[0;34m(text_list)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Function to clean and combine a list of texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcombine_and_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\s+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcombined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "\n",
        "        # Use a sliding window to chunk the data into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_encoded_dataloader(txt, tokenizer, batch_size=4, max_length=128,\n",
        "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = CustomDataset(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "UI-gPEUWixDy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_characters = len(train_text_data)\n",
        "total_tokens = len(tokenizer.encode(train_text_data))\n",
        "\n",
        "print(\"Characters:\", total_characters)\n",
        "print(\"Tokens:\", total_tokens)\n",
        "\n",
        "# Sanity check\n",
        "\n",
        "if total_tokens * (0.95) < context_length:\n",
        "    print(\"Not enough tokens for the training loader. \"\n",
        "          \"Try to lower the context_length or \"\n",
        "          \"increase the `training_ratio`\")\n",
        "\n",
        "if total_tokens * (1-0.95) <context_length:\n",
        "    print(\"Not enough tokens for the validation loader. \"\n",
        "          \"Try to lower the context_length or \"\n",
        "          \"decrease the `training_ratio`\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQCa0ecplA0I",
        "outputId": "80315c87-f645-4ceb-8d79-1d8e1b5c7966"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters: 33151242\n",
            "Tokens: 7488757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create training and validation dataloaders\n",
        "train_dataloader = create_encoded_dataloader(\n",
        "    train_text_data,\n",
        "    tokenizer=tokenizer,\n",
        "    batch_size=2,\n",
        "    max_length=context_length,\n",
        "    stride=context_length,\n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "test_dataloader = create_encoded_dataloader(\n",
        "    test_text_data,\n",
        "    tokenizer=tokenizer,\n",
        "    batch_size=2,\n",
        "    max_length=context_length,\n",
        "    stride=context_length,\n",
        "    shuffle=False,\n",
        "    drop_last=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "dCBbgD7-aVUg"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of batches in training dataloader: {len(train_dataloader)}\")\n",
        "print(f\"Number of batches in validation dataloader: {len(test_dataloader)}\")\n",
        "print(\"Train loader:\")\n",
        "for x, y in train_dataloader:\n",
        "    print(x.shape, y.shape)\n",
        "    break\n",
        "\n",
        "print(\"\\ntest loader:\")\n",
        "for x, y in test_dataloader:\n",
        "    print(x.shape, y.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0s-lwq_omPG7",
        "outputId": "0e77b1ac-4a2e-4563-d188-251d534755fd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of batches in training dataloader: 384\n",
            "Number of batches in validation dataloader: 54\n",
            "Train loader:\n",
            "torch.Size([2, 64]) torch.Size([2, 64])\n",
            "\n",
            "test loader:\n",
            "torch.Size([2, 64]) torch.Size([2, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokens = 0\n",
        "for input_batch, target_batch in train_dataloader:\n",
        "    train_tokens += input_batch.numel()\n",
        "\n",
        "val_tokens = 0\n",
        "for input_batch, target_batch in test_dataloader:\n",
        "    val_tokens += input_batch.numel()\n",
        "\n",
        "print(\"Training tokens:\", train_tokens)\n",
        "print(\"Validation tokens:\", val_tokens)\n",
        "print(\"All tokens:\", train_tokens + val_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2o8IJ5wpoDbv",
        "outputId": "cb2e2712-3c43-4485-e485-ee7d137802ce"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training tokens: 29184\n",
            "Validation tokens: 2816\n",
            "All tokens: 32000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "settings = {\n",
        "    \"learning_rate\": 8e-5,              # Lower starting LR\n",
        "    \"weight_decay\": 0.008,\n",
        "    \"num_epochs\": 150,                  # More epochs with lower LR\n",
        "    \"batch_size\": 8,                   # Slightly smaller batch size\n",
        "    \"warmup_steps\": 800,                # Shorter warmup\n",
        "    \"max_lr\": 1.5e-4,                     # Much lower peak learning rate\n",
        "    \"min_lr\": 5e-7,                     # Lower minimum learning rate\n",
        "    \"eval_freq\": 25,                    # Less frequent evaluation\n",
        "    \"eval_iter\": 8,                     # Fewer batches for evaluation\n",
        "    \"gradient_clip\": 0.8,               # Lower gradient clipping\n",
        "    \"patience\": 25,                     # More patience\n",
        "    \"min_improvement\": 2e-4             # Minimum improvement threshold\n",
        "}\n",
        "\n",
        "def initialize_weights(module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        if module.bias is not None:\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "    elif isinstance(module, nn.LayerNorm):\n",
        "        torch.nn.init.ones_(module.weight)\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "# Apply initialization\n",
        "model.apply(initialize_weights)"
      ],
      "metadata": {
        "id": "Ibn6iWWnp6rH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ace5402-ef93-4265-cbe6-c43a6c1074a2"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (embedding): Embedding(50257, 256)\n",
              "  (positional_embedding): Embedding(256, 256)\n",
              "  (decoders): ModuleList(\n",
              "    (0-3): 4 x Decoder(\n",
              "      (masked_multihead): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x SelfAttention(\n",
              "            (w_key): Linear(in_features=256, out_features=64, bias=False)\n",
              "            (w_query): Linear(in_features=256, out_features=64, bias=False)\n",
              "            (w_value): Linear(in_features=256, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (up): Linear(in_features=256, out_features=1024, bias=True)\n",
              "        (relu): GELU(approximate='none')\n",
              "        (down): Linear(in_features=1024, out_features=256, bias=True)\n",
              "      )\n",
              "      (n1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (n2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (exit_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "  (linear): Linear(in_features=256, out_features=50257, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, last_epoch=-1):\n",
        "    \"\"\"\n",
        "    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n",
        "    initial lr set in the optimizer to 0, after a warmup period during which it increases from 0 to the initial lr.\n",
        "    \"\"\"\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
        "\n",
        "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
        "\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    # Reshape logits and targets for CrossEntropyLoss\n",
        "    loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), target_batch.view(-1))\n",
        "    return loss\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations during evaluation\n",
        "        for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "            if i < num_batches:\n",
        "                loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "                total_loss += loss.item()\n",
        "            else:\n",
        "                break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context, context_length, num_chars=100):\n",
        "    model.eval()\n",
        "    # Convert start_context to token IDs using the provided function\n",
        "    context_tensor = text_to_token_ids(start_context, tokenizer, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Generate text using the generate function\n",
        "        generated_tokens = generate(model, num_chars, context_tensor, context_length=context_length)\n",
        "        # Decode tokens to text using the provided function\n",
        "        decoded_text = token_ids_to_text(generated_tokens, tokenizer)\n",
        "\n",
        "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, device, settings, tokenizer, context_length):\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=settings[\"learning_rate\"],\n",
        "        weight_decay=settings[\"weight_decay\"],\n",
        "        betas=(0.9, 0.95),\n",
        "        eps=1e-8\n",
        "    )\n",
        "\n",
        "    # Calculate total training steps\n",
        "    total_steps = len(train_loader) * settings[\"num_epochs\"]\n",
        "\n",
        "    # Initialize scheduler with warmup\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=settings[\"warmup_steps\"],\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Training tracking\n",
        "    train_losses, val_losses = [], []\n",
        "    epochs_seen, tokens_seen = [], []\n",
        "    learning_rates = []\n",
        "    total_tokens = 0\n",
        "    global_step = 0\n",
        "\n",
        "    # Improved early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    val_loss_history = []\n",
        "\n",
        "    print(f\"Starting training for {settings['num_epochs']} epochs\")\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"Total training steps: {total_steps}\")\n",
        "    print(f\"Warmup steps: {settings['warmup_steps']}\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(settings[\"num_epochs\"]):\n",
        "        epoch_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for i, (input_batch, target_batch) in enumerate(train_loader):\n",
        "            global_step += 1\n",
        "            total_tokens += input_batch.numel()\n",
        "\n",
        "            # Forward pass\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "\n",
        "            # Check for NaN/Inf\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                print(f\"WARNING: Invalid loss detected at step {global_step}\")\n",
        "                continue\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), settings[\"gradient_clip\"])\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # Log current learning rate\n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "            learning_rates.append(current_lr)\n",
        "\n",
        "            # Evaluation and logging\n",
        "            if global_step % settings[\"eval_freq\"] == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, settings[\"eval_iter\"]\n",
        "                )\n",
        "\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                epochs_seen.append(epoch + (i + 1) / len(train_loader))\n",
        "                tokens_seen.append(total_tokens)\n",
        "                val_loss_history.append(val_loss)\n",
        "\n",
        "                print(f\"Epoch {epoch+1}/{settings['num_epochs']}, Step {global_step}: \"\n",
        "                      f\"Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, \"\n",
        "                      f\"LR={current_lr:.2e}, Tokens={total_tokens:,}, GradNorm={grad_norm:.3f}\")\n",
        "\n",
        "                # Improved early stopping logic\n",
        "                improvement = best_val_loss - val_loss\n",
        "\n",
        "                if improvement > settings.get(\"min_improvement\", 1e-4):\n",
        "                    best_val_loss = val_loss\n",
        "                    patience_counter = 0\n",
        "                    # Save best model\n",
        "                    torch.save({\n",
        "                        'epoch': epoch,\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'optimizer_state_dict': optimizer.state_dict(),\n",
        "                        'scheduler_state_dict': scheduler.state_dict(),\n",
        "                        'best_val_loss': best_val_loss,\n",
        "                        'global_step': global_step,\n",
        "                        'settings': settings\n",
        "                    }, 'best_model_improved.pth')\n",
        "                    print(f\"New best model saved! Val Loss: {val_loss:.4f} (improvement: {improvement:.4f})\")\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "\n",
        "                # Check for plateau - if loss hasn't improved significantly in last 10 evaluations\n",
        "                if len(val_loss_history) >= 10:\n",
        "                    recent_losses = val_loss_history[-10:]\n",
        "                    if max(recent_losses) - min(recent_losses) < 0.01:  # Very small variation\n",
        "                        print(f\"Loss plateaued. Recent variation: {max(recent_losses) - min(recent_losses):.4f}\")\n",
        "                        patience_counter += 5  # Accelerate early stopping\n",
        "\n",
        "                if patience_counter >= settings[\"patience\"]:\n",
        "                    print(f\"Early stopping triggered at epoch {epoch+1}, step {global_step}\")\n",
        "                    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "                    return {\n",
        "                        'train_losses': train_losses,\n",
        "                        'val_losses': val_losses,\n",
        "                        'epochs_seen': epochs_seen,\n",
        "                        'tokens_seen': tokens_seen,\n",
        "                        'learning_rates': learning_rates,\n",
        "                        'best_val_loss': best_val_loss\n",
        "                    }\n",
        "\n",
        "        # End of epoch logging\n",
        "        avg_epoch_loss = epoch_loss / num_batches if num_batches > 0 else float('inf')\n",
        "        print(f\"Epoch {epoch+1} completed. Average loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "        # Generate sample text less frequently\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Sample generation after epoch {epoch+1}:\")\n",
        "            generate_and_print_sample(\n",
        "                model, tokenizer, device, \"This movie\", context_length, num_chars=80\n",
        "            )\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "    print(f\"Training completed! Best validation loss: {best_val_loss:.4f}\")\n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'epochs_seen': epochs_seen,\n",
        "        'tokens_seen': tokens_seen,\n",
        "        'learning_rates': learning_rates,\n",
        "        'best_val_loss': best_val_loss\n",
        "    }"
      ],
      "metadata": {
        "id": "t1YEkbIhotbl"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_results = train_model(\n",
        "    model, train_dataloader, test_dataloader, device, settings, tokenizer, context_length\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jGNSQVWrIMd",
        "outputId": "0e9fdacf-6a9f-44d8-d2da-e1e3d08aaf88"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 150 epochs\n",
            "Model parameters: 28,740,689\n",
            "Total training steps: 57600\n",
            "Warmup steps: 800\n",
            "Epoch 1/150, Step 25: Train Loss=10.7836, Val Loss=10.7437, LR=2.50e-06, Tokens=3,200, GradNorm=8.476\n",
            "New best model saved! Val Loss: 10.7437 (improvement: inf)\n",
            "Epoch 1/150, Step 50: Train Loss=10.5780, Val Loss=10.5188, LR=5.00e-06, Tokens=6,400, GradNorm=6.544\n",
            "New best model saved! Val Loss: 10.5188 (improvement: 0.2249)\n",
            "Epoch 1/150, Step 75: Train Loss=10.4161, Val Loss=10.3212, LR=7.50e-06, Tokens=9,600, GradNorm=5.135\n",
            "New best model saved! Val Loss: 10.3212 (improvement: 0.1975)\n",
            "Epoch 1/150, Step 100: Train Loss=10.2632, Val Loss=10.1239, LR=1.00e-05, Tokens=12,800, GradNorm=3.930\n",
            "New best model saved! Val Loss: 10.1239 (improvement: 0.1973)\n",
            "Epoch 1/150, Step 125: Train Loss=10.1105, Val Loss=9.9936, LR=1.25e-05, Tokens=16,000, GradNorm=4.230\n",
            "New best model saved! Val Loss: 9.9936 (improvement: 0.1303)\n",
            "Epoch 1/150, Step 150: Train Loss=10.0195, Val Loss=9.8624, LR=1.50e-05, Tokens=19,200, GradNorm=3.582\n",
            "New best model saved! Val Loss: 9.8624 (improvement: 0.1313)\n",
            "Epoch 1/150, Step 175: Train Loss=9.9241, Val Loss=9.7541, LR=1.75e-05, Tokens=22,400, GradNorm=3.718\n",
            "New best model saved! Val Loss: 9.7541 (improvement: 0.1083)\n",
            "Epoch 1/150, Step 200: Train Loss=9.6096, Val Loss=9.6215, LR=2.00e-05, Tokens=25,600, GradNorm=3.617\n",
            "New best model saved! Val Loss: 9.6215 (improvement: 0.1326)\n",
            "Epoch 1/150, Step 225: Train Loss=9.5454, Val Loss=9.4554, LR=2.25e-05, Tokens=28,800, GradNorm=3.165\n",
            "New best model saved! Val Loss: 9.4554 (improvement: 0.1662)\n",
            "Epoch 1/150, Step 250: Train Loss=9.4221, Val Loss=9.2805, LR=2.50e-05, Tokens=32,000, GradNorm=3.088\n",
            "New best model saved! Val Loss: 9.2805 (improvement: 0.1749)\n",
            "Epoch 1/150, Step 275: Train Loss=9.1809, Val Loss=9.0775, LR=2.75e-05, Tokens=35,200, GradNorm=3.507\n",
            "New best model saved! Val Loss: 9.0775 (improvement: 0.2030)\n",
            "Epoch 1/150, Step 300: Train Loss=9.0109, Val Loss=8.8507, LR=3.00e-05, Tokens=38,400, GradNorm=2.916\n",
            "New best model saved! Val Loss: 8.8507 (improvement: 0.2268)\n",
            "Epoch 1/150, Step 325: Train Loss=8.7245, Val Loss=8.6165, LR=3.25e-05, Tokens=41,600, GradNorm=2.698\n",
            "New best model saved! Val Loss: 8.6165 (improvement: 0.2342)\n",
            "Epoch 1/150, Step 350: Train Loss=8.4380, Val Loss=8.3634, LR=3.50e-05, Tokens=44,800, GradNorm=3.308\n",
            "New best model saved! Val Loss: 8.3634 (improvement: 0.2531)\n",
            "Epoch 1/150, Step 375: Train Loss=7.9618, Val Loss=8.1174, LR=3.75e-05, Tokens=48,000, GradNorm=2.543\n",
            "New best model saved! Val Loss: 8.1174 (improvement: 0.2460)\n",
            "Epoch 1 completed. Average loss: 9.6600\n",
            "Epoch 2/150, Step 400: Train Loss=7.8434, Val Loss=7.8268, LR=4.00e-05, Tokens=51,200, GradNorm=2.375\n",
            "New best model saved! Val Loss: 7.8268 (improvement: 0.2907)\n",
            "Epoch 2/150, Step 425: Train Loss=7.4684, Val Loss=7.5731, LR=4.25e-05, Tokens=54,400, GradNorm=3.704\n",
            "New best model saved! Val Loss: 7.5731 (improvement: 0.2536)\n",
            "Epoch 2/150, Step 450: Train Loss=7.2559, Val Loss=7.2957, LR=4.50e-05, Tokens=57,600, GradNorm=2.596\n",
            "New best model saved! Val Loss: 7.2957 (improvement: 0.2775)\n",
            "Epoch 2/150, Step 475: Train Loss=6.8912, Val Loss=7.0441, LR=4.75e-05, Tokens=60,800, GradNorm=3.516\n",
            "New best model saved! Val Loss: 7.0441 (improvement: 0.2515)\n",
            "Epoch 2/150, Step 500: Train Loss=6.6656, Val Loss=6.8318, LR=5.00e-05, Tokens=64,000, GradNorm=3.109\n",
            "New best model saved! Val Loss: 6.8318 (improvement: 0.2123)\n",
            "Epoch 2/150, Step 525: Train Loss=6.7086, Val Loss=6.6514, LR=5.25e-05, Tokens=67,200, GradNorm=2.285\n",
            "New best model saved! Val Loss: 6.6514 (improvement: 0.1804)\n",
            "Epoch 2/150, Step 550: Train Loss=6.6962, Val Loss=6.5110, LR=5.50e-05, Tokens=70,400, GradNorm=2.477\n",
            "New best model saved! Val Loss: 6.5110 (improvement: 0.1404)\n",
            "Epoch 2/150, Step 575: Train Loss=6.2406, Val Loss=6.3631, LR=5.75e-05, Tokens=73,600, GradNorm=1.929\n",
            "New best model saved! Val Loss: 6.3631 (improvement: 0.1479)\n",
            "Epoch 2/150, Step 600: Train Loss=6.0547, Val Loss=6.2660, LR=6.00e-05, Tokens=76,800, GradNorm=2.165\n",
            "New best model saved! Val Loss: 6.2660 (improvement: 0.0971)\n",
            "Epoch 2/150, Step 625: Train Loss=5.9890, Val Loss=6.1866, LR=6.25e-05, Tokens=80,000, GradNorm=2.496\n",
            "New best model saved! Val Loss: 6.1866 (improvement: 0.0794)\n",
            "Epoch 2/150, Step 650: Train Loss=6.3303, Val Loss=6.1483, LR=6.50e-05, Tokens=83,200, GradNorm=2.337\n",
            "New best model saved! Val Loss: 6.1483 (improvement: 0.0383)\n",
            "Epoch 2/150, Step 675: Train Loss=5.8534, Val Loss=6.0908, LR=6.75e-05, Tokens=86,400, GradNorm=3.803\n",
            "New best model saved! Val Loss: 6.0908 (improvement: 0.0575)\n",
            "Epoch 2/150, Step 700: Train Loss=6.2435, Val Loss=6.0828, LR=7.00e-05, Tokens=89,600, GradNorm=2.738\n",
            "New best model saved! Val Loss: 6.0828 (improvement: 0.0080)\n",
            "Epoch 2/150, Step 725: Train Loss=6.2256, Val Loss=6.0571, LR=7.25e-05, Tokens=92,800, GradNorm=2.004\n",
            "New best model saved! Val Loss: 6.0571 (improvement: 0.0257)\n",
            "Epoch 2/150, Step 750: Train Loss=5.9257, Val Loss=6.0266, LR=7.50e-05, Tokens=96,000, GradNorm=2.633\n",
            "New best model saved! Val Loss: 6.0266 (improvement: 0.0305)\n",
            "Epoch 2 completed. Average loss: 6.5987\n",
            "Epoch 3/150, Step 775: Train Loss=5.9023, Val Loss=6.0247, LR=7.75e-05, Tokens=99,200, GradNorm=1.882\n",
            "New best model saved! Val Loss: 6.0247 (improvement: 0.0019)\n",
            "Epoch 3/150, Step 800: Train Loss=5.8505, Val Loss=5.9741, LR=8.00e-05, Tokens=102,400, GradNorm=2.265\n",
            "New best model saved! Val Loss: 5.9741 (improvement: 0.0507)\n",
            "Epoch 3/150, Step 825: Train Loss=5.9054, Val Loss=5.9461, LR=8.00e-05, Tokens=105,600, GradNorm=3.571\n",
            "New best model saved! Val Loss: 5.9461 (improvement: 0.0279)\n",
            "Epoch 3/150, Step 850: Train Loss=5.5922, Val Loss=5.9293, LR=8.00e-05, Tokens=108,800, GradNorm=2.957\n",
            "New best model saved! Val Loss: 5.9293 (improvement: 0.0169)\n",
            "Epoch 3/150, Step 875: Train Loss=5.6921, Val Loss=5.8855, LR=8.00e-05, Tokens=112,000, GradNorm=2.644\n",
            "New best model saved! Val Loss: 5.8855 (improvement: 0.0437)\n",
            "Epoch 3/150, Step 900: Train Loss=5.4795, Val Loss=5.8696, LR=8.00e-05, Tokens=115,200, GradNorm=3.416\n",
            "New best model saved! Val Loss: 5.8696 (improvement: 0.0159)\n",
            "Epoch 3/150, Step 925: Train Loss=5.6834, Val Loss=5.8526, LR=8.00e-05, Tokens=118,400, GradNorm=2.903\n",
            "New best model saved! Val Loss: 5.8526 (improvement: 0.0170)\n",
            "Epoch 3/150, Step 950: Train Loss=5.5535, Val Loss=5.8258, LR=8.00e-05, Tokens=121,600, GradNorm=2.900\n",
            "New best model saved! Val Loss: 5.8258 (improvement: 0.0268)\n",
            "Epoch 3/150, Step 975: Train Loss=5.6482, Val Loss=5.7876, LR=8.00e-05, Tokens=124,800, GradNorm=2.088\n",
            "New best model saved! Val Loss: 5.7876 (improvement: 0.0382)\n",
            "Epoch 3/150, Step 1000: Train Loss=5.5117, Val Loss=5.7760, LR=8.00e-05, Tokens=128,000, GradNorm=2.548\n",
            "New best model saved! Val Loss: 5.7760 (improvement: 0.0116)\n",
            "Epoch 3/150, Step 1025: Train Loss=5.7009, Val Loss=5.7550, LR=8.00e-05, Tokens=131,200, GradNorm=3.325\n",
            "New best model saved! Val Loss: 5.7550 (improvement: 0.0210)\n",
            "Epoch 3/150, Step 1050: Train Loss=5.5147, Val Loss=5.6929, LR=8.00e-05, Tokens=134,400, GradNorm=2.775\n",
            "New best model saved! Val Loss: 5.6929 (improvement: 0.0621)\n",
            "Epoch 3/150, Step 1075: Train Loss=5.2618, Val Loss=5.6723, LR=8.00e-05, Tokens=137,600, GradNorm=2.650\n",
            "New best model saved! Val Loss: 5.6723 (improvement: 0.0206)\n",
            "Epoch 3/150, Step 1100: Train Loss=5.5602, Val Loss=5.6372, LR=8.00e-05, Tokens=140,800, GradNorm=3.491\n",
            "New best model saved! Val Loss: 5.6372 (improvement: 0.0351)\n",
            "Epoch 3/150, Step 1125: Train Loss=5.5543, Val Loss=5.5978, LR=8.00e-05, Tokens=144,000, GradNorm=3.733\n",
            "New best model saved! Val Loss: 5.5978 (improvement: 0.0394)\n",
            "Epoch 3/150, Step 1150: Train Loss=5.3177, Val Loss=5.5992, LR=8.00e-05, Tokens=147,200, GradNorm=2.789\n",
            "Epoch 3 completed. Average loss: 5.7118\n",
            "Epoch 4/150, Step 1175: Train Loss=5.7793, Val Loss=5.5593, LR=8.00e-05, Tokens=150,400, GradNorm=3.513\n",
            "New best model saved! Val Loss: 5.5593 (improvement: 0.0385)\n",
            "Epoch 4/150, Step 1200: Train Loss=5.2444, Val Loss=5.5674, LR=8.00e-05, Tokens=153,600, GradNorm=3.086\n",
            "Epoch 4/150, Step 1225: Train Loss=5.1987, Val Loss=5.5356, LR=8.00e-05, Tokens=156,800, GradNorm=2.760\n",
            "New best model saved! Val Loss: 5.5356 (improvement: 0.0237)\n",
            "Epoch 4/150, Step 1250: Train Loss=5.4932, Val Loss=5.5182, LR=8.00e-05, Tokens=160,000, GradNorm=2.728\n",
            "New best model saved! Val Loss: 5.5182 (improvement: 0.0174)\n",
            "Epoch 4/150, Step 1275: Train Loss=4.7425, Val Loss=5.5078, LR=8.00e-05, Tokens=163,200, GradNorm=3.479\n",
            "New best model saved! Val Loss: 5.5078 (improvement: 0.0104)\n",
            "Epoch 4/150, Step 1300: Train Loss=5.2610, Val Loss=5.4813, LR=8.00e-05, Tokens=166,400, GradNorm=3.298\n",
            "New best model saved! Val Loss: 5.4813 (improvement: 0.0265)\n",
            "Epoch 4/150, Step 1325: Train Loss=5.5047, Val Loss=5.4391, LR=8.00e-05, Tokens=169,600, GradNorm=5.611\n",
            "New best model saved! Val Loss: 5.4391 (improvement: 0.0422)\n",
            "Epoch 4/150, Step 1350: Train Loss=4.9795, Val Loss=5.4510, LR=8.00e-05, Tokens=172,800, GradNorm=5.455\n",
            "Epoch 4/150, Step 1375: Train Loss=5.0479, Val Loss=5.4230, LR=8.00e-05, Tokens=176,000, GradNorm=4.003\n",
            "New best model saved! Val Loss: 5.4230 (improvement: 0.0161)\n",
            "Epoch 4/150, Step 1400: Train Loss=5.0800, Val Loss=5.4231, LR=8.00e-05, Tokens=179,200, GradNorm=4.565\n",
            "Epoch 4/150, Step 1425: Train Loss=5.3999, Val Loss=5.4015, LR=8.00e-05, Tokens=182,400, GradNorm=4.227\n",
            "New best model saved! Val Loss: 5.4015 (improvement: 0.0215)\n",
            "Epoch 4/150, Step 1450: Train Loss=5.0299, Val Loss=5.3738, LR=8.00e-05, Tokens=185,600, GradNorm=4.169\n",
            "New best model saved! Val Loss: 5.3738 (improvement: 0.0277)\n",
            "Epoch 4/150, Step 1475: Train Loss=5.1411, Val Loss=5.3918, LR=8.00e-05, Tokens=188,800, GradNorm=4.444\n",
            "Epoch 4/150, Step 1500: Train Loss=4.8957, Val Loss=5.3031, LR=8.00e-05, Tokens=192,000, GradNorm=4.881\n",
            "New best model saved! Val Loss: 5.3031 (improvement: 0.0707)\n",
            "Epoch 4/150, Step 1525: Train Loss=4.7039, Val Loss=5.3122, LR=8.00e-05, Tokens=195,200, GradNorm=4.126\n",
            "Epoch 4 completed. Average loss: 5.3014\n",
            "Epoch 5/150, Step 1550: Train Loss=5.3739, Val Loss=5.2646, LR=8.00e-05, Tokens=198,400, GradNorm=3.173\n",
            "New best model saved! Val Loss: 5.2646 (improvement: 0.0385)\n",
            "Epoch 5/150, Step 1575: Train Loss=5.0546, Val Loss=5.2554, LR=8.00e-05, Tokens=201,600, GradNorm=4.168\n",
            "New best model saved! Val Loss: 5.2554 (improvement: 0.0092)\n",
            "Epoch 5/150, Step 1600: Train Loss=5.0225, Val Loss=5.2617, LR=8.00e-05, Tokens=204,800, GradNorm=3.345\n",
            "Epoch 5/150, Step 1625: Train Loss=4.6753, Val Loss=5.2682, LR=8.00e-05, Tokens=208,000, GradNorm=6.580\n",
            "Epoch 5/150, Step 1650: Train Loss=5.2090, Val Loss=5.2926, LR=8.00e-05, Tokens=211,200, GradNorm=4.167\n",
            "Epoch 5/150, Step 1675: Train Loss=5.0294, Val Loss=5.2335, LR=8.00e-05, Tokens=214,400, GradNorm=4.899\n",
            "New best model saved! Val Loss: 5.2335 (improvement: 0.0219)\n",
            "Epoch 5/150, Step 1700: Train Loss=4.9222, Val Loss=5.2204, LR=8.00e-05, Tokens=217,600, GradNorm=6.046\n",
            "New best model saved! Val Loss: 5.2204 (improvement: 0.0132)\n",
            "Epoch 5/150, Step 1725: Train Loss=4.6472, Val Loss=5.1899, LR=7.99e-05, Tokens=220,800, GradNorm=4.154\n",
            "New best model saved! Val Loss: 5.1899 (improvement: 0.0305)\n",
            "Epoch 5/150, Step 1750: Train Loss=5.0105, Val Loss=5.1728, LR=7.99e-05, Tokens=224,000, GradNorm=3.904\n",
            "New best model saved! Val Loss: 5.1728 (improvement: 0.0171)\n",
            "Epoch 5/150, Step 1775: Train Loss=5.2125, Val Loss=5.1643, LR=7.99e-05, Tokens=227,200, GradNorm=4.105\n",
            "New best model saved! Val Loss: 5.1643 (improvement: 0.0086)\n",
            "Epoch 5/150, Step 1800: Train Loss=5.3116, Val Loss=5.1619, LR=7.99e-05, Tokens=230,400, GradNorm=5.759\n",
            "New best model saved! Val Loss: 5.1619 (improvement: 0.0024)\n",
            "Epoch 5/150, Step 1825: Train Loss=4.9133, Val Loss=5.1883, LR=7.99e-05, Tokens=233,600, GradNorm=4.262\n",
            "Epoch 5/150, Step 1850: Train Loss=4.6557, Val Loss=5.1420, LR=7.99e-05, Tokens=236,800, GradNorm=4.631\n",
            "New best model saved! Val Loss: 5.1420 (improvement: 0.0199)\n",
            "Epoch 5/150, Step 1875: Train Loss=5.2012, Val Loss=5.1447, LR=7.99e-05, Tokens=240,000, GradNorm=3.725\n",
            "Epoch 5/150, Step 1900: Train Loss=4.7827, Val Loss=5.1141, LR=7.99e-05, Tokens=243,200, GradNorm=4.988\n",
            "New best model saved! Val Loss: 5.1141 (improvement: 0.0279)\n",
            "Epoch 5 completed. Average loss: 5.0395\n",
            "Epoch 6/150, Step 1925: Train Loss=4.9082, Val Loss=5.1265, LR=7.99e-05, Tokens=246,400, GradNorm=4.477\n",
            "Epoch 6/150, Step 1950: Train Loss=4.8297, Val Loss=5.0870, LR=7.99e-05, Tokens=249,600, GradNorm=5.029\n",
            "New best model saved! Val Loss: 5.0870 (improvement: 0.0270)\n",
            "Epoch 6/150, Step 1975: Train Loss=4.8090, Val Loss=5.0754, LR=7.99e-05, Tokens=252,800, GradNorm=3.912\n",
            "New best model saved! Val Loss: 5.0754 (improvement: 0.0117)\n",
            "Epoch 6/150, Step 2000: Train Loss=4.9656, Val Loss=5.1162, LR=7.99e-05, Tokens=256,000, GradNorm=5.583\n",
            "Epoch 6/150, Step 2025: Train Loss=5.2802, Val Loss=5.0990, LR=7.99e-05, Tokens=259,200, GradNorm=4.478\n",
            "Epoch 6/150, Step 2050: Train Loss=4.8178, Val Loss=5.0826, LR=7.99e-05, Tokens=262,400, GradNorm=5.089\n",
            "Epoch 6/150, Step 2075: Train Loss=4.5662, Val Loss=5.0727, LR=7.99e-05, Tokens=265,600, GradNorm=4.969\n",
            "New best model saved! Val Loss: 5.0727 (improvement: 0.0027)\n",
            "Epoch 6/150, Step 2100: Train Loss=4.6135, Val Loss=5.0396, LR=7.99e-05, Tokens=268,800, GradNorm=5.079\n",
            "New best model saved! Val Loss: 5.0396 (improvement: 0.0332)\n",
            "Epoch 6/150, Step 2125: Train Loss=4.7268, Val Loss=5.0088, LR=7.99e-05, Tokens=272,000, GradNorm=5.230\n",
            "New best model saved! Val Loss: 5.0088 (improvement: 0.0307)\n",
            "Epoch 6/150, Step 2150: Train Loss=4.5578, Val Loss=5.0123, LR=7.99e-05, Tokens=275,200, GradNorm=4.871\n",
            "Epoch 6/150, Step 2175: Train Loss=4.5905, Val Loss=4.9880, LR=7.99e-05, Tokens=278,400, GradNorm=5.541\n",
            "New best model saved! Val Loss: 4.9880 (improvement: 0.0208)\n",
            "Epoch 6/150, Step 2200: Train Loss=4.6318, Val Loss=4.9712, LR=7.99e-05, Tokens=281,600, GradNorm=4.770\n",
            "New best model saved! Val Loss: 4.9712 (improvement: 0.0168)\n",
            "Epoch 6/150, Step 2225: Train Loss=4.8751, Val Loss=4.9731, LR=7.99e-05, Tokens=284,800, GradNorm=5.314\n",
            "Epoch 6/150, Step 2250: Train Loss=4.5156, Val Loss=4.9760, LR=7.99e-05, Tokens=288,000, GradNorm=5.728\n",
            "Epoch 6/150, Step 2275: Train Loss=5.1537, Val Loss=4.9477, LR=7.99e-05, Tokens=291,200, GradNorm=4.285\n",
            "New best model saved! Val Loss: 4.9477 (improvement: 0.0235)\n",
            "Epoch 6/150, Step 2300: Train Loss=5.2228, Val Loss=4.9710, LR=7.99e-05, Tokens=294,400, GradNorm=4.714\n",
            "Epoch 6 completed. Average loss: 4.8224\n",
            "Epoch 7/150, Step 2325: Train Loss=4.4535, Val Loss=4.9489, LR=7.99e-05, Tokens=297,600, GradNorm=5.737\n",
            "Epoch 7/150, Step 2350: Train Loss=4.5993, Val Loss=4.9090, LR=7.99e-05, Tokens=300,800, GradNorm=6.029\n",
            "New best model saved! Val Loss: 4.9090 (improvement: 0.0387)\n",
            "Epoch 7/150, Step 2375: Train Loss=4.7728, Val Loss=4.9501, LR=7.98e-05, Tokens=304,000, GradNorm=4.484\n",
            "Epoch 7/150, Step 2400: Train Loss=5.0441, Val Loss=4.9752, LR=7.98e-05, Tokens=307,200, GradNorm=4.599\n",
            "Epoch 7/150, Step 2425: Train Loss=4.6502, Val Loss=4.9336, LR=7.98e-05, Tokens=310,400, GradNorm=5.631\n",
            "Epoch 7/150, Step 2450: Train Loss=4.4854, Val Loss=4.8931, LR=7.98e-05, Tokens=313,600, GradNorm=5.885\n",
            "New best model saved! Val Loss: 4.8931 (improvement: 0.0159)\n",
            "Epoch 7/150, Step 2475: Train Loss=4.6850, Val Loss=4.8589, LR=7.98e-05, Tokens=316,800, GradNorm=5.654\n",
            "New best model saved! Val Loss: 4.8589 (improvement: 0.0342)\n",
            "Epoch 7/150, Step 2500: Train Loss=4.7095, Val Loss=4.8809, LR=7.98e-05, Tokens=320,000, GradNorm=5.700\n",
            "Epoch 7/150, Step 2525: Train Loss=4.9679, Val Loss=4.8572, LR=7.98e-05, Tokens=323,200, GradNorm=8.484\n",
            "New best model saved! Val Loss: 4.8572 (improvement: 0.0017)\n",
            "Epoch 7/150, Step 2550: Train Loss=4.6907, Val Loss=4.8789, LR=7.98e-05, Tokens=326,400, GradNorm=5.130\n",
            "Epoch 7/150, Step 2575: Train Loss=4.6767, Val Loss=4.8667, LR=7.98e-05, Tokens=329,600, GradNorm=5.996\n",
            "Epoch 7/150, Step 2600: Train Loss=4.5846, Val Loss=4.8334, LR=7.98e-05, Tokens=332,800, GradNorm=6.562\n",
            "New best model saved! Val Loss: 4.8334 (improvement: 0.0238)\n",
            "Epoch 7/150, Step 2625: Train Loss=4.5077, Val Loss=4.8612, LR=7.98e-05, Tokens=336,000, GradNorm=5.994\n",
            "Epoch 7/150, Step 2650: Train Loss=4.5143, Val Loss=4.8494, LR=7.98e-05, Tokens=339,200, GradNorm=5.483\n",
            "Epoch 7/150, Step 2675: Train Loss=4.0879, Val Loss=4.8617, LR=7.98e-05, Tokens=342,400, GradNorm=5.938\n",
            "Epoch 7 completed. Average loss: 4.6385\n",
            "Epoch 8/150, Step 2700: Train Loss=4.8475, Val Loss=4.8701, LR=7.98e-05, Tokens=345,600, GradNorm=6.912\n",
            "Epoch 8/150, Step 2725: Train Loss=4.2922, Val Loss=4.8857, LR=7.98e-05, Tokens=348,800, GradNorm=6.729\n",
            "Epoch 8/150, Step 2750: Train Loss=4.3112, Val Loss=4.8963, LR=7.98e-05, Tokens=352,000, GradNorm=6.177\n",
            "Epoch 8/150, Step 2775: Train Loss=4.5674, Val Loss=4.8896, LR=7.98e-05, Tokens=355,200, GradNorm=5.699\n",
            "Epoch 8/150, Step 2800: Train Loss=4.1496, Val Loss=4.8455, LR=7.98e-05, Tokens=358,400, GradNorm=5.305\n",
            "Epoch 8/150, Step 2825: Train Loss=4.2053, Val Loss=4.8441, LR=7.97e-05, Tokens=361,600, GradNorm=5.836\n",
            "Epoch 8/150, Step 2850: Train Loss=4.6549, Val Loss=4.8801, LR=7.97e-05, Tokens=364,800, GradNorm=6.625\n",
            "Epoch 8/150, Step 2875: Train Loss=3.8196, Val Loss=4.8615, LR=7.97e-05, Tokens=368,000, GradNorm=6.228\n",
            "Epoch 8/150, Step 2900: Train Loss=4.5263, Val Loss=4.8521, LR=7.97e-05, Tokens=371,200, GradNorm=6.787\n",
            "Epoch 8/150, Step 2925: Train Loss=4.2874, Val Loss=4.8196, LR=7.97e-05, Tokens=374,400, GradNorm=5.390\n",
            "New best model saved! Val Loss: 4.8196 (improvement: 0.0138)\n",
            "Epoch 8/150, Step 2950: Train Loss=4.4987, Val Loss=4.7996, LR=7.97e-05, Tokens=377,600, GradNorm=7.072\n",
            "New best model saved! Val Loss: 4.7996 (improvement: 0.0200)\n",
            "Epoch 8/150, Step 2975: Train Loss=4.2473, Val Loss=4.7763, LR=7.97e-05, Tokens=380,800, GradNorm=5.513\n",
            "New best model saved! Val Loss: 4.7763 (improvement: 0.0233)\n",
            "Epoch 8/150, Step 3000: Train Loss=4.6707, Val Loss=4.7340, LR=7.97e-05, Tokens=384,000, GradNorm=5.931\n",
            "New best model saved! Val Loss: 4.7340 (improvement: 0.0423)\n",
            "Epoch 8/150, Step 3025: Train Loss=4.4141, Val Loss=4.7529, LR=7.97e-05, Tokens=387,200, GradNorm=5.728\n",
            "Epoch 8/150, Step 3050: Train Loss=4.3258, Val Loss=4.7566, LR=7.97e-05, Tokens=390,400, GradNorm=6.363\n",
            "Epoch 8 completed. Average loss: 4.4753\n",
            "Epoch 9/150, Step 3075: Train Loss=4.4724, Val Loss=4.7217, LR=7.97e-05, Tokens=393,600, GradNorm=6.123\n",
            "New best model saved! Val Loss: 4.7217 (improvement: 0.0123)\n",
            "Epoch 9/150, Step 3100: Train Loss=4.0705, Val Loss=4.7386, LR=7.97e-05, Tokens=396,800, GradNorm=5.882\n",
            "Epoch 9/150, Step 3125: Train Loss=4.3001, Val Loss=4.7331, LR=7.97e-05, Tokens=400,000, GradNorm=6.131\n",
            "Epoch 9/150, Step 3150: Train Loss=4.5082, Val Loss=4.7765, LR=7.97e-05, Tokens=403,200, GradNorm=7.282\n",
            "Epoch 9/150, Step 3175: Train Loss=4.8585, Val Loss=4.7479, LR=7.97e-05, Tokens=406,400, GradNorm=5.781\n",
            "Epoch 9/150, Step 3200: Train Loss=3.5923, Val Loss=4.7215, LR=7.96e-05, Tokens=409,600, GradNorm=6.496\n",
            "Epoch 9/150, Step 3225: Train Loss=3.7601, Val Loss=4.7415, LR=7.96e-05, Tokens=412,800, GradNorm=7.847\n",
            "Epoch 9/150, Step 3250: Train Loss=4.0554, Val Loss=4.6828, LR=7.96e-05, Tokens=416,000, GradNorm=7.117\n",
            "New best model saved! Val Loss: 4.6828 (improvement: 0.0389)\n",
            "Epoch 9/150, Step 3275: Train Loss=4.6335, Val Loss=4.6740, LR=7.96e-05, Tokens=419,200, GradNorm=5.123\n",
            "New best model saved! Val Loss: 4.6740 (improvement: 0.0088)\n",
            "Epoch 9/150, Step 3300: Train Loss=3.9861, Val Loss=4.6548, LR=7.96e-05, Tokens=422,400, GradNorm=5.557\n",
            "New best model saved! Val Loss: 4.6548 (improvement: 0.0192)\n",
            "Epoch 9/150, Step 3325: Train Loss=4.5007, Val Loss=4.6746, LR=7.96e-05, Tokens=425,600, GradNorm=6.253\n",
            "Epoch 9/150, Step 3350: Train Loss=4.0911, Val Loss=4.7070, LR=7.96e-05, Tokens=428,800, GradNorm=7.490\n",
            "Epoch 9/150, Step 3375: Train Loss=4.4216, Val Loss=4.7075, LR=7.96e-05, Tokens=432,000, GradNorm=7.635\n",
            "Epoch 9/150, Step 3400: Train Loss=3.9596, Val Loss=4.6837, LR=7.96e-05, Tokens=435,200, GradNorm=7.122\n",
            "Epoch 9/150, Step 3425: Train Loss=4.0919, Val Loss=4.6741, LR=7.96e-05, Tokens=438,400, GradNorm=7.154\n",
            "Epoch 9/150, Step 3450: Train Loss=3.9993, Val Loss=4.6926, LR=7.96e-05, Tokens=441,600, GradNorm=7.826\n",
            "Epoch 9 completed. Average loss: 4.3162\n",
            "Epoch 10/150, Step 3475: Train Loss=3.9966, Val Loss=4.6914, LR=7.96e-05, Tokens=444,800, GradNorm=6.342\n",
            "Epoch 10/150, Step 3500: Train Loss=4.0076, Val Loss=4.6802, LR=7.96e-05, Tokens=448,000, GradNorm=6.575\n",
            "Epoch 10/150, Step 3525: Train Loss=3.9988, Val Loss=4.6969, LR=7.95e-05, Tokens=451,200, GradNorm=7.042\n",
            "Epoch 10/150, Step 3550: Train Loss=4.1445, Val Loss=4.6389, LR=7.95e-05, Tokens=454,400, GradNorm=8.616\n",
            "New best model saved! Val Loss: 4.6389 (improvement: 0.0160)\n",
            "Epoch 10/150, Step 3575: Train Loss=4.1573, Val Loss=4.6431, LR=7.95e-05, Tokens=457,600, GradNorm=6.285\n",
            "Epoch 10/150, Step 3600: Train Loss=4.1552, Val Loss=4.6032, LR=7.95e-05, Tokens=460,800, GradNorm=7.959\n",
            "New best model saved! Val Loss: 4.6032 (improvement: 0.0356)\n",
            "Epoch 10/150, Step 3625: Train Loss=3.8580, Val Loss=4.6139, LR=7.95e-05, Tokens=464,000, GradNorm=7.300\n",
            "Epoch 10/150, Step 3650: Train Loss=3.6492, Val Loss=4.5814, LR=7.95e-05, Tokens=467,200, GradNorm=6.796\n",
            "New best model saved! Val Loss: 4.5814 (improvement: 0.0219)\n",
            "Epoch 10/150, Step 3675: Train Loss=4.1868, Val Loss=4.6126, LR=7.95e-05, Tokens=470,400, GradNorm=7.512\n",
            "Epoch 10/150, Step 3700: Train Loss=3.6553, Val Loss=4.6083, LR=7.95e-05, Tokens=473,600, GradNorm=7.049\n",
            "Epoch 10/150, Step 3725: Train Loss=4.2096, Val Loss=4.5751, LR=7.95e-05, Tokens=476,800, GradNorm=6.809\n",
            "New best model saved! Val Loss: 4.5751 (improvement: 0.0063)\n",
            "Epoch 10/150, Step 3750: Train Loss=4.1748, Val Loss=4.5285, LR=7.95e-05, Tokens=480,000, GradNorm=6.970\n",
            "New best model saved! Val Loss: 4.5285 (improvement: 0.0466)\n",
            "Epoch 10/150, Step 3775: Train Loss=3.8465, Val Loss=4.5858, LR=7.95e-05, Tokens=483,200, GradNorm=6.802\n",
            "Epoch 10/150, Step 3800: Train Loss=3.5483, Val Loss=4.5823, LR=7.95e-05, Tokens=486,400, GradNorm=4.509\n",
            "Epoch 10/150, Step 3825: Train Loss=4.0250, Val Loss=4.5656, LR=7.94e-05, Tokens=489,600, GradNorm=5.394\n",
            "Epoch 10 completed. Average loss: 4.1756\n",
            "Sample generation after epoch 10:\n",
            "This movie  soon, with it UFO And, such your ever be world deutter It somebody Not wonder all the fell for youre going waiting saved And apart And been a weapon and somebody Oh,In the... We bureaucracy He do  give you could- feelingThey arms, feel me on up it? Fly the way it It occupiesMay he Im in\n",
            "------------------------------------------------------------\n",
            "Epoch 11/150, Step 3850: Train Loss=3.9705, Val Loss=4.5375, LR=7.94e-05, Tokens=492,800, GradNorm=6.325\n",
            "Epoch 11/150, Step 3875: Train Loss=3.9612, Val Loss=4.5186, LR=7.94e-05, Tokens=496,000, GradNorm=7.989\n",
            "New best model saved! Val Loss: 4.5186 (improvement: 0.0099)\n",
            "Epoch 11/150, Step 3900: Train Loss=4.1250, Val Loss=4.5369, LR=7.94e-05, Tokens=499,200, GradNorm=7.362\n",
            "Epoch 11/150, Step 3925: Train Loss=3.7985, Val Loss=4.5100, LR=7.94e-05, Tokens=502,400, GradNorm=8.744\n",
            "New best model saved! Val Loss: 4.5100 (improvement: 0.0086)\n",
            "Epoch 11/150, Step 3950: Train Loss=4.0286, Val Loss=4.5085, LR=7.94e-05, Tokens=505,600, GradNorm=6.375\n",
            "New best model saved! Val Loss: 4.5085 (improvement: 0.0015)\n",
            "Epoch 11/150, Step 3975: Train Loss=3.4948, Val Loss=4.4686, LR=7.94e-05, Tokens=508,800, GradNorm=7.735\n",
            "New best model saved! Val Loss: 4.4686 (improvement: 0.0399)\n",
            "Epoch 11/150, Step 4000: Train Loss=4.1066, Val Loss=4.4946, LR=7.94e-05, Tokens=512,000, GradNorm=7.642\n",
            "Epoch 11/150, Step 4025: Train Loss=3.8920, Val Loss=4.5083, LR=7.94e-05, Tokens=515,200, GradNorm=9.238\n",
            "Epoch 11/150, Step 4050: Train Loss=3.6599, Val Loss=4.4649, LR=7.94e-05, Tokens=518,400, GradNorm=7.085\n",
            "New best model saved! Val Loss: 4.4649 (improvement: 0.0037)\n",
            "Epoch 11/150, Step 4075: Train Loss=3.5123, Val Loss=4.4831, LR=7.93e-05, Tokens=521,600, GradNorm=7.788\n",
            "Epoch 11/150, Step 4100: Train Loss=4.1493, Val Loss=4.4711, LR=7.93e-05, Tokens=524,800, GradNorm=8.231\n",
            "Epoch 11/150, Step 4125: Train Loss=3.9250, Val Loss=4.4837, LR=7.93e-05, Tokens=528,000, GradNorm=7.186\n",
            "Epoch 11/150, Step 4150: Train Loss=4.0861, Val Loss=4.4961, LR=7.93e-05, Tokens=531,200, GradNorm=8.554\n",
            "Epoch 11/150, Step 4175: Train Loss=3.8714, Val Loss=4.4819, LR=7.93e-05, Tokens=534,400, GradNorm=8.617\n",
            "Epoch 11/150, Step 4200: Train Loss=3.6386, Val Loss=4.4978, LR=7.93e-05, Tokens=537,600, GradNorm=7.217\n",
            "Epoch 11 completed. Average loss: 4.0349\n",
            "Epoch 12/150, Step 4225: Train Loss=3.8638, Val Loss=4.5233, LR=7.93e-05, Tokens=540,800, GradNorm=9.386\n",
            "Epoch 12/150, Step 4250: Train Loss=3.9342, Val Loss=4.5479, LR=7.93e-05, Tokens=544,000, GradNorm=8.195\n",
            "Epoch 12/150, Step 4275: Train Loss=4.1553, Val Loss=4.4526, LR=7.93e-05, Tokens=547,200, GradNorm=8.399\n",
            "New best model saved! Val Loss: 4.4526 (improvement: 0.0124)\n",
            "Epoch 12/150, Step 4300: Train Loss=3.9031, Val Loss=4.4500, LR=7.93e-05, Tokens=550,400, GradNorm=6.880\n",
            "New best model saved! Val Loss: 4.4500 (improvement: 0.0025)\n",
            "Epoch 12/150, Step 4325: Train Loss=3.9887, Val Loss=4.4284, LR=7.92e-05, Tokens=553,600, GradNorm=8.388\n",
            "New best model saved! Val Loss: 4.4284 (improvement: 0.0216)\n",
            "Epoch 12/150, Step 4350: Train Loss=3.9287, Val Loss=4.4677, LR=7.92e-05, Tokens=556,800, GradNorm=7.147\n",
            "Epoch 12/150, Step 4375: Train Loss=3.8174, Val Loss=4.4637, LR=7.92e-05, Tokens=560,000, GradNorm=8.059\n",
            "Epoch 12/150, Step 4400: Train Loss=4.2038, Val Loss=4.4811, LR=7.92e-05, Tokens=563,200, GradNorm=8.321\n",
            "Epoch 12/150, Step 4425: Train Loss=3.7817, Val Loss=4.4846, LR=7.92e-05, Tokens=566,400, GradNorm=8.574\n",
            "Epoch 12/150, Step 4450: Train Loss=3.4772, Val Loss=4.4797, LR=7.92e-05, Tokens=569,600, GradNorm=9.050\n",
            "Epoch 12/150, Step 4475: Train Loss=3.9798, Val Loss=4.4725, LR=7.92e-05, Tokens=572,800, GradNorm=8.128\n",
            "Epoch 12/150, Step 4500: Train Loss=3.7304, Val Loss=4.4659, LR=7.92e-05, Tokens=576,000, GradNorm=8.996\n",
            "Epoch 12/150, Step 4525: Train Loss=3.8418, Val Loss=4.4667, LR=7.92e-05, Tokens=579,200, GradNorm=8.193\n",
            "Epoch 12/150, Step 4550: Train Loss=3.5922, Val Loss=4.4456, LR=7.91e-05, Tokens=582,400, GradNorm=6.519\n",
            "Epoch 12/150, Step 4575: Train Loss=4.0815, Val Loss=4.4381, LR=7.91e-05, Tokens=585,600, GradNorm=7.367\n",
            "Epoch 12/150, Step 4600: Train Loss=3.7395, Val Loss=4.3858, LR=7.91e-05, Tokens=588,800, GradNorm=7.622\n",
            "New best model saved! Val Loss: 4.3858 (improvement: 0.0426)\n",
            "Epoch 12 completed. Average loss: 3.9001\n",
            "Epoch 13/150, Step 4625: Train Loss=3.5766, Val Loss=4.3903, LR=7.91e-05, Tokens=592,000, GradNorm=7.466\n",
            "Epoch 13/150, Step 4650: Train Loss=3.9033, Val Loss=4.4205, LR=7.91e-05, Tokens=595,200, GradNorm=8.626\n",
            "Epoch 13/150, Step 4675: Train Loss=3.8385, Val Loss=4.4167, LR=7.91e-05, Tokens=598,400, GradNorm=7.846\n",
            "Epoch 13/150, Step 4700: Train Loss=3.4336, Val Loss=4.4394, LR=7.91e-05, Tokens=601,600, GradNorm=10.401\n",
            "Epoch 13/150, Step 4725: Train Loss=3.7037, Val Loss=4.4506, LR=7.91e-05, Tokens=604,800, GradNorm=7.477\n",
            "Epoch 13/150, Step 4750: Train Loss=3.5650, Val Loss=4.4062, LR=7.90e-05, Tokens=608,000, GradNorm=7.941\n",
            "Epoch 13/150, Step 4775: Train Loss=3.7339, Val Loss=4.3585, LR=7.90e-05, Tokens=611,200, GradNorm=6.885\n",
            "New best model saved! Val Loss: 4.3585 (improvement: 0.0273)\n",
            "Epoch 13/150, Step 4800: Train Loss=3.5679, Val Loss=4.3401, LR=7.90e-05, Tokens=614,400, GradNorm=9.339\n",
            "New best model saved! Val Loss: 4.3401 (improvement: 0.0184)\n",
            "Epoch 13/150, Step 4825: Train Loss=3.4830, Val Loss=4.3758, LR=7.90e-05, Tokens=617,600, GradNorm=9.108\n",
            "Epoch 13/150, Step 4850: Train Loss=3.3121, Val Loss=4.3990, LR=7.90e-05, Tokens=620,800, GradNorm=7.934\n",
            "Epoch 13/150, Step 4875: Train Loss=4.0063, Val Loss=4.3696, LR=7.90e-05, Tokens=624,000, GradNorm=9.268\n",
            "Epoch 13/150, Step 4900: Train Loss=3.2377, Val Loss=4.3610, LR=7.90e-05, Tokens=627,200, GradNorm=8.104\n",
            "Epoch 13/150, Step 4925: Train Loss=3.8025, Val Loss=4.3806, LR=7.90e-05, Tokens=630,400, GradNorm=6.378\n",
            "Epoch 13/150, Step 4950: Train Loss=3.8025, Val Loss=4.3143, LR=7.90e-05, Tokens=633,600, GradNorm=8.787\n",
            "New best model saved! Val Loss: 4.3143 (improvement: 0.0258)\n",
            "Epoch 13/150, Step 4975: Train Loss=3.4858, Val Loss=4.3271, LR=7.89e-05, Tokens=636,800, GradNorm=9.200\n",
            "Epoch 13 completed. Average loss: 3.7618\n",
            "Epoch 14/150, Step 5000: Train Loss=3.5004, Val Loss=4.2482, LR=7.89e-05, Tokens=640,000, GradNorm=8.302\n",
            "New best model saved! Val Loss: 4.2482 (improvement: 0.0661)\n",
            "Epoch 14/150, Step 5025: Train Loss=3.2371, Val Loss=4.2762, LR=7.89e-05, Tokens=643,200, GradNorm=8.790\n",
            "Epoch 14/150, Step 5050: Train Loss=3.3152, Val Loss=4.2798, LR=7.89e-05, Tokens=646,400, GradNorm=7.996\n",
            "Epoch 14/150, Step 5075: Train Loss=3.3979, Val Loss=4.2835, LR=7.89e-05, Tokens=649,600, GradNorm=8.580\n",
            "Epoch 14/150, Step 5100: Train Loss=3.7457, Val Loss=4.2751, LR=7.89e-05, Tokens=652,800, GradNorm=8.268\n",
            "Epoch 14/150, Step 5125: Train Loss=3.2852, Val Loss=4.2857, LR=7.89e-05, Tokens=656,000, GradNorm=7.097\n",
            "Epoch 14/150, Step 5150: Train Loss=3.4475, Val Loss=4.2261, LR=7.88e-05, Tokens=659,200, GradNorm=10.187\n",
            "New best model saved! Val Loss: 4.2261 (improvement: 0.0221)\n",
            "Epoch 14/150, Step 5175: Train Loss=3.3356, Val Loss=4.2719, LR=7.88e-05, Tokens=662,400, GradNorm=8.431\n",
            "Epoch 14/150, Step 5200: Train Loss=3.2128, Val Loss=4.2233, LR=7.88e-05, Tokens=665,600, GradNorm=7.584\n",
            "New best model saved! Val Loss: 4.2233 (improvement: 0.0028)\n",
            "Epoch 14/150, Step 5225: Train Loss=3.4225, Val Loss=4.2175, LR=7.88e-05, Tokens=668,800, GradNorm=10.137\n",
            "New best model saved! Val Loss: 4.2175 (improvement: 0.0058)\n",
            "Epoch 14/150, Step 5250: Train Loss=3.8893, Val Loss=4.2610, LR=7.88e-05, Tokens=672,000, GradNorm=7.685\n",
            "Epoch 14/150, Step 5275: Train Loss=3.4195, Val Loss=4.2435, LR=7.88e-05, Tokens=675,200, GradNorm=10.864\n",
            "Epoch 14/150, Step 5300: Train Loss=3.6716, Val Loss=4.2516, LR=7.88e-05, Tokens=678,400, GradNorm=8.766\n",
            "Epoch 14/150, Step 5325: Train Loss=3.8216, Val Loss=4.2325, LR=7.88e-05, Tokens=681,600, GradNorm=8.722\n",
            "Epoch 14/150, Step 5350: Train Loss=3.3933, Val Loss=4.2217, LR=7.87e-05, Tokens=684,800, GradNorm=10.215\n",
            "Epoch 14/150, Step 5375: Train Loss=3.3202, Val Loss=4.1890, LR=7.87e-05, Tokens=688,000, GradNorm=8.092\n",
            "New best model saved! Val Loss: 4.1890 (improvement: 0.0285)\n",
            "Epoch 14 completed. Average loss: 3.6293\n",
            "Epoch 15/150, Step 5400: Train Loss=3.5340, Val Loss=4.1976, LR=7.87e-05, Tokens=691,200, GradNorm=7.838\n",
            "Epoch 15/150, Step 5425: Train Loss=3.5742, Val Loss=4.2115, LR=7.87e-05, Tokens=694,400, GradNorm=9.202\n",
            "Epoch 15/150, Step 5450: Train Loss=3.4672, Val Loss=4.1190, LR=7.87e-05, Tokens=697,600, GradNorm=8.674\n",
            "New best model saved! Val Loss: 4.1190 (improvement: 0.0700)\n",
            "Epoch 15/150, Step 5475: Train Loss=3.4529, Val Loss=4.1322, LR=7.87e-05, Tokens=700,800, GradNorm=8.226\n",
            "Epoch 15/150, Step 5500: Train Loss=3.5910, Val Loss=4.1270, LR=7.87e-05, Tokens=704,000, GradNorm=12.014\n",
            "Epoch 15/150, Step 5525: Train Loss=3.6406, Val Loss=4.1243, LR=7.86e-05, Tokens=707,200, GradNorm=9.074\n",
            "Epoch 15/150, Step 5550: Train Loss=2.9680, Val Loss=4.1351, LR=7.86e-05, Tokens=710,400, GradNorm=9.309\n",
            "Epoch 15/150, Step 5575: Train Loss=3.2125, Val Loss=4.1421, LR=7.86e-05, Tokens=713,600, GradNorm=8.040\n",
            "Epoch 15/150, Step 5600: Train Loss=3.6018, Val Loss=4.1752, LR=7.86e-05, Tokens=716,800, GradNorm=8.436\n",
            "Epoch 15/150, Step 5625: Train Loss=3.0488, Val Loss=4.1362, LR=7.86e-05, Tokens=720,000, GradNorm=8.531\n",
            "Epoch 15/150, Step 5650: Train Loss=3.1921, Val Loss=4.0909, LR=7.86e-05, Tokens=723,200, GradNorm=8.638\n",
            "New best model saved! Val Loss: 4.0909 (improvement: 0.0282)\n",
            "Epoch 15/150, Step 5675: Train Loss=2.8074, Val Loss=4.1119, LR=7.86e-05, Tokens=726,400, GradNorm=9.862\n",
            "Epoch 15/150, Step 5700: Train Loss=3.3192, Val Loss=4.1040, LR=7.85e-05, Tokens=729,600, GradNorm=10.812\n",
            "Epoch 15/150, Step 5725: Train Loss=3.7064, Val Loss=4.1159, LR=7.85e-05, Tokens=732,800, GradNorm=8.520\n",
            "Epoch 15/150, Step 5750: Train Loss=3.0656, Val Loss=4.1230, LR=7.85e-05, Tokens=736,000, GradNorm=7.376\n",
            "Epoch 15 completed. Average loss: 3.4915\n",
            "Epoch 16/150, Step 5775: Train Loss=2.9238, Val Loss=4.1158, LR=7.85e-05, Tokens=739,200, GradNorm=7.617\n",
            "Epoch 16/150, Step 5800: Train Loss=3.0359, Val Loss=4.1387, LR=7.85e-05, Tokens=742,400, GradNorm=9.722\n",
            "Epoch 16/150, Step 5825: Train Loss=3.3095, Val Loss=4.1261, LR=7.85e-05, Tokens=745,600, GradNorm=10.026\n",
            "Epoch 16/150, Step 5850: Train Loss=3.0248, Val Loss=4.1365, LR=7.84e-05, Tokens=748,800, GradNorm=8.390\n",
            "Epoch 16/150, Step 5875: Train Loss=3.2171, Val Loss=4.1307, LR=7.84e-05, Tokens=752,000, GradNorm=7.334\n",
            "Epoch 16/150, Step 5900: Train Loss=3.0404, Val Loss=4.1114, LR=7.84e-05, Tokens=755,200, GradNorm=9.749\n",
            "Epoch 16/150, Step 5925: Train Loss=2.6376, Val Loss=4.0893, LR=7.84e-05, Tokens=758,400, GradNorm=8.475\n",
            "New best model saved! Val Loss: 4.0893 (improvement: 0.0015)\n",
            "Epoch 16/150, Step 5950: Train Loss=3.1366, Val Loss=4.0806, LR=7.84e-05, Tokens=761,600, GradNorm=8.869\n",
            "New best model saved! Val Loss: 4.0806 (improvement: 0.0087)\n",
            "Epoch 16/150, Step 5975: Train Loss=3.2378, Val Loss=4.0806, LR=7.84e-05, Tokens=764,800, GradNorm=8.690\n",
            "Epoch 16/150, Step 6000: Train Loss=3.2949, Val Loss=4.0683, LR=7.84e-05, Tokens=768,000, GradNorm=10.143\n",
            "New best model saved! Val Loss: 4.0683 (improvement: 0.0124)\n",
            "Epoch 16/150, Step 6025: Train Loss=2.6141, Val Loss=4.0656, LR=7.83e-05, Tokens=771,200, GradNorm=11.497\n",
            "New best model saved! Val Loss: 4.0656 (improvement: 0.0027)\n",
            "Epoch 16/150, Step 6050: Train Loss=3.3743, Val Loss=4.0706, LR=7.83e-05, Tokens=774,400, GradNorm=9.945\n",
            "Epoch 16/150, Step 6075: Train Loss=2.9947, Val Loss=4.0616, LR=7.83e-05, Tokens=777,600, GradNorm=9.964\n",
            "New best model saved! Val Loss: 4.0616 (improvement: 0.0040)\n",
            "Epoch 16/150, Step 6100: Train Loss=3.1301, Val Loss=4.0505, LR=7.83e-05, Tokens=780,800, GradNorm=8.528\n",
            "New best model saved! Val Loss: 4.0505 (improvement: 0.0110)\n",
            "Epoch 16/150, Step 6125: Train Loss=3.0763, Val Loss=4.0758, LR=7.83e-05, Tokens=784,000, GradNorm=10.350\n",
            "Epoch 16 completed. Average loss: 3.3629\n",
            "Epoch 17/150, Step 6150: Train Loss=2.6747, Val Loss=3.9897, LR=7.83e-05, Tokens=787,200, GradNorm=10.321\n",
            "New best model saved! Val Loss: 3.9897 (improvement: 0.0608)\n",
            "Epoch 17/150, Step 6175: Train Loss=2.6001, Val Loss=3.9473, LR=7.82e-05, Tokens=790,400, GradNorm=8.502\n",
            "New best model saved! Val Loss: 3.9473 (improvement: 0.0425)\n",
            "Epoch 17/150, Step 6200: Train Loss=2.5696, Val Loss=3.9692, LR=7.82e-05, Tokens=793,600, GradNorm=9.593\n",
            "Epoch 17/150, Step 6225: Train Loss=3.4260, Val Loss=3.9600, LR=7.82e-05, Tokens=796,800, GradNorm=10.512\n",
            "Epoch 17/150, Step 6250: Train Loss=3.0930, Val Loss=3.9152, LR=7.82e-05, Tokens=800,000, GradNorm=8.486\n",
            "New best model saved! Val Loss: 3.9152 (improvement: 0.0321)\n",
            "Epoch 17/150, Step 6275: Train Loss=2.9624, Val Loss=3.9696, LR=7.82e-05, Tokens=803,200, GradNorm=12.162\n",
            "Epoch 17/150, Step 6300: Train Loss=3.2466, Val Loss=3.9575, LR=7.82e-05, Tokens=806,400, GradNorm=10.122\n",
            "Epoch 17/150, Step 6325: Train Loss=2.8852, Val Loss=3.9772, LR=7.81e-05, Tokens=809,600, GradNorm=7.776\n",
            "Epoch 17/150, Step 6350: Train Loss=2.9747, Val Loss=4.0237, LR=7.81e-05, Tokens=812,800, GradNorm=8.801\n",
            "Epoch 17/150, Step 6375: Train Loss=3.6412, Val Loss=3.9985, LR=7.81e-05, Tokens=816,000, GradNorm=9.741\n",
            "Epoch 17/150, Step 6400: Train Loss=2.8660, Val Loss=3.9823, LR=7.81e-05, Tokens=819,200, GradNorm=8.803\n",
            "Epoch 17/150, Step 6425: Train Loss=3.1879, Val Loss=3.8480, LR=7.81e-05, Tokens=822,400, GradNorm=10.029\n",
            "New best model saved! Val Loss: 3.8480 (improvement: 0.0673)\n",
            "Epoch 17/150, Step 6450: Train Loss=2.9793, Val Loss=3.8884, LR=7.81e-05, Tokens=825,600, GradNorm=10.820\n",
            "Epoch 17/150, Step 6475: Train Loss=3.1368, Val Loss=3.9192, LR=7.80e-05, Tokens=828,800, GradNorm=9.865\n",
            "Epoch 17/150, Step 6500: Train Loss=2.9364, Val Loss=3.9416, LR=7.80e-05, Tokens=832,000, GradNorm=10.742\n",
            "Epoch 17/150, Step 6525: Train Loss=2.9616, Val Loss=3.9151, LR=7.80e-05, Tokens=835,200, GradNorm=11.060\n",
            "Epoch 17 completed. Average loss: 3.2348\n",
            "Epoch 18/150, Step 6550: Train Loss=3.0375, Val Loss=3.9385, LR=7.80e-05, Tokens=838,400, GradNorm=10.042\n",
            "Epoch 18/150, Step 6575: Train Loss=3.2153, Val Loss=3.8900, LR=7.80e-05, Tokens=841,600, GradNorm=9.475\n",
            "Epoch 18/150, Step 6600: Train Loss=3.2671, Val Loss=3.8911, LR=7.80e-05, Tokens=844,800, GradNorm=8.374\n",
            "Epoch 18/150, Step 6625: Train Loss=2.4792, Val Loss=3.8473, LR=7.79e-05, Tokens=848,000, GradNorm=10.865\n",
            "New best model saved! Val Loss: 3.8473 (improvement: 0.0007)\n",
            "Epoch 18/150, Step 6650: Train Loss=2.5741, Val Loss=3.8660, LR=7.79e-05, Tokens=851,200, GradNorm=7.598\n",
            "Epoch 18/150, Step 6675: Train Loss=2.3966, Val Loss=3.8225, LR=7.79e-05, Tokens=854,400, GradNorm=9.635\n",
            "New best model saved! Val Loss: 3.8225 (improvement: 0.0248)\n",
            "Epoch 18/150, Step 6700: Train Loss=2.9037, Val Loss=3.8508, LR=7.79e-05, Tokens=857,600, GradNorm=11.424\n",
            "Epoch 18/150, Step 6725: Train Loss=3.0689, Val Loss=3.8633, LR=7.79e-05, Tokens=860,800, GradNorm=10.995\n",
            "Epoch 18/150, Step 6750: Train Loss=2.5014, Val Loss=3.8399, LR=7.79e-05, Tokens=864,000, GradNorm=9.787\n",
            "Epoch 18/150, Step 6775: Train Loss=3.1144, Val Loss=3.8162, LR=7.78e-05, Tokens=867,200, GradNorm=12.083\n",
            "New best model saved! Val Loss: 3.8162 (improvement: 0.0063)\n",
            "Epoch 18/150, Step 6800: Train Loss=2.8841, Val Loss=3.7968, LR=7.78e-05, Tokens=870,400, GradNorm=14.477\n",
            "New best model saved! Val Loss: 3.7968 (improvement: 0.0194)\n",
            "Epoch 18/150, Step 6825: Train Loss=2.7144, Val Loss=3.8298, LR=7.78e-05, Tokens=873,600, GradNorm=10.541\n",
            "Epoch 18/150, Step 6850: Train Loss=2.7097, Val Loss=3.7929, LR=7.78e-05, Tokens=876,800, GradNorm=13.084\n",
            "New best model saved! Val Loss: 3.7929 (improvement: 0.0038)\n",
            "Epoch 18/150, Step 6875: Train Loss=3.0334, Val Loss=3.7710, LR=7.78e-05, Tokens=880,000, GradNorm=12.126\n",
            "New best model saved! Val Loss: 3.7710 (improvement: 0.0220)\n",
            "Epoch 18/150, Step 6900: Train Loss=2.9386, Val Loss=3.8080, LR=7.77e-05, Tokens=883,200, GradNorm=14.258\n",
            "Epoch 18 completed. Average loss: 3.0993\n",
            "Epoch 19/150, Step 6925: Train Loss=2.6839, Val Loss=3.7755, LR=7.77e-05, Tokens=886,400, GradNorm=13.138\n",
            "Epoch 19/150, Step 6950: Train Loss=2.7863, Val Loss=3.7716, LR=7.77e-05, Tokens=889,600, GradNorm=8.887\n",
            "Epoch 19/150, Step 6975: Train Loss=2.8093, Val Loss=3.7887, LR=7.77e-05, Tokens=892,800, GradNorm=8.629\n",
            "Epoch 19/150, Step 7000: Train Loss=2.8793, Val Loss=3.7621, LR=7.77e-05, Tokens=896,000, GradNorm=12.933\n",
            "New best model saved! Val Loss: 3.7621 (improvement: 0.0089)\n",
            "Epoch 19/150, Step 7025: Train Loss=2.8715, Val Loss=3.7237, LR=7.77e-05, Tokens=899,200, GradNorm=9.716\n",
            "New best model saved! Val Loss: 3.7237 (improvement: 0.0384)\n",
            "Epoch 19/150, Step 7050: Train Loss=3.1265, Val Loss=3.7426, LR=7.76e-05, Tokens=902,400, GradNorm=10.688\n",
            "Epoch 19/150, Step 7075: Train Loss=2.4266, Val Loss=3.7484, LR=7.76e-05, Tokens=905,600, GradNorm=8.970\n",
            "Epoch 19/150, Step 7100: Train Loss=3.0058, Val Loss=3.7985, LR=7.76e-05, Tokens=908,800, GradNorm=12.244\n",
            "Epoch 19/150, Step 7125: Train Loss=2.9450, Val Loss=3.7623, LR=7.76e-05, Tokens=912,000, GradNorm=12.912\n",
            "Epoch 19/150, Step 7150: Train Loss=2.7666, Val Loss=3.7737, LR=7.76e-05, Tokens=915,200, GradNorm=10.384\n",
            "Epoch 19/150, Step 7175: Train Loss=2.8182, Val Loss=3.7077, LR=7.75e-05, Tokens=918,400, GradNorm=10.143\n",
            "New best model saved! Val Loss: 3.7077 (improvement: 0.0160)\n",
            "Epoch 19/150, Step 7200: Train Loss=2.3736, Val Loss=3.7189, LR=7.75e-05, Tokens=921,600, GradNorm=12.059\n",
            "Epoch 19/150, Step 7225: Train Loss=3.3002, Val Loss=3.6939, LR=7.75e-05, Tokens=924,800, GradNorm=8.454\n",
            "New best model saved! Val Loss: 3.6939 (improvement: 0.0139)\n",
            "Epoch 19/150, Step 7250: Train Loss=2.3585, Val Loss=3.6922, LR=7.75e-05, Tokens=928,000, GradNorm=7.800\n",
            "New best model saved! Val Loss: 3.6922 (improvement: 0.0017)\n",
            "Epoch 19/150, Step 7275: Train Loss=2.6251, Val Loss=3.7090, LR=7.75e-05, Tokens=931,200, GradNorm=13.361\n",
            "Epoch 19 completed. Average loss: 2.9845\n",
            "Epoch 20/150, Step 7300: Train Loss=3.3777, Val Loss=3.7139, LR=7.74e-05, Tokens=934,400, GradNorm=12.935\n",
            "Epoch 20/150, Step 7325: Train Loss=2.7658, Val Loss=3.6863, LR=7.74e-05, Tokens=937,600, GradNorm=13.471\n",
            "New best model saved! Val Loss: 3.6863 (improvement: 0.0059)\n",
            "Epoch 20/150, Step 7350: Train Loss=2.5322, Val Loss=3.6962, LR=7.74e-05, Tokens=940,800, GradNorm=9.286\n",
            "Epoch 20/150, Step 7375: Train Loss=2.8597, Val Loss=3.6868, LR=7.74e-05, Tokens=944,000, GradNorm=12.393\n",
            "Epoch 20/150, Step 7400: Train Loss=3.0926, Val Loss=3.6556, LR=7.74e-05, Tokens=947,200, GradNorm=9.365\n",
            "New best model saved! Val Loss: 3.6556 (improvement: 0.0308)\n",
            "Epoch 20/150, Step 7425: Train Loss=2.5877, Val Loss=3.6412, LR=7.73e-05, Tokens=950,400, GradNorm=13.476\n",
            "New best model saved! Val Loss: 3.6412 (improvement: 0.0143)\n",
            "Epoch 20/150, Step 7450: Train Loss=2.6129, Val Loss=3.6448, LR=7.73e-05, Tokens=953,600, GradNorm=11.600\n",
            "Epoch 20/150, Step 7475: Train Loss=2.6551, Val Loss=3.6644, LR=7.73e-05, Tokens=956,800, GradNorm=13.181\n",
            "Epoch 20/150, Step 7500: Train Loss=2.4861, Val Loss=3.6131, LR=7.73e-05, Tokens=960,000, GradNorm=11.716\n",
            "New best model saved! Val Loss: 3.6131 (improvement: 0.0281)\n",
            "Epoch 20/150, Step 7525: Train Loss=2.8374, Val Loss=3.5766, LR=7.73e-05, Tokens=963,200, GradNorm=9.530\n",
            "New best model saved! Val Loss: 3.5766 (improvement: 0.0365)\n",
            "Epoch 20/150, Step 7550: Train Loss=2.2427, Val Loss=3.5738, LR=7.72e-05, Tokens=966,400, GradNorm=10.589\n",
            "New best model saved! Val Loss: 3.5738 (improvement: 0.0029)\n",
            "Epoch 20/150, Step 7575: Train Loss=2.3883, Val Loss=3.5466, LR=7.72e-05, Tokens=969,600, GradNorm=12.838\n",
            "New best model saved! Val Loss: 3.5466 (improvement: 0.0272)\n",
            "Epoch 20/150, Step 7600: Train Loss=2.1528, Val Loss=3.6015, LR=7.72e-05, Tokens=972,800, GradNorm=12.006\n",
            "Epoch 20/150, Step 7625: Train Loss=2.6309, Val Loss=3.6105, LR=7.72e-05, Tokens=976,000, GradNorm=11.294\n",
            "Epoch 20/150, Step 7650: Train Loss=2.5794, Val Loss=3.5954, LR=7.72e-05, Tokens=979,200, GradNorm=10.657\n",
            "Epoch 20/150, Step 7675: Train Loss=2.5378, Val Loss=3.5629, LR=7.71e-05, Tokens=982,400, GradNorm=10.662\n",
            "Epoch 20 completed. Average loss: 2.8549\n",
            "Sample generation after epoch 20:\n",
            "This movie in earth could haveI dont ah And neverThe little somehow Para, and time isrec waves All she-woo,ima Every tear Oh, just good, if its all the world, we want something you cant be of its lost But I know you  be tired,When you But joy, we fly and wasn You want ringing  stars P\n",
            "------------------------------------------------------------\n",
            "Epoch 21/150, Step 7700: Train Loss=2.4184, Val Loss=3.5509, LR=7.71e-05, Tokens=985,600, GradNorm=10.947\n",
            "Epoch 21/150, Step 7725: Train Loss=2.2110, Val Loss=3.6132, LR=7.71e-05, Tokens=988,800, GradNorm=9.354\n",
            "Epoch 21/150, Step 7750: Train Loss=2.5593, Val Loss=3.5534, LR=7.71e-05, Tokens=992,000, GradNorm=10.256\n",
            "Epoch 21/150, Step 7775: Train Loss=2.8075, Val Loss=3.5436, LR=7.71e-05, Tokens=995,200, GradNorm=9.662\n",
            "New best model saved! Val Loss: 3.5436 (improvement: 0.0030)\n",
            "Epoch 21/150, Step 7800: Train Loss=2.1816, Val Loss=3.5639, LR=7.70e-05, Tokens=998,400, GradNorm=11.396\n",
            "Epoch 21/150, Step 7825: Train Loss=2.8772, Val Loss=3.5444, LR=7.70e-05, Tokens=1,001,600, GradNorm=15.091\n",
            "Epoch 21/150, Step 7850: Train Loss=2.0092, Val Loss=3.5918, LR=7.70e-05, Tokens=1,004,800, GradNorm=8.734\n",
            "Epoch 21/150, Step 7875: Train Loss=2.2710, Val Loss=3.5798, LR=7.70e-05, Tokens=1,008,000, GradNorm=12.784\n",
            "Epoch 21/150, Step 7900: Train Loss=2.3725, Val Loss=3.5196, LR=7.70e-05, Tokens=1,011,200, GradNorm=12.729\n",
            "New best model saved! Val Loss: 3.5196 (improvement: 0.0241)\n",
            "Epoch 21/150, Step 7925: Train Loss=2.2571, Val Loss=3.5022, LR=7.69e-05, Tokens=1,014,400, GradNorm=7.843\n",
            "New best model saved! Val Loss: 3.5022 (improvement: 0.0174)\n",
            "Epoch 21/150, Step 7950: Train Loss=2.1676, Val Loss=3.5178, LR=7.69e-05, Tokens=1,017,600, GradNorm=12.173\n",
            "Epoch 21/150, Step 7975: Train Loss=2.2111, Val Loss=3.4356, LR=7.69e-05, Tokens=1,020,800, GradNorm=10.924\n",
            "New best model saved! Val Loss: 3.4356 (improvement: 0.0666)\n",
            "Epoch 21/150, Step 8000: Train Loss=2.3360, Val Loss=3.4270, LR=7.69e-05, Tokens=1,024,000, GradNorm=13.797\n",
            "New best model saved! Val Loss: 3.4270 (improvement: 0.0086)\n",
            "Epoch 21/150, Step 8025: Train Loss=2.2345, Val Loss=3.4998, LR=7.68e-05, Tokens=1,027,200, GradNorm=11.641\n",
            "Epoch 21/150, Step 8050: Train Loss=2.6219, Val Loss=3.4840, LR=7.68e-05, Tokens=1,030,400, GradNorm=10.236\n",
            "Epoch 21 completed. Average loss: 2.7237\n",
            "Epoch 22/150, Step 8075: Train Loss=2.7491, Val Loss=3.4408, LR=7.68e-05, Tokens=1,033,600, GradNorm=10.307\n",
            "Epoch 22/150, Step 8100: Train Loss=2.1397, Val Loss=3.4561, LR=7.68e-05, Tokens=1,036,800, GradNorm=12.681\n",
            "Epoch 22/150, Step 8125: Train Loss=2.3880, Val Loss=3.5199, LR=7.68e-05, Tokens=1,040,000, GradNorm=9.138\n",
            "Epoch 22/150, Step 8150: Train Loss=2.3761, Val Loss=3.4564, LR=7.67e-05, Tokens=1,043,200, GradNorm=10.877\n",
            "Epoch 22/150, Step 8175: Train Loss=2.1652, Val Loss=3.4521, LR=7.67e-05, Tokens=1,046,400, GradNorm=11.261\n",
            "Epoch 22/150, Step 8200: Train Loss=2.5518, Val Loss=3.4527, LR=7.67e-05, Tokens=1,049,600, GradNorm=9.597\n",
            "Epoch 22/150, Step 8225: Train Loss=2.6713, Val Loss=3.4756, LR=7.67e-05, Tokens=1,052,800, GradNorm=12.972\n",
            "Epoch 22/150, Step 8250: Train Loss=2.2168, Val Loss=3.4692, LR=7.67e-05, Tokens=1,056,000, GradNorm=12.067\n",
            "Epoch 22/150, Step 8275: Train Loss=1.8670, Val Loss=3.4868, LR=7.66e-05, Tokens=1,059,200, GradNorm=13.890\n",
            "Epoch 22/150, Step 8300: Train Loss=2.2070, Val Loss=3.4215, LR=7.66e-05, Tokens=1,062,400, GradNorm=11.104\n",
            "New best model saved! Val Loss: 3.4215 (improvement: 0.0055)\n",
            "Epoch 22/150, Step 8325: Train Loss=2.1491, Val Loss=3.4526, LR=7.66e-05, Tokens=1,065,600, GradNorm=12.485\n",
            "Epoch 22/150, Step 8350: Train Loss=2.5557, Val Loss=3.4526, LR=7.66e-05, Tokens=1,068,800, GradNorm=14.347\n",
            "Epoch 22/150, Step 8375: Train Loss=2.7872, Val Loss=3.4458, LR=7.65e-05, Tokens=1,072,000, GradNorm=10.842\n",
            "Epoch 22/150, Step 8400: Train Loss=2.3149, Val Loss=3.4226, LR=7.65e-05, Tokens=1,075,200, GradNorm=9.340\n",
            "Epoch 22/150, Step 8425: Train Loss=2.5290, Val Loss=3.4044, LR=7.65e-05, Tokens=1,078,400, GradNorm=14.187\n",
            "New best model saved! Val Loss: 3.4044 (improvement: 0.0171)\n",
            "Epoch 22 completed. Average loss: 2.6063\n",
            "Epoch 23/150, Step 8450: Train Loss=1.8016, Val Loss=3.3917, LR=7.65e-05, Tokens=1,081,600, GradNorm=12.676\n",
            "New best model saved! Val Loss: 3.3917 (improvement: 0.0126)\n",
            "Epoch 23/150, Step 8475: Train Loss=2.0534, Val Loss=3.3567, LR=7.64e-05, Tokens=1,084,800, GradNorm=10.845\n",
            "New best model saved! Val Loss: 3.3567 (improvement: 0.0350)\n",
            "Epoch 23/150, Step 8500: Train Loss=2.3159, Val Loss=3.3724, LR=7.64e-05, Tokens=1,088,000, GradNorm=10.317\n",
            "Epoch 23/150, Step 8525: Train Loss=1.8499, Val Loss=3.4414, LR=7.64e-05, Tokens=1,091,200, GradNorm=12.637\n",
            "Epoch 23/150, Step 8550: Train Loss=2.4333, Val Loss=3.4182, LR=7.64e-05, Tokens=1,094,400, GradNorm=14.103\n",
            "Epoch 23/150, Step 8575: Train Loss=1.9827, Val Loss=3.4329, LR=7.64e-05, Tokens=1,097,600, GradNorm=9.904\n",
            "Epoch 23/150, Step 8600: Train Loss=2.4187, Val Loss=3.3869, LR=7.63e-05, Tokens=1,100,800, GradNorm=11.844\n",
            "Epoch 23/150, Step 8625: Train Loss=1.9951, Val Loss=3.3511, LR=7.63e-05, Tokens=1,104,000, GradNorm=14.028\n",
            "New best model saved! Val Loss: 3.3511 (improvement: 0.0056)\n",
            "Epoch 23/150, Step 8650: Train Loss=2.2427, Val Loss=3.4025, LR=7.63e-05, Tokens=1,107,200, GradNorm=11.554\n",
            "Epoch 23/150, Step 8675: Train Loss=2.1123, Val Loss=3.3866, LR=7.63e-05, Tokens=1,110,400, GradNorm=12.608\n",
            "Epoch 23/150, Step 8700: Train Loss=2.6119, Val Loss=3.3369, LR=7.62e-05, Tokens=1,113,600, GradNorm=11.722\n",
            "New best model saved! Val Loss: 3.3369 (improvement: 0.0142)\n",
            "Epoch 23/150, Step 8725: Train Loss=2.1439, Val Loss=3.4137, LR=7.62e-05, Tokens=1,116,800, GradNorm=11.085\n",
            "Epoch 23/150, Step 8750: Train Loss=1.9462, Val Loss=3.3999, LR=7.62e-05, Tokens=1,120,000, GradNorm=10.098\n",
            "Epoch 23/150, Step 8775: Train Loss=2.2058, Val Loss=3.3276, LR=7.62e-05, Tokens=1,123,200, GradNorm=13.967\n",
            "New best model saved! Val Loss: 3.3276 (improvement: 0.0093)\n",
            "Epoch 23/150, Step 8800: Train Loss=2.3054, Val Loss=3.2813, LR=7.61e-05, Tokens=1,126,400, GradNorm=11.656\n",
            "New best model saved! Val Loss: 3.2813 (improvement: 0.0464)\n",
            "Epoch 23/150, Step 8825: Train Loss=1.9517, Val Loss=3.2642, LR=7.61e-05, Tokens=1,129,600, GradNorm=10.947\n",
            "New best model saved! Val Loss: 3.2642 (improvement: 0.0171)\n",
            "Epoch 23 completed. Average loss: 2.4894\n",
            "Epoch 24/150, Step 8850: Train Loss=2.2479, Val Loss=3.2864, LR=7.61e-05, Tokens=1,132,800, GradNorm=10.029\n",
            "Epoch 24/150, Step 8875: Train Loss=2.2907, Val Loss=3.2416, LR=7.61e-05, Tokens=1,136,000, GradNorm=11.570\n",
            "New best model saved! Val Loss: 3.2416 (improvement: 0.0226)\n",
            "Epoch 24/150, Step 8900: Train Loss=2.1390, Val Loss=3.2619, LR=7.61e-05, Tokens=1,139,200, GradNorm=14.348\n",
            "Epoch 24/150, Step 8925: Train Loss=2.2025, Val Loss=3.3035, LR=7.60e-05, Tokens=1,142,400, GradNorm=14.614\n",
            "Epoch 24/150, Step 8950: Train Loss=2.3088, Val Loss=3.2569, LR=7.60e-05, Tokens=1,145,600, GradNorm=10.461\n",
            "Epoch 24/150, Step 8975: Train Loss=2.1737, Val Loss=3.3032, LR=7.60e-05, Tokens=1,148,800, GradNorm=10.033\n",
            "Epoch 24/150, Step 9000: Train Loss=2.4703, Val Loss=3.2122, LR=7.60e-05, Tokens=1,152,000, GradNorm=12.087\n",
            "New best model saved! Val Loss: 3.2122 (improvement: 0.0295)\n",
            "Epoch 24/150, Step 9025: Train Loss=2.0729, Val Loss=3.2390, LR=7.59e-05, Tokens=1,155,200, GradNorm=12.519\n",
            "Epoch 24/150, Step 9050: Train Loss=2.0876, Val Loss=3.2422, LR=7.59e-05, Tokens=1,158,400, GradNorm=12.894\n",
            "Epoch 24/150, Step 9075: Train Loss=2.5499, Val Loss=3.2449, LR=7.59e-05, Tokens=1,161,600, GradNorm=13.231\n",
            "Epoch 24/150, Step 9100: Train Loss=1.9963, Val Loss=3.2677, LR=7.59e-05, Tokens=1,164,800, GradNorm=10.875\n",
            "Epoch 24/150, Step 9125: Train Loss=1.7312, Val Loss=3.2970, LR=7.58e-05, Tokens=1,168,000, GradNorm=14.093\n",
            "Epoch 24/150, Step 9150: Train Loss=2.1828, Val Loss=3.2248, LR=7.58e-05, Tokens=1,171,200, GradNorm=13.055\n",
            "Epoch 24/150, Step 9175: Train Loss=2.0510, Val Loss=3.2247, LR=7.58e-05, Tokens=1,174,400, GradNorm=13.149\n",
            "Epoch 24/150, Step 9200: Train Loss=1.8348, Val Loss=3.1799, LR=7.58e-05, Tokens=1,177,600, GradNorm=10.305\n",
            "New best model saved! Val Loss: 3.1799 (improvement: 0.0323)\n",
            "Epoch 24 completed. Average loss: 2.3791\n",
            "Epoch 25/150, Step 9225: Train Loss=2.0296, Val Loss=3.1810, LR=7.57e-05, Tokens=1,180,800, GradNorm=13.136\n",
            "Epoch 25/150, Step 9250: Train Loss=2.4456, Val Loss=3.1860, LR=7.57e-05, Tokens=1,184,000, GradNorm=13.926\n",
            "Epoch 25/150, Step 9275: Train Loss=1.7794, Val Loss=3.1279, LR=7.57e-05, Tokens=1,187,200, GradNorm=13.293\n",
            "New best model saved! Val Loss: 3.1279 (improvement: 0.0520)\n",
            "Epoch 25/150, Step 9300: Train Loss=2.3976, Val Loss=3.1322, LR=7.57e-05, Tokens=1,190,400, GradNorm=13.233\n",
            "Epoch 25/150, Step 9325: Train Loss=2.1840, Val Loss=3.1420, LR=7.56e-05, Tokens=1,193,600, GradNorm=16.487\n",
            "Epoch 25/150, Step 9350: Train Loss=2.2873, Val Loss=3.0624, LR=7.56e-05, Tokens=1,196,800, GradNorm=11.830\n",
            "New best model saved! Val Loss: 3.0624 (improvement: 0.0655)\n",
            "Epoch 25/150, Step 9375: Train Loss=1.9183, Val Loss=3.1012, LR=7.56e-05, Tokens=1,200,000, GradNorm=12.386\n",
            "Epoch 25/150, Step 9400: Train Loss=2.2015, Val Loss=3.0800, LR=7.56e-05, Tokens=1,203,200, GradNorm=14.314\n",
            "Epoch 25/150, Step 9425: Train Loss=1.7922, Val Loss=3.1213, LR=7.55e-05, Tokens=1,206,400, GradNorm=9.121\n",
            "Epoch 25/150, Step 9450: Train Loss=1.9830, Val Loss=3.1749, LR=7.55e-05, Tokens=1,209,600, GradNorm=12.961\n",
            "Epoch 25/150, Step 9475: Train Loss=2.7902, Val Loss=3.1055, LR=7.55e-05, Tokens=1,212,800, GradNorm=13.084\n",
            "Epoch 25/150, Step 9500: Train Loss=1.9304, Val Loss=3.0872, LR=7.55e-05, Tokens=1,216,000, GradNorm=13.615\n",
            "Epoch 25/150, Step 9525: Train Loss=1.6498, Val Loss=3.1138, LR=7.54e-05, Tokens=1,219,200, GradNorm=14.545\n",
            "Epoch 25/150, Step 9550: Train Loss=1.9789, Val Loss=3.1164, LR=7.54e-05, Tokens=1,222,400, GradNorm=11.082\n",
            "Epoch 25/150, Step 9575: Train Loss=2.7554, Val Loss=3.0918, LR=7.54e-05, Tokens=1,225,600, GradNorm=17.478\n",
            "Epoch 25/150, Step 9600: Train Loss=2.0319, Val Loss=3.0928, LR=7.54e-05, Tokens=1,228,800, GradNorm=10.980\n",
            "Epoch 25 completed. Average loss: 2.2651\n",
            "Epoch 26/150, Step 9625: Train Loss=1.6163, Val Loss=3.0917, LR=7.53e-05, Tokens=1,232,000, GradNorm=10.084\n",
            "Epoch 26/150, Step 9650: Train Loss=1.9637, Val Loss=3.0523, LR=7.53e-05, Tokens=1,235,200, GradNorm=11.145\n",
            "New best model saved! Val Loss: 3.0523 (improvement: 0.0101)\n",
            "Epoch 26/150, Step 9675: Train Loss=1.7177, Val Loss=3.0676, LR=7.53e-05, Tokens=1,238,400, GradNorm=14.635\n",
            "Epoch 26/150, Step 9700: Train Loss=1.8432, Val Loss=3.0760, LR=7.53e-05, Tokens=1,241,600, GradNorm=14.403\n",
            "Epoch 26/150, Step 9725: Train Loss=2.0004, Val Loss=3.1039, LR=7.52e-05, Tokens=1,244,800, GradNorm=11.717\n",
            "Epoch 26/150, Step 9750: Train Loss=1.7765, Val Loss=3.1548, LR=7.52e-05, Tokens=1,248,000, GradNorm=12.686\n",
            "Epoch 26/150, Step 9775: Train Loss=2.1973, Val Loss=3.1005, LR=7.52e-05, Tokens=1,251,200, GradNorm=15.450\n",
            "Epoch 26/150, Step 9800: Train Loss=1.3795, Val Loss=3.0594, LR=7.51e-05, Tokens=1,254,400, GradNorm=8.568\n",
            "Epoch 26/150, Step 9825: Train Loss=2.0655, Val Loss=3.0594, LR=7.51e-05, Tokens=1,257,600, GradNorm=12.496\n",
            "Epoch 26/150, Step 9850: Train Loss=1.5051, Val Loss=3.0338, LR=7.51e-05, Tokens=1,260,800, GradNorm=13.847\n",
            "New best model saved! Val Loss: 3.0338 (improvement: 0.0185)\n",
            "Epoch 26/150, Step 9875: Train Loss=2.3897, Val Loss=3.0654, LR=7.51e-05, Tokens=1,264,000, GradNorm=12.566\n",
            "Epoch 26/150, Step 9900: Train Loss=1.6417, Val Loss=3.0420, LR=7.50e-05, Tokens=1,267,200, GradNorm=10.671\n",
            "Epoch 26/150, Step 9925: Train Loss=1.9427, Val Loss=3.0305, LR=7.50e-05, Tokens=1,270,400, GradNorm=11.664\n",
            "New best model saved! Val Loss: 3.0305 (improvement: 0.0034)\n",
            "Epoch 26/150, Step 9950: Train Loss=1.8807, Val Loss=2.9913, LR=7.50e-05, Tokens=1,273,600, GradNorm=11.513\n",
            "New best model saved! Val Loss: 2.9913 (improvement: 0.0392)\n",
            "Epoch 26/150, Step 9975: Train Loss=1.8716, Val Loss=2.9946, LR=7.50e-05, Tokens=1,276,800, GradNorm=12.178\n",
            "Epoch 26 completed. Average loss: 2.1501\n",
            "Epoch 27/150, Step 10000: Train Loss=2.1460, Val Loss=2.9578, LR=7.49e-05, Tokens=1,280,000, GradNorm=11.556\n",
            "New best model saved! Val Loss: 2.9578 (improvement: 0.0334)\n",
            "Epoch 27/150, Step 10025: Train Loss=2.0630, Val Loss=3.0016, LR=7.49e-05, Tokens=1,283,200, GradNorm=13.824\n",
            "Epoch 27/150, Step 10050: Train Loss=2.0454, Val Loss=3.0072, LR=7.49e-05, Tokens=1,286,400, GradNorm=11.987\n",
            "Epoch 27/150, Step 10075: Train Loss=1.7605, Val Loss=3.0145, LR=7.49e-05, Tokens=1,289,600, GradNorm=10.203\n",
            "Epoch 27/150, Step 10100: Train Loss=1.9631, Val Loss=2.9888, LR=7.48e-05, Tokens=1,292,800, GradNorm=16.162\n",
            "Epoch 27/150, Step 10125: Train Loss=1.8446, Val Loss=2.9890, LR=7.48e-05, Tokens=1,296,000, GradNorm=10.383\n",
            "Epoch 27/150, Step 10150: Train Loss=1.8128, Val Loss=2.9968, LR=7.48e-05, Tokens=1,299,200, GradNorm=9.329\n",
            "Epoch 27/150, Step 10175: Train Loss=1.9725, Val Loss=3.0032, LR=7.47e-05, Tokens=1,302,400, GradNorm=12.352\n",
            "Epoch 27/150, Step 10200: Train Loss=1.7069, Val Loss=3.0248, LR=7.47e-05, Tokens=1,305,600, GradNorm=13.563\n",
            "Epoch 27/150, Step 10225: Train Loss=2.2283, Val Loss=3.0147, LR=7.47e-05, Tokens=1,308,800, GradNorm=13.846\n",
            "Epoch 27/150, Step 10250: Train Loss=2.0717, Val Loss=2.9893, LR=7.47e-05, Tokens=1,312,000, GradNorm=13.171\n",
            "Epoch 27/150, Step 10275: Train Loss=1.9535, Val Loss=2.9329, LR=7.46e-05, Tokens=1,315,200, GradNorm=14.421\n",
            "New best model saved! Val Loss: 2.9329 (improvement: 0.0249)\n",
            "Epoch 27/150, Step 10300: Train Loss=1.8105, Val Loss=2.8911, LR=7.46e-05, Tokens=1,318,400, GradNorm=10.423\n",
            "New best model saved! Val Loss: 2.8911 (improvement: 0.0419)\n",
            "Epoch 27/150, Step 10325: Train Loss=1.8116, Val Loss=2.9113, LR=7.46e-05, Tokens=1,321,600, GradNorm=11.963\n",
            "Epoch 27/150, Step 10350: Train Loss=2.1429, Val Loss=2.9450, LR=7.45e-05, Tokens=1,324,800, GradNorm=14.847\n",
            "Epoch 27 completed. Average loss: 2.0436\n",
            "Epoch 28/150, Step 10375: Train Loss=1.7977, Val Loss=2.8513, LR=7.45e-05, Tokens=1,328,000, GradNorm=11.438\n",
            "New best model saved! Val Loss: 2.8513 (improvement: 0.0397)\n",
            "Epoch 28/150, Step 10400: Train Loss=2.1613, Val Loss=2.8311, LR=7.45e-05, Tokens=1,331,200, GradNorm=14.364\n",
            "New best model saved! Val Loss: 2.8311 (improvement: 0.0202)\n",
            "Epoch 28/150, Step 10425: Train Loss=2.0207, Val Loss=2.8468, LR=7.45e-05, Tokens=1,334,400, GradNorm=12.446\n",
            "Epoch 28/150, Step 10450: Train Loss=1.8369, Val Loss=2.8858, LR=7.44e-05, Tokens=1,337,600, GradNorm=12.180\n",
            "Epoch 28/150, Step 10475: Train Loss=1.7260, Val Loss=2.8630, LR=7.44e-05, Tokens=1,340,800, GradNorm=13.951\n",
            "Epoch 28/150, Step 10500: Train Loss=2.0311, Val Loss=2.8669, LR=7.44e-05, Tokens=1,344,000, GradNorm=13.450\n",
            "Epoch 28/150, Step 10525: Train Loss=1.9480, Val Loss=2.8600, LR=7.44e-05, Tokens=1,347,200, GradNorm=10.204\n",
            "Epoch 28/150, Step 10550: Train Loss=1.7239, Val Loss=2.8725, LR=7.43e-05, Tokens=1,350,400, GradNorm=13.306\n",
            "Epoch 28/150, Step 10575: Train Loss=1.6793, Val Loss=2.8530, LR=7.43e-05, Tokens=1,353,600, GradNorm=10.847\n",
            "Epoch 28/150, Step 10600: Train Loss=1.7528, Val Loss=2.8321, LR=7.43e-05, Tokens=1,356,800, GradNorm=11.616\n",
            "Epoch 28/150, Step 10625: Train Loss=2.0062, Val Loss=2.9096, LR=7.42e-05, Tokens=1,360,000, GradNorm=9.844\n",
            "Epoch 28/150, Step 10650: Train Loss=1.5486, Val Loss=2.8354, LR=7.42e-05, Tokens=1,363,200, GradNorm=12.782\n",
            "Epoch 28/150, Step 10675: Train Loss=1.6666, Val Loss=2.8288, LR=7.42e-05, Tokens=1,366,400, GradNorm=15.243\n",
            "New best model saved! Val Loss: 2.8288 (improvement: 0.0023)\n",
            "Epoch 28/150, Step 10700: Train Loss=1.3845, Val Loss=2.8523, LR=7.42e-05, Tokens=1,369,600, GradNorm=9.946\n",
            "Epoch 28/150, Step 10725: Train Loss=1.8859, Val Loss=2.8044, LR=7.41e-05, Tokens=1,372,800, GradNorm=9.458\n",
            "New best model saved! Val Loss: 2.8044 (improvement: 0.0245)\n",
            "Epoch 28/150, Step 10750: Train Loss=1.6769, Val Loss=2.7771, LR=7.41e-05, Tokens=1,376,000, GradNorm=15.060\n",
            "New best model saved! Val Loss: 2.7771 (improvement: 0.0273)\n",
            "Epoch 28 completed. Average loss: 1.9426\n",
            "Epoch 29/150, Step 10775: Train Loss=1.9217, Val Loss=2.7865, LR=7.41e-05, Tokens=1,379,200, GradNorm=12.592\n",
            "Epoch 29/150, Step 10800: Train Loss=1.2432, Val Loss=2.7669, LR=7.40e-05, Tokens=1,382,400, GradNorm=13.029\n",
            "New best model saved! Val Loss: 2.7669 (improvement: 0.0101)\n",
            "Epoch 29/150, Step 10825: Train Loss=1.4443, Val Loss=2.7736, LR=7.40e-05, Tokens=1,385,600, GradNorm=16.682\n",
            "Epoch 29/150, Step 10850: Train Loss=1.2780, Val Loss=2.8443, LR=7.40e-05, Tokens=1,388,800, GradNorm=8.608\n",
            "Epoch 29/150, Step 10875: Train Loss=1.6003, Val Loss=2.8176, LR=7.39e-05, Tokens=1,392,000, GradNorm=13.709\n",
            "Epoch 29/150, Step 10900: Train Loss=1.0118, Val Loss=2.8523, LR=7.39e-05, Tokens=1,395,200, GradNorm=14.034\n",
            "Epoch 29/150, Step 10925: Train Loss=1.6525, Val Loss=2.7883, LR=7.39e-05, Tokens=1,398,400, GradNorm=15.433\n",
            "Epoch 29/150, Step 10950: Train Loss=1.6931, Val Loss=2.7616, LR=7.39e-05, Tokens=1,401,600, GradNorm=12.219\n",
            "New best model saved! Val Loss: 2.7616 (improvement: 0.0053)\n",
            "Epoch 29/150, Step 10975: Train Loss=1.5585, Val Loss=2.6998, LR=7.38e-05, Tokens=1,404,800, GradNorm=11.319\n",
            "New best model saved! Val Loss: 2.6998 (improvement: 0.0618)\n",
            "Epoch 29/150, Step 11000: Train Loss=1.1539, Val Loss=2.6805, LR=7.38e-05, Tokens=1,408,000, GradNorm=12.501\n",
            "New best model saved! Val Loss: 2.6805 (improvement: 0.0193)\n",
            "Epoch 29/150, Step 11025: Train Loss=1.7904, Val Loss=2.7550, LR=7.38e-05, Tokens=1,411,200, GradNorm=12.456\n",
            "Epoch 29/150, Step 11050: Train Loss=1.6777, Val Loss=2.7080, LR=7.37e-05, Tokens=1,414,400, GradNorm=16.663\n",
            "Epoch 29/150, Step 11075: Train Loss=1.5353, Val Loss=2.7193, LR=7.37e-05, Tokens=1,417,600, GradNorm=22.896\n",
            "Epoch 29/150, Step 11100: Train Loss=1.7665, Val Loss=2.7064, LR=7.37e-05, Tokens=1,420,800, GradNorm=16.783\n",
            "Epoch 29/150, Step 11125: Train Loss=1.6893, Val Loss=2.7003, LR=7.37e-05, Tokens=1,424,000, GradNorm=14.921\n",
            "Epoch 29 completed. Average loss: 1.8450\n",
            "Epoch 30/150, Step 11150: Train Loss=1.4516, Val Loss=2.6297, LR=7.36e-05, Tokens=1,427,200, GradNorm=14.348\n",
            "New best model saved! Val Loss: 2.6297 (improvement: 0.0508)\n",
            "Epoch 30/150, Step 11175: Train Loss=1.4192, Val Loss=2.6520, LR=7.36e-05, Tokens=1,430,400, GradNorm=11.224\n",
            "Epoch 30/150, Step 11200: Train Loss=1.6712, Val Loss=2.6738, LR=7.36e-05, Tokens=1,433,600, GradNorm=13.188\n",
            "Epoch 30/150, Step 11225: Train Loss=1.5062, Val Loss=2.6992, LR=7.35e-05, Tokens=1,436,800, GradNorm=13.664\n",
            "Epoch 30/150, Step 11250: Train Loss=1.0296, Val Loss=2.7176, LR=7.35e-05, Tokens=1,440,000, GradNorm=11.767\n",
            "Epoch 30/150, Step 11275: Train Loss=1.6874, Val Loss=2.6634, LR=7.35e-05, Tokens=1,443,200, GradNorm=8.222\n",
            "Epoch 30/150, Step 11300: Train Loss=1.3998, Val Loss=2.6698, LR=7.34e-05, Tokens=1,446,400, GradNorm=9.244\n",
            "Epoch 30/150, Step 11325: Train Loss=1.3683, Val Loss=2.6717, LR=7.34e-05, Tokens=1,449,600, GradNorm=14.448\n",
            "Epoch 30/150, Step 11350: Train Loss=1.3540, Val Loss=2.7497, LR=7.34e-05, Tokens=1,452,800, GradNorm=16.435\n",
            "Epoch 30/150, Step 11375: Train Loss=1.0803, Val Loss=2.7041, LR=7.34e-05, Tokens=1,456,000, GradNorm=11.791\n",
            "Epoch 30/150, Step 11400: Train Loss=1.7111, Val Loss=2.6495, LR=7.33e-05, Tokens=1,459,200, GradNorm=17.575\n",
            "Epoch 30/150, Step 11425: Train Loss=1.4827, Val Loss=2.6051, LR=7.33e-05, Tokens=1,462,400, GradNorm=14.108\n",
            "New best model saved! Val Loss: 2.6051 (improvement: 0.0245)\n",
            "Epoch 30/150, Step 11450: Train Loss=1.4719, Val Loss=2.6890, LR=7.33e-05, Tokens=1,465,600, GradNorm=12.949\n",
            "Epoch 30/150, Step 11475: Train Loss=1.1029, Val Loss=2.6335, LR=7.32e-05, Tokens=1,468,800, GradNorm=11.646\n",
            "Epoch 30/150, Step 11500: Train Loss=1.5407, Val Loss=2.6056, LR=7.32e-05, Tokens=1,472,000, GradNorm=13.164\n",
            "Epoch 30 completed. Average loss: 1.7498\n",
            "Sample generation after epoch 30:\n",
            "This movie askyrollk Till the brought itNot own fors I could see flood Give me in theakis polit had anise Oh, rise  red, feeling a dream that been underneath the world, still the guideWhen I some blood to be some sinking like Oh, thought of theirony So grace  him, nBecause field That betweenAndover thatOn\n",
            "------------------------------------------------------------\n",
            "Epoch 31/150, Step 11525: Train Loss=1.3275, Val Loss=2.6097, LR=7.32e-05, Tokens=1,475,200, GradNorm=13.641\n",
            "Epoch 31/150, Step 11550: Train Loss=1.1863, Val Loss=2.5865, LR=7.31e-05, Tokens=1,478,400, GradNorm=13.411\n",
            "New best model saved! Val Loss: 2.5865 (improvement: 0.0187)\n",
            "Epoch 31/150, Step 11575: Train Loss=1.5204, Val Loss=2.5911, LR=7.31e-05, Tokens=1,481,600, GradNorm=12.483\n",
            "Epoch 31/150, Step 11600: Train Loss=1.7061, Val Loss=2.5670, LR=7.31e-05, Tokens=1,484,800, GradNorm=10.694\n",
            "New best model saved! Val Loss: 2.5670 (improvement: 0.0195)\n",
            "Epoch 31/150, Step 11625: Train Loss=1.7550, Val Loss=2.5627, LR=7.30e-05, Tokens=1,488,000, GradNorm=14.903\n",
            "New best model saved! Val Loss: 2.5627 (improvement: 0.0042)\n",
            "Epoch 31/150, Step 11650: Train Loss=1.5658, Val Loss=2.5517, LR=7.30e-05, Tokens=1,491,200, GradNorm=12.189\n",
            "New best model saved! Val Loss: 2.5517 (improvement: 0.0111)\n",
            "Epoch 31/150, Step 11675: Train Loss=1.2504, Val Loss=2.5616, LR=7.30e-05, Tokens=1,494,400, GradNorm=14.128\n",
            "Epoch 31/150, Step 11700: Train Loss=1.5212, Val Loss=2.5382, LR=7.29e-05, Tokens=1,497,600, GradNorm=13.423\n",
            "New best model saved! Val Loss: 2.5382 (improvement: 0.0134)\n",
            "Epoch 31/150, Step 11725: Train Loss=1.8935, Val Loss=2.5614, LR=7.29e-05, Tokens=1,500,800, GradNorm=11.085\n",
            "Epoch 31/150, Step 11750: Train Loss=0.9917, Val Loss=2.5245, LR=7.29e-05, Tokens=1,504,000, GradNorm=15.039\n",
            "New best model saved! Val Loss: 2.5245 (improvement: 0.0137)\n",
            "Epoch 31/150, Step 11775: Train Loss=1.5661, Val Loss=2.5264, LR=7.29e-05, Tokens=1,507,200, GradNorm=10.775\n",
            "Epoch 31/150, Step 11800: Train Loss=1.3529, Val Loss=2.5135, LR=7.28e-05, Tokens=1,510,400, GradNorm=14.109\n",
            "New best model saved! Val Loss: 2.5135 (improvement: 0.0111)\n",
            "Epoch 31/150, Step 11825: Train Loss=1.4948, Val Loss=2.5045, LR=7.28e-05, Tokens=1,513,600, GradNorm=10.114\n",
            "New best model saved! Val Loss: 2.5045 (improvement: 0.0090)\n",
            "Epoch 31/150, Step 11850: Train Loss=1.5997, Val Loss=2.5495, LR=7.28e-05, Tokens=1,516,800, GradNorm=15.614\n",
            "Epoch 31/150, Step 11875: Train Loss=1.3128, Val Loss=2.5083, LR=7.27e-05, Tokens=1,520,000, GradNorm=12.362\n",
            "Epoch 31/150, Step 11900: Train Loss=1.6168, Val Loss=2.5450, LR=7.27e-05, Tokens=1,523,200, GradNorm=12.908\n",
            "Epoch 31 completed. Average loss: 1.6653\n",
            "Epoch 32/150, Step 11925: Train Loss=1.1694, Val Loss=2.5385, LR=7.27e-05, Tokens=1,526,400, GradNorm=10.995\n",
            "Epoch 32/150, Step 11950: Train Loss=1.2035, Val Loss=2.5747, LR=7.26e-05, Tokens=1,529,600, GradNorm=13.261\n",
            "Epoch 32/150, Step 11975: Train Loss=1.1173, Val Loss=2.5726, LR=7.26e-05, Tokens=1,532,800, GradNorm=11.880\n",
            "Epoch 32/150, Step 12000: Train Loss=1.0842, Val Loss=2.5155, LR=7.26e-05, Tokens=1,536,000, GradNorm=15.793\n",
            "Epoch 32/150, Step 12025: Train Loss=1.4618, Val Loss=2.5061, LR=7.25e-05, Tokens=1,539,200, GradNorm=12.074\n",
            "Epoch 32/150, Step 12050: Train Loss=1.4436, Val Loss=2.5087, LR=7.25e-05, Tokens=1,542,400, GradNorm=15.746\n",
            "Epoch 32/150, Step 12075: Train Loss=1.1087, Val Loss=2.4369, LR=7.25e-05, Tokens=1,545,600, GradNorm=11.857\n",
            "New best model saved! Val Loss: 2.4369 (improvement: 0.0675)\n",
            "Epoch 32/150, Step 12100: Train Loss=1.5370, Val Loss=2.4655, LR=7.24e-05, Tokens=1,548,800, GradNorm=10.789\n",
            "Epoch 32/150, Step 12125: Train Loss=1.3812, Val Loss=2.4658, LR=7.24e-05, Tokens=1,552,000, GradNorm=11.075\n",
            "Epoch 32/150, Step 12150: Train Loss=1.2908, Val Loss=2.5170, LR=7.24e-05, Tokens=1,555,200, GradNorm=12.906\n",
            "Epoch 32/150, Step 12175: Train Loss=1.2322, Val Loss=2.5333, LR=7.23e-05, Tokens=1,558,400, GradNorm=14.345\n",
            "Epoch 32/150, Step 12200: Train Loss=1.2114, Val Loss=2.4774, LR=7.23e-05, Tokens=1,561,600, GradNorm=12.778\n",
            "Epoch 32/150, Step 12225: Train Loss=1.5062, Val Loss=2.4879, LR=7.23e-05, Tokens=1,564,800, GradNorm=14.644\n",
            "Epoch 32/150, Step 12250: Train Loss=1.4930, Val Loss=2.4746, LR=7.22e-05, Tokens=1,568,000, GradNorm=10.185\n",
            "Epoch 32/150, Step 12275: Train Loss=1.9875, Val Loss=2.4819, LR=7.22e-05, Tokens=1,571,200, GradNorm=15.132\n",
            "Epoch 32 completed. Average loss: 1.5680\n",
            "Epoch 33/150, Step 12300: Train Loss=1.8964, Val Loss=2.4618, LR=7.22e-05, Tokens=1,574,400, GradNorm=12.400\n",
            "Epoch 33/150, Step 12325: Train Loss=1.1955, Val Loss=2.4361, LR=7.21e-05, Tokens=1,577,600, GradNorm=12.479\n",
            "New best model saved! Val Loss: 2.4361 (improvement: 0.0009)\n",
            "Epoch 33/150, Step 12350: Train Loss=1.0145, Val Loss=2.3985, LR=7.21e-05, Tokens=1,580,800, GradNorm=19.692\n",
            "New best model saved! Val Loss: 2.3985 (improvement: 0.0376)\n",
            "Epoch 33/150, Step 12375: Train Loss=1.3842, Val Loss=2.4231, LR=7.21e-05, Tokens=1,584,000, GradNorm=12.279\n",
            "Epoch 33/150, Step 12400: Train Loss=1.2693, Val Loss=2.4177, LR=7.20e-05, Tokens=1,587,200, GradNorm=11.828\n",
            "Epoch 33/150, Step 12425: Train Loss=1.0742, Val Loss=2.4081, LR=7.20e-05, Tokens=1,590,400, GradNorm=13.246\n",
            "Epoch 33/150, Step 12450: Train Loss=1.5678, Val Loss=2.4407, LR=7.20e-05, Tokens=1,593,600, GradNorm=13.204\n",
            "Epoch 33/150, Step 12475: Train Loss=1.4337, Val Loss=2.4319, LR=7.19e-05, Tokens=1,596,800, GradNorm=11.324\n",
            "Epoch 33/150, Step 12500: Train Loss=1.0218, Val Loss=2.4126, LR=7.19e-05, Tokens=1,600,000, GradNorm=10.969\n",
            "Epoch 33/150, Step 12525: Train Loss=1.1367, Val Loss=2.4618, LR=7.19e-05, Tokens=1,603,200, GradNorm=13.466\n",
            "Epoch 33/150, Step 12550: Train Loss=1.3526, Val Loss=2.4038, LR=7.18e-05, Tokens=1,606,400, GradNorm=11.473\n",
            "Epoch 33/150, Step 12575: Train Loss=1.3434, Val Loss=2.3742, LR=7.18e-05, Tokens=1,609,600, GradNorm=12.408\n",
            "New best model saved! Val Loss: 2.3742 (improvement: 0.0243)\n",
            "Epoch 33/150, Step 12600: Train Loss=1.2361, Val Loss=2.3729, LR=7.18e-05, Tokens=1,612,800, GradNorm=12.156\n",
            "New best model saved! Val Loss: 2.3729 (improvement: 0.0012)\n",
            "Epoch 33/150, Step 12625: Train Loss=1.0527, Val Loss=2.2997, LR=7.17e-05, Tokens=1,616,000, GradNorm=13.132\n",
            "New best model saved! Val Loss: 2.2997 (improvement: 0.0732)\n",
            "Epoch 33/150, Step 12650: Train Loss=1.5897, Val Loss=2.3502, LR=7.17e-05, Tokens=1,619,200, GradNorm=14.705\n",
            "Epoch 33 completed. Average loss: 1.4922\n",
            "Epoch 34/150, Step 12675: Train Loss=1.3119, Val Loss=2.3849, LR=7.17e-05, Tokens=1,622,400, GradNorm=12.190\n",
            "Epoch 34/150, Step 12700: Train Loss=1.2074, Val Loss=2.3470, LR=7.16e-05, Tokens=1,625,600, GradNorm=13.326\n",
            "Epoch 34/150, Step 12725: Train Loss=1.0180, Val Loss=2.3672, LR=7.16e-05, Tokens=1,628,800, GradNorm=12.214\n",
            "Epoch 34/150, Step 12750: Train Loss=1.3331, Val Loss=2.3716, LR=7.16e-05, Tokens=1,632,000, GradNorm=10.419\n",
            "Epoch 34/150, Step 12775: Train Loss=1.2402, Val Loss=2.3746, LR=7.15e-05, Tokens=1,635,200, GradNorm=10.935\n",
            "Epoch 34/150, Step 12800: Train Loss=1.1598, Val Loss=2.3802, LR=7.15e-05, Tokens=1,638,400, GradNorm=17.892\n",
            "Epoch 34/150, Step 12825: Train Loss=1.4556, Val Loss=2.3918, LR=7.15e-05, Tokens=1,641,600, GradNorm=13.829\n",
            "Epoch 34/150, Step 12850: Train Loss=1.0913, Val Loss=2.3766, LR=7.14e-05, Tokens=1,644,800, GradNorm=15.361\n",
            "Epoch 34/150, Step 12875: Train Loss=1.1199, Val Loss=2.3577, LR=7.14e-05, Tokens=1,648,000, GradNorm=8.024\n",
            "Epoch 34/150, Step 12900: Train Loss=0.9712, Val Loss=2.2921, LR=7.14e-05, Tokens=1,651,200, GradNorm=12.105\n",
            "New best model saved! Val Loss: 2.2921 (improvement: 0.0077)\n",
            "Epoch 34/150, Step 12925: Train Loss=1.0474, Val Loss=2.2846, LR=7.13e-05, Tokens=1,654,400, GradNorm=11.564\n",
            "New best model saved! Val Loss: 2.2846 (improvement: 0.0074)\n",
            "Epoch 34/150, Step 12950: Train Loss=0.9580, Val Loss=2.3352, LR=7.13e-05, Tokens=1,657,600, GradNorm=13.419\n",
            "Epoch 34/150, Step 12975: Train Loss=0.9483, Val Loss=2.3225, LR=7.13e-05, Tokens=1,660,800, GradNorm=18.123\n",
            "Epoch 34/150, Step 13000: Train Loss=1.3299, Val Loss=2.2784, LR=7.12e-05, Tokens=1,664,000, GradNorm=9.636\n",
            "New best model saved! Val Loss: 2.2784 (improvement: 0.0062)\n",
            "Epoch 34/150, Step 13025: Train Loss=1.1653, Val Loss=2.2758, LR=7.12e-05, Tokens=1,667,200, GradNorm=16.167\n",
            "New best model saved! Val Loss: 2.2758 (improvement: 0.0026)\n",
            "Epoch 34/150, Step 13050: Train Loss=0.9594, Val Loss=2.2360, LR=7.12e-05, Tokens=1,670,400, GradNorm=10.253\n",
            "New best model saved! Val Loss: 2.2360 (improvement: 0.0397)\n",
            "Epoch 34 completed. Average loss: 1.4214\n",
            "Epoch 35/150, Step 13075: Train Loss=0.7852, Val Loss=2.2897, LR=7.11e-05, Tokens=1,673,600, GradNorm=10.816\n",
            "Epoch 35/150, Step 13100: Train Loss=1.5721, Val Loss=2.2267, LR=7.11e-05, Tokens=1,676,800, GradNorm=10.733\n",
            "New best model saved! Val Loss: 2.2267 (improvement: 0.0093)\n",
            "Epoch 35/150, Step 13125: Train Loss=1.1106, Val Loss=2.2680, LR=7.11e-05, Tokens=1,680,000, GradNorm=13.889\n",
            "Epoch 35/150, Step 13150: Train Loss=1.1723, Val Loss=2.2744, LR=7.10e-05, Tokens=1,683,200, GradNorm=12.461\n",
            "Epoch 35/150, Step 13175: Train Loss=1.1572, Val Loss=2.2547, LR=7.10e-05, Tokens=1,686,400, GradNorm=11.329\n",
            "Epoch 35/150, Step 13200: Train Loss=0.8143, Val Loss=2.2536, LR=7.10e-05, Tokens=1,689,600, GradNorm=13.508\n",
            "Epoch 35/150, Step 13225: Train Loss=1.2922, Val Loss=2.3093, LR=7.09e-05, Tokens=1,692,800, GradNorm=9.924\n",
            "Epoch 35/150, Step 13250: Train Loss=1.2495, Val Loss=2.3036, LR=7.09e-05, Tokens=1,696,000, GradNorm=13.071\n",
            "Epoch 35/150, Step 13275: Train Loss=1.0945, Val Loss=2.2999, LR=7.09e-05, Tokens=1,699,200, GradNorm=10.510\n",
            "Epoch 35/150, Step 13300: Train Loss=1.2677, Val Loss=2.2912, LR=7.08e-05, Tokens=1,702,400, GradNorm=8.766\n",
            "Epoch 35/150, Step 13325: Train Loss=1.0783, Val Loss=2.2897, LR=7.08e-05, Tokens=1,705,600, GradNorm=10.411\n",
            "Epoch 35/150, Step 13350: Train Loss=0.7626, Val Loss=2.2634, LR=7.07e-05, Tokens=1,708,800, GradNorm=10.238\n",
            "Epoch 35/150, Step 13375: Train Loss=0.9548, Val Loss=2.2641, LR=7.07e-05, Tokens=1,712,000, GradNorm=10.676\n",
            "Epoch 35/150, Step 13400: Train Loss=0.8257, Val Loss=2.1695, LR=7.07e-05, Tokens=1,715,200, GradNorm=14.646\n",
            "New best model saved! Val Loss: 2.1695 (improvement: 0.0572)\n",
            "Epoch 35/150, Step 13425: Train Loss=0.9600, Val Loss=2.1982, LR=7.06e-05, Tokens=1,718,400, GradNorm=11.190\n",
            "Epoch 35 completed. Average loss: 1.3443\n",
            "Epoch 36/150, Step 13450: Train Loss=0.9246, Val Loss=2.1702, LR=7.06e-05, Tokens=1,721,600, GradNorm=14.516\n",
            "Epoch 36/150, Step 13475: Train Loss=0.9499, Val Loss=2.1693, LR=7.06e-05, Tokens=1,724,800, GradNorm=11.791\n",
            "Epoch 36/150, Step 13500: Train Loss=1.2176, Val Loss=2.1670, LR=7.05e-05, Tokens=1,728,000, GradNorm=14.502\n",
            "New best model saved! Val Loss: 2.1670 (improvement: 0.0025)\n",
            "Epoch 36/150, Step 13525: Train Loss=1.2687, Val Loss=2.1445, LR=7.05e-05, Tokens=1,731,200, GradNorm=16.922\n",
            "New best model saved! Val Loss: 2.1445 (improvement: 0.0224)\n",
            "Epoch 36/150, Step 13550: Train Loss=1.0650, Val Loss=2.1541, LR=7.05e-05, Tokens=1,734,400, GradNorm=9.815\n",
            "Epoch 36/150, Step 13575: Train Loss=0.6844, Val Loss=2.1623, LR=7.04e-05, Tokens=1,737,600, GradNorm=13.440\n",
            "Epoch 36/150, Step 13600: Train Loss=0.7921, Val Loss=2.1614, LR=7.04e-05, Tokens=1,740,800, GradNorm=15.136\n",
            "Epoch 36/150, Step 13625: Train Loss=0.8115, Val Loss=2.1588, LR=7.04e-05, Tokens=1,744,000, GradNorm=15.695\n",
            "Epoch 36/150, Step 13650: Train Loss=0.9831, Val Loss=2.1282, LR=7.03e-05, Tokens=1,747,200, GradNorm=11.729\n",
            "New best model saved! Val Loss: 2.1282 (improvement: 0.0164)\n",
            "Epoch 36/150, Step 13675: Train Loss=1.0764, Val Loss=2.1984, LR=7.03e-05, Tokens=1,750,400, GradNorm=11.543\n",
            "Epoch 36/150, Step 13700: Train Loss=0.9105, Val Loss=2.1550, LR=7.02e-05, Tokens=1,753,600, GradNorm=15.376\n",
            "Epoch 36/150, Step 13725: Train Loss=0.9906, Val Loss=2.1365, LR=7.02e-05, Tokens=1,756,800, GradNorm=13.136\n",
            "Epoch 36/150, Step 13750: Train Loss=1.1854, Val Loss=2.1683, LR=7.02e-05, Tokens=1,760,000, GradNorm=11.652\n",
            "Epoch 36/150, Step 13775: Train Loss=0.8423, Val Loss=2.2270, LR=7.01e-05, Tokens=1,763,200, GradNorm=16.733\n",
            "Epoch 36/150, Step 13800: Train Loss=0.9006, Val Loss=2.1818, LR=7.01e-05, Tokens=1,766,400, GradNorm=11.570\n",
            "Epoch 36 completed. Average loss: 1.2737\n",
            "Epoch 37/150, Step 13825: Train Loss=0.8991, Val Loss=2.0814, LR=7.01e-05, Tokens=1,769,600, GradNorm=14.675\n",
            "New best model saved! Val Loss: 2.0814 (improvement: 0.0468)\n",
            "Epoch 37/150, Step 13850: Train Loss=0.8251, Val Loss=2.1201, LR=7.00e-05, Tokens=1,772,800, GradNorm=10.533\n",
            "Epoch 37/150, Step 13875: Train Loss=0.8444, Val Loss=2.1145, LR=7.00e-05, Tokens=1,776,000, GradNorm=11.456\n",
            "Epoch 37/150, Step 13900: Train Loss=1.0782, Val Loss=2.0679, LR=7.00e-05, Tokens=1,779,200, GradNorm=11.257\n",
            "New best model saved! Val Loss: 2.0679 (improvement: 0.0134)\n",
            "Epoch 37/150, Step 13925: Train Loss=0.9042, Val Loss=2.0602, LR=6.99e-05, Tokens=1,782,400, GradNorm=15.192\n",
            "New best model saved! Val Loss: 2.0602 (improvement: 0.0078)\n",
            "Epoch 37/150, Step 13950: Train Loss=1.0285, Val Loss=2.0134, LR=6.99e-05, Tokens=1,785,600, GradNorm=15.014\n",
            "New best model saved! Val Loss: 2.0134 (improvement: 0.0468)\n",
            "Epoch 37/150, Step 13975: Train Loss=1.0202, Val Loss=1.9899, LR=6.98e-05, Tokens=1,788,800, GradNorm=13.319\n",
            "New best model saved! Val Loss: 1.9899 (improvement: 0.0235)\n",
            "Epoch 37/150, Step 14000: Train Loss=1.1528, Val Loss=2.0125, LR=6.98e-05, Tokens=1,792,000, GradNorm=10.938\n",
            "Epoch 37/150, Step 14025: Train Loss=1.0682, Val Loss=2.0070, LR=6.98e-05, Tokens=1,795,200, GradNorm=12.190\n",
            "Epoch 37/150, Step 14050: Train Loss=1.0608, Val Loss=2.0100, LR=6.97e-05, Tokens=1,798,400, GradNorm=17.007\n",
            "Epoch 37/150, Step 14075: Train Loss=1.0887, Val Loss=2.0287, LR=6.97e-05, Tokens=1,801,600, GradNorm=13.544\n",
            "Epoch 37/150, Step 14100: Train Loss=1.0478, Val Loss=2.0504, LR=6.97e-05, Tokens=1,804,800, GradNorm=10.806\n",
            "Epoch 37/150, Step 14125: Train Loss=1.0679, Val Loss=2.0543, LR=6.96e-05, Tokens=1,808,000, GradNorm=9.307\n",
            "Epoch 37/150, Step 14150: Train Loss=0.9848, Val Loss=2.0589, LR=6.96e-05, Tokens=1,811,200, GradNorm=11.704\n",
            "Epoch 37/150, Step 14175: Train Loss=1.0102, Val Loss=2.1079, LR=6.95e-05, Tokens=1,814,400, GradNorm=13.610\n",
            "Epoch 37/150, Step 14200: Train Loss=1.0337, Val Loss=2.0709, LR=6.95e-05, Tokens=1,817,600, GradNorm=12.855\n",
            "Epoch 37 completed. Average loss: 1.1974\n",
            "Epoch 38/150, Step 14225: Train Loss=0.8777, Val Loss=2.0763, LR=6.95e-05, Tokens=1,820,800, GradNorm=16.794\n",
            "Epoch 38/150, Step 14250: Train Loss=0.5828, Val Loss=2.0337, LR=6.94e-05, Tokens=1,824,000, GradNorm=13.739\n",
            "Epoch 38/150, Step 14275: Train Loss=0.8673, Val Loss=2.0101, LR=6.94e-05, Tokens=1,827,200, GradNorm=12.749\n",
            "Epoch 38/150, Step 14300: Train Loss=0.6772, Val Loss=2.0350, LR=6.94e-05, Tokens=1,830,400, GradNorm=12.696\n",
            "Epoch 38/150, Step 14325: Train Loss=0.6691, Val Loss=1.9801, LR=6.93e-05, Tokens=1,833,600, GradNorm=10.274\n",
            "New best model saved! Val Loss: 1.9801 (improvement: 0.0098)\n",
            "Epoch 38/150, Step 14350: Train Loss=0.6432, Val Loss=2.0060, LR=6.93e-05, Tokens=1,836,800, GradNorm=11.931\n",
            "Epoch 38/150, Step 14375: Train Loss=0.9993, Val Loss=1.9722, LR=6.92e-05, Tokens=1,840,000, GradNorm=13.140\n",
            "New best model saved! Val Loss: 1.9722 (improvement: 0.0078)\n",
            "Epoch 38/150, Step 14400: Train Loss=0.7798, Val Loss=1.9907, LR=6.92e-05, Tokens=1,843,200, GradNorm=15.765\n",
            "Epoch 38/150, Step 14425: Train Loss=0.7252, Val Loss=1.9772, LR=6.92e-05, Tokens=1,846,400, GradNorm=20.481\n",
            "Epoch 38/150, Step 14450: Train Loss=0.9270, Val Loss=1.9626, LR=6.91e-05, Tokens=1,849,600, GradNorm=12.622\n",
            "New best model saved! Val Loss: 1.9626 (improvement: 0.0096)\n",
            "Epoch 38/150, Step 14475: Train Loss=0.7305, Val Loss=1.9749, LR=6.91e-05, Tokens=1,852,800, GradNorm=7.276\n",
            "Epoch 38/150, Step 14500: Train Loss=1.2642, Val Loss=1.9968, LR=6.91e-05, Tokens=1,856,000, GradNorm=13.049\n",
            "Epoch 38/150, Step 14525: Train Loss=1.1309, Val Loss=1.9898, LR=6.90e-05, Tokens=1,859,200, GradNorm=11.294\n",
            "Epoch 38/150, Step 14550: Train Loss=0.9462, Val Loss=1.9362, LR=6.90e-05, Tokens=1,862,400, GradNorm=14.523\n",
            "New best model saved! Val Loss: 1.9362 (improvement: 0.0264)\n",
            "Epoch 38/150, Step 14575: Train Loss=0.9889, Val Loss=1.9771, LR=6.89e-05, Tokens=1,865,600, GradNorm=11.260\n",
            "Epoch 38 completed. Average loss: 1.1365\n",
            "Epoch 39/150, Step 14600: Train Loss=1.1018, Val Loss=1.9097, LR=6.89e-05, Tokens=1,868,800, GradNorm=12.936\n",
            "New best model saved! Val Loss: 1.9097 (improvement: 0.0265)\n",
            "Epoch 39/150, Step 14625: Train Loss=1.1952, Val Loss=1.9598, LR=6.89e-05, Tokens=1,872,000, GradNorm=8.477\n",
            "Epoch 39/150, Step 14650: Train Loss=0.9614, Val Loss=1.9737, LR=6.88e-05, Tokens=1,875,200, GradNorm=11.486\n",
            "Epoch 39/150, Step 14675: Train Loss=0.8847, Val Loss=2.0261, LR=6.88e-05, Tokens=1,878,400, GradNorm=13.192\n",
            "Epoch 39/150, Step 14700: Train Loss=0.8788, Val Loss=1.9895, LR=6.87e-05, Tokens=1,881,600, GradNorm=12.721\n",
            "Epoch 39/150, Step 14725: Train Loss=0.5943, Val Loss=2.0095, LR=6.87e-05, Tokens=1,884,800, GradNorm=10.597\n",
            "Epoch 39/150, Step 14750: Train Loss=0.6911, Val Loss=2.0150, LR=6.87e-05, Tokens=1,888,000, GradNorm=11.070\n",
            "Epoch 39/150, Step 14775: Train Loss=1.0036, Val Loss=1.9985, LR=6.86e-05, Tokens=1,891,200, GradNorm=13.056\n",
            "Epoch 39/150, Step 14800: Train Loss=0.5221, Val Loss=1.9674, LR=6.86e-05, Tokens=1,894,400, GradNorm=10.399\n",
            "Epoch 39/150, Step 14825: Train Loss=0.6882, Val Loss=1.8706, LR=6.86e-05, Tokens=1,897,600, GradNorm=12.993\n",
            "New best model saved! Val Loss: 1.8706 (improvement: 0.0391)\n",
            "Epoch 39/150, Step 14850: Train Loss=0.8562, Val Loss=1.9042, LR=6.85e-05, Tokens=1,900,800, GradNorm=13.744\n",
            "Epoch 39/150, Step 14875: Train Loss=0.8137, Val Loss=1.9164, LR=6.85e-05, Tokens=1,904,000, GradNorm=11.470\n",
            "Epoch 39/150, Step 14900: Train Loss=0.7357, Val Loss=1.9211, LR=6.84e-05, Tokens=1,907,200, GradNorm=11.687\n",
            "Epoch 39/150, Step 14925: Train Loss=0.5819, Val Loss=1.8948, LR=6.84e-05, Tokens=1,910,400, GradNorm=16.381\n",
            "Epoch 39/150, Step 14950: Train Loss=0.8576, Val Loss=1.9024, LR=6.84e-05, Tokens=1,913,600, GradNorm=15.358\n",
            "Epoch 39/150, Step 14975: Train Loss=0.7095, Val Loss=1.9275, LR=6.83e-05, Tokens=1,916,800, GradNorm=11.136\n",
            "Epoch 39 completed. Average loss: 1.0771\n",
            "Epoch 40/150, Step 15000: Train Loss=0.8321, Val Loss=1.8927, LR=6.83e-05, Tokens=1,920,000, GradNorm=11.014\n",
            "Epoch 40/150, Step 15025: Train Loss=0.6107, Val Loss=1.9608, LR=6.82e-05, Tokens=1,923,200, GradNorm=16.814\n",
            "Epoch 40/150, Step 15050: Train Loss=0.5774, Val Loss=1.9459, LR=6.82e-05, Tokens=1,926,400, GradNorm=15.963\n",
            "Epoch 40/150, Step 15075: Train Loss=0.8675, Val Loss=1.8962, LR=6.82e-05, Tokens=1,929,600, GradNorm=11.391\n",
            "Epoch 40/150, Step 15100: Train Loss=0.5922, Val Loss=1.9303, LR=6.81e-05, Tokens=1,932,800, GradNorm=8.619\n",
            "Epoch 40/150, Step 15125: Train Loss=0.8119, Val Loss=1.8366, LR=6.81e-05, Tokens=1,936,000, GradNorm=9.172\n",
            "New best model saved! Val Loss: 1.8366 (improvement: 0.0340)\n",
            "Epoch 40/150, Step 15150: Train Loss=0.8048, Val Loss=1.8213, LR=6.80e-05, Tokens=1,939,200, GradNorm=13.976\n",
            "New best model saved! Val Loss: 1.8213 (improvement: 0.0153)\n",
            "Epoch 40/150, Step 15175: Train Loss=0.7236, Val Loss=1.9030, LR=6.80e-05, Tokens=1,942,400, GradNorm=14.502\n",
            "Epoch 40/150, Step 15200: Train Loss=0.6745, Val Loss=1.8837, LR=6.80e-05, Tokens=1,945,600, GradNorm=11.147\n",
            "Epoch 40/150, Step 15225: Train Loss=1.1462, Val Loss=1.8395, LR=6.79e-05, Tokens=1,948,800, GradNorm=15.718\n",
            "Epoch 40/150, Step 15250: Train Loss=0.9874, Val Loss=1.8379, LR=6.79e-05, Tokens=1,952,000, GradNorm=8.967\n",
            "Epoch 40/150, Step 15275: Train Loss=0.5395, Val Loss=1.8369, LR=6.79e-05, Tokens=1,955,200, GradNorm=13.182\n",
            "Epoch 40/150, Step 15300: Train Loss=0.7680, Val Loss=1.8833, LR=6.78e-05, Tokens=1,958,400, GradNorm=15.016\n",
            "Epoch 40/150, Step 15325: Train Loss=0.6184, Val Loss=1.8326, LR=6.78e-05, Tokens=1,961,600, GradNorm=15.295\n",
            "Epoch 40/150, Step 15350: Train Loss=1.0459, Val Loss=1.8277, LR=6.77e-05, Tokens=1,964,800, GradNorm=7.731\n",
            "Epoch 40 completed. Average loss: 1.0192\n",
            "Sample generation after epoch 40:\n",
            "This movie could be para-para-paradise Oh, oh-oh, oh-oh, oh-oh Oh, oh-oh-oh-oh, oh-oh Para-a-oh-oh-oh This could be para-paradise This could be para-para-paradise This could be parapara-oh-\n",
            "------------------------------------------------------------\n",
            "Epoch 41/150, Step 15375: Train Loss=0.6347, Val Loss=1.8444, LR=6.77e-05, Tokens=1,968,000, GradNorm=9.357\n",
            "Epoch 41/150, Step 15400: Train Loss=0.9950, Val Loss=1.8534, LR=6.77e-05, Tokens=1,971,200, GradNorm=19.398\n",
            "Epoch 41/150, Step 15425: Train Loss=0.8282, Val Loss=1.9185, LR=6.76e-05, Tokens=1,974,400, GradNorm=12.458\n",
            "Epoch 41/150, Step 15450: Train Loss=1.1007, Val Loss=1.9306, LR=6.76e-05, Tokens=1,977,600, GradNorm=12.742\n",
            "Epoch 41/150, Step 15475: Train Loss=0.8224, Val Loss=1.9074, LR=6.75e-05, Tokens=1,980,800, GradNorm=11.157\n",
            "Epoch 41/150, Step 15500: Train Loss=0.8606, Val Loss=1.8470, LR=6.75e-05, Tokens=1,984,000, GradNorm=11.097\n",
            "Epoch 41/150, Step 15525: Train Loss=0.6396, Val Loss=1.8670, LR=6.75e-05, Tokens=1,987,200, GradNorm=9.288\n",
            "Epoch 41/150, Step 15550: Train Loss=0.8007, Val Loss=1.8380, LR=6.74e-05, Tokens=1,990,400, GradNorm=13.105\n",
            "Epoch 41/150, Step 15575: Train Loss=0.8356, Val Loss=1.8124, LR=6.74e-05, Tokens=1,993,600, GradNorm=10.680\n",
            "New best model saved! Val Loss: 1.8124 (improvement: 0.0089)\n",
            "Epoch 41/150, Step 15600: Train Loss=0.7809, Val Loss=1.7647, LR=6.73e-05, Tokens=1,996,800, GradNorm=11.640\n",
            "New best model saved! Val Loss: 1.7647 (improvement: 0.0477)\n",
            "Epoch 41/150, Step 15625: Train Loss=0.9299, Val Loss=1.8068, LR=6.73e-05, Tokens=2,000,000, GradNorm=13.756\n",
            "Epoch 41/150, Step 15650: Train Loss=0.6559, Val Loss=1.7479, LR=6.72e-05, Tokens=2,003,200, GradNorm=16.805\n",
            "New best model saved! Val Loss: 1.7479 (improvement: 0.0168)\n",
            "Epoch 41/150, Step 15675: Train Loss=0.6918, Val Loss=1.7289, LR=6.72e-05, Tokens=2,006,400, GradNorm=11.259\n",
            "New best model saved! Val Loss: 1.7289 (improvement: 0.0189)\n",
            "Epoch 41/150, Step 15700: Train Loss=0.6286, Val Loss=1.7351, LR=6.72e-05, Tokens=2,009,600, GradNorm=10.287\n",
            "Epoch 41/150, Step 15725: Train Loss=0.6932, Val Loss=1.7513, LR=6.71e-05, Tokens=2,012,800, GradNorm=15.845\n",
            "Epoch 41 completed. Average loss: 0.9695\n",
            "Epoch 42/150, Step 15750: Train Loss=0.6213, Val Loss=1.7726, LR=6.71e-05, Tokens=2,016,000, GradNorm=17.058\n",
            "Epoch 42/150, Step 15775: Train Loss=0.8740, Val Loss=1.7960, LR=6.70e-05, Tokens=2,019,200, GradNorm=7.116\n",
            "Epoch 42/150, Step 15800: Train Loss=0.6081, Val Loss=1.8161, LR=6.70e-05, Tokens=2,022,400, GradNorm=11.262\n",
            "Epoch 42/150, Step 15825: Train Loss=0.8331, Val Loss=1.7881, LR=6.70e-05, Tokens=2,025,600, GradNorm=13.746\n",
            "Epoch 42/150, Step 15850: Train Loss=0.9517, Val Loss=1.8064, LR=6.69e-05, Tokens=2,028,800, GradNorm=7.122\n",
            "Epoch 42/150, Step 15875: Train Loss=0.8069, Val Loss=1.7973, LR=6.69e-05, Tokens=2,032,000, GradNorm=13.219\n",
            "Epoch 42/150, Step 15900: Train Loss=0.9898, Val Loss=1.8181, LR=6.68e-05, Tokens=2,035,200, GradNorm=7.170\n",
            "Epoch 42/150, Step 15925: Train Loss=0.9842, Val Loss=1.7510, LR=6.68e-05, Tokens=2,038,400, GradNorm=14.948\n",
            "Epoch 42/150, Step 15950: Train Loss=0.7318, Val Loss=1.7267, LR=6.68e-05, Tokens=2,041,600, GradNorm=14.987\n",
            "New best model saved! Val Loss: 1.7267 (improvement: 0.0022)\n",
            "Epoch 42/150, Step 15975: Train Loss=0.5317, Val Loss=1.7088, LR=6.67e-05, Tokens=2,044,800, GradNorm=10.418\n",
            "New best model saved! Val Loss: 1.7088 (improvement: 0.0180)\n",
            "Epoch 42/150, Step 16000: Train Loss=0.7478, Val Loss=1.7193, LR=6.67e-05, Tokens=2,048,000, GradNorm=15.242\n",
            "Epoch 42/150, Step 16025: Train Loss=0.9475, Val Loss=1.6870, LR=6.66e-05, Tokens=2,051,200, GradNorm=13.172\n",
            "New best model saved! Val Loss: 1.6870 (improvement: 0.0218)\n",
            "Epoch 42/150, Step 16050: Train Loss=0.5952, Val Loss=1.6952, LR=6.66e-05, Tokens=2,054,400, GradNorm=8.184\n",
            "Epoch 42/150, Step 16075: Train Loss=0.6150, Val Loss=1.6914, LR=6.66e-05, Tokens=2,057,600, GradNorm=10.537\n",
            "Epoch 42/150, Step 16100: Train Loss=0.9515, Val Loss=1.7342, LR=6.65e-05, Tokens=2,060,800, GradNorm=10.548\n",
            "Epoch 42/150, Step 16125: Train Loss=0.8564, Val Loss=1.7642, LR=6.65e-05, Tokens=2,064,000, GradNorm=17.329\n",
            "Epoch 42 completed. Average loss: 0.9176\n",
            "Epoch 43/150, Step 16150: Train Loss=0.9659, Val Loss=1.7550, LR=6.64e-05, Tokens=2,067,200, GradNorm=13.721\n",
            "Epoch 43/150, Step 16175: Train Loss=0.6834, Val Loss=1.7022, LR=6.64e-05, Tokens=2,070,400, GradNorm=11.688\n",
            "Epoch 43/150, Step 16200: Train Loss=0.9400, Val Loss=1.7191, LR=6.63e-05, Tokens=2,073,600, GradNorm=10.296\n",
            "Epoch 43/150, Step 16225: Train Loss=0.6601, Val Loss=1.6881, LR=6.63e-05, Tokens=2,076,800, GradNorm=9.169\n",
            "Epoch 43/150, Step 16250: Train Loss=0.7516, Val Loss=1.7198, LR=6.63e-05, Tokens=2,080,000, GradNorm=12.655\n",
            "Epoch 43/150, Step 16275: Train Loss=1.2404, Val Loss=1.7375, LR=6.62e-05, Tokens=2,083,200, GradNorm=12.314\n",
            "Epoch 43/150, Step 16300: Train Loss=0.7346, Val Loss=1.6703, LR=6.62e-05, Tokens=2,086,400, GradNorm=14.074\n",
            "New best model saved! Val Loss: 1.6703 (improvement: 0.0167)\n",
            "Epoch 43/150, Step 16325: Train Loss=0.8108, Val Loss=1.6907, LR=6.61e-05, Tokens=2,089,600, GradNorm=11.478\n",
            "Epoch 43/150, Step 16350: Train Loss=1.0626, Val Loss=1.6769, LR=6.61e-05, Tokens=2,092,800, GradNorm=10.185\n",
            "Epoch 43/150, Step 16375: Train Loss=0.7146, Val Loss=1.7062, LR=6.61e-05, Tokens=2,096,000, GradNorm=12.103\n",
            "Epoch 43/150, Step 16400: Train Loss=0.4354, Val Loss=1.6580, LR=6.60e-05, Tokens=2,099,200, GradNorm=14.334\n",
            "New best model saved! Val Loss: 1.6580 (improvement: 0.0123)\n",
            "Epoch 43/150, Step 16425: Train Loss=0.6053, Val Loss=1.6893, LR=6.60e-05, Tokens=2,102,400, GradNorm=11.502\n",
            "Epoch 43/150, Step 16450: Train Loss=0.5205, Val Loss=1.7139, LR=6.59e-05, Tokens=2,105,600, GradNorm=9.142\n",
            "Epoch 43/150, Step 16475: Train Loss=0.5529, Val Loss=1.6887, LR=6.59e-05, Tokens=2,108,800, GradNorm=11.780\n",
            "Epoch 43/150, Step 16500: Train Loss=0.6786, Val Loss=1.6669, LR=6.58e-05, Tokens=2,112,000, GradNorm=14.675\n",
            "Epoch 43 completed. Average loss: 0.8776\n",
            "Epoch 44/150, Step 16525: Train Loss=0.6432, Val Loss=1.6756, LR=6.58e-05, Tokens=2,115,200, GradNorm=11.613\n",
            "Epoch 44/150, Step 16550: Train Loss=0.6763, Val Loss=1.6483, LR=6.58e-05, Tokens=2,118,400, GradNorm=12.633\n",
            "New best model saved! Val Loss: 1.6483 (improvement: 0.0097)\n",
            "Epoch 44/150, Step 16575: Train Loss=0.7662, Val Loss=1.6185, LR=6.57e-05, Tokens=2,121,600, GradNorm=11.890\n",
            "New best model saved! Val Loss: 1.6185 (improvement: 0.0299)\n",
            "Epoch 44/150, Step 16600: Train Loss=0.8349, Val Loss=1.6653, LR=6.57e-05, Tokens=2,124,800, GradNorm=12.336\n",
            "Epoch 44/150, Step 16625: Train Loss=0.6826, Val Loss=1.6628, LR=6.56e-05, Tokens=2,128,000, GradNorm=13.306\n",
            "Epoch 44/150, Step 16650: Train Loss=0.4742, Val Loss=1.6930, LR=6.56e-05, Tokens=2,131,200, GradNorm=14.218\n",
            "Epoch 44/150, Step 16675: Train Loss=0.8185, Val Loss=1.6843, LR=6.55e-05, Tokens=2,134,400, GradNorm=11.855\n",
            "Epoch 44/150, Step 16700: Train Loss=0.4987, Val Loss=1.7133, LR=6.55e-05, Tokens=2,137,600, GradNorm=11.268\n",
            "Epoch 44/150, Step 16725: Train Loss=0.7801, Val Loss=1.7158, LR=6.55e-05, Tokens=2,140,800, GradNorm=11.527\n",
            "Epoch 44/150, Step 16750: Train Loss=0.8671, Val Loss=1.6778, LR=6.54e-05, Tokens=2,144,000, GradNorm=14.706\n",
            "Epoch 44/150, Step 16775: Train Loss=0.6638, Val Loss=1.6690, LR=6.54e-05, Tokens=2,147,200, GradNorm=12.144\n",
            "Epoch 44/150, Step 16800: Train Loss=0.3997, Val Loss=1.6427, LR=6.53e-05, Tokens=2,150,400, GradNorm=8.107\n",
            "Epoch 44/150, Step 16825: Train Loss=0.5564, Val Loss=1.6609, LR=6.53e-05, Tokens=2,153,600, GradNorm=7.913\n",
            "Epoch 44/150, Step 16850: Train Loss=0.7218, Val Loss=1.6510, LR=6.52e-05, Tokens=2,156,800, GradNorm=15.391\n",
            "Epoch 44/150, Step 16875: Train Loss=1.0229, Val Loss=1.6634, LR=6.52e-05, Tokens=2,160,000, GradNorm=11.418\n",
            "Epoch 44 completed. Average loss: 0.8275\n",
            "Epoch 45/150, Step 16900: Train Loss=0.6442, Val Loss=1.6412, LR=6.52e-05, Tokens=2,163,200, GradNorm=9.545\n",
            "Epoch 45/150, Step 16925: Train Loss=0.5960, Val Loss=1.6137, LR=6.51e-05, Tokens=2,166,400, GradNorm=11.795\n",
            "New best model saved! Val Loss: 1.6137 (improvement: 0.0048)\n",
            "Epoch 45/150, Step 16950: Train Loss=0.8974, Val Loss=1.6138, LR=6.51e-05, Tokens=2,169,600, GradNorm=15.502\n",
            "Epoch 45/150, Step 16975: Train Loss=0.7037, Val Loss=1.5681, LR=6.50e-05, Tokens=2,172,800, GradNorm=8.066\n",
            "New best model saved! Val Loss: 1.5681 (improvement: 0.0455)\n",
            "Epoch 45/150, Step 17000: Train Loss=0.6598, Val Loss=1.6307, LR=6.50e-05, Tokens=2,176,000, GradNorm=9.831\n",
            "Epoch 45/150, Step 17025: Train Loss=0.6085, Val Loss=1.5892, LR=6.49e-05, Tokens=2,179,200, GradNorm=13.796\n",
            "Epoch 45/150, Step 17050: Train Loss=0.5757, Val Loss=1.5888, LR=6.49e-05, Tokens=2,182,400, GradNorm=12.301\n",
            "Epoch 45/150, Step 17075: Train Loss=0.7623, Val Loss=1.6102, LR=6.49e-05, Tokens=2,185,600, GradNorm=11.521\n",
            "Epoch 45/150, Step 17100: Train Loss=0.3775, Val Loss=1.5767, LR=6.48e-05, Tokens=2,188,800, GradNorm=12.417\n",
            "Epoch 45/150, Step 17125: Train Loss=0.6049, Val Loss=1.6136, LR=6.48e-05, Tokens=2,192,000, GradNorm=8.297\n",
            "Epoch 45/150, Step 17150: Train Loss=0.5083, Val Loss=1.6753, LR=6.47e-05, Tokens=2,195,200, GradNorm=12.166\n",
            "Epoch 45/150, Step 17175: Train Loss=0.5033, Val Loss=1.6133, LR=6.47e-05, Tokens=2,198,400, GradNorm=10.446\n",
            "Epoch 45/150, Step 17200: Train Loss=0.6773, Val Loss=1.6284, LR=6.46e-05, Tokens=2,201,600, GradNorm=8.331\n",
            "Epoch 45/150, Step 17225: Train Loss=0.6022, Val Loss=1.6490, LR=6.46e-05, Tokens=2,204,800, GradNorm=9.820\n",
            "Epoch 45/150, Step 17250: Train Loss=1.0147, Val Loss=1.6285, LR=6.46e-05, Tokens=2,208,000, GradNorm=12.178\n",
            "Epoch 45/150, Step 17275: Train Loss=0.4583, Val Loss=1.5920, LR=6.45e-05, Tokens=2,211,200, GradNorm=10.396\n",
            "Epoch 45 completed. Average loss: 0.7901\n",
            "Epoch 46/150, Step 17300: Train Loss=0.5710, Val Loss=1.6213, LR=6.45e-05, Tokens=2,214,400, GradNorm=13.113\n",
            "Epoch 46/150, Step 17325: Train Loss=0.3835, Val Loss=1.6043, LR=6.44e-05, Tokens=2,217,600, GradNorm=9.109\n",
            "Epoch 46/150, Step 17350: Train Loss=0.3675, Val Loss=1.5513, LR=6.44e-05, Tokens=2,220,800, GradNorm=8.935\n",
            "New best model saved! Val Loss: 1.5513 (improvement: 0.0169)\n",
            "Epoch 46/150, Step 17375: Train Loss=0.3860, Val Loss=1.5795, LR=6.43e-05, Tokens=2,224,000, GradNorm=14.056\n",
            "Epoch 46/150, Step 17400: Train Loss=0.4768, Val Loss=1.5299, LR=6.43e-05, Tokens=2,227,200, GradNorm=12.882\n",
            "New best model saved! Val Loss: 1.5299 (improvement: 0.0214)\n",
            "Epoch 46/150, Step 17425: Train Loss=0.5412, Val Loss=1.5535, LR=6.42e-05, Tokens=2,230,400, GradNorm=11.581\n",
            "Epoch 46/150, Step 17450: Train Loss=0.7029, Val Loss=1.5719, LR=6.42e-05, Tokens=2,233,600, GradNorm=12.777\n",
            "Epoch 46/150, Step 17475: Train Loss=0.4189, Val Loss=1.5683, LR=6.42e-05, Tokens=2,236,800, GradNorm=9.650\n",
            "Epoch 46/150, Step 17500: Train Loss=0.6057, Val Loss=1.5973, LR=6.41e-05, Tokens=2,240,000, GradNorm=14.981\n",
            "Epoch 46/150, Step 17525: Train Loss=0.3816, Val Loss=1.5736, LR=6.41e-05, Tokens=2,243,200, GradNorm=10.251\n",
            "Epoch 46/150, Step 17550: Train Loss=0.7720, Val Loss=1.5482, LR=6.40e-05, Tokens=2,246,400, GradNorm=14.603\n",
            "Epoch 46/150, Step 17575: Train Loss=0.4517, Val Loss=1.5508, LR=6.40e-05, Tokens=2,249,600, GradNorm=10.074\n",
            "Epoch 46/150, Step 17600: Train Loss=0.6355, Val Loss=1.5427, LR=6.39e-05, Tokens=2,252,800, GradNorm=14.338\n",
            "Epoch 46/150, Step 17625: Train Loss=0.5784, Val Loss=1.5667, LR=6.39e-05, Tokens=2,256,000, GradNorm=8.862\n",
            "Epoch 46/150, Step 17650: Train Loss=0.5020, Val Loss=1.5598, LR=6.39e-05, Tokens=2,259,200, GradNorm=15.038\n",
            "Epoch 46 completed. Average loss: 0.7488\n",
            "Epoch 47/150, Step 17675: Train Loss=0.4937, Val Loss=1.5435, LR=6.38e-05, Tokens=2,262,400, GradNorm=11.031\n",
            "Epoch 47/150, Step 17700: Train Loss=0.7254, Val Loss=1.5448, LR=6.38e-05, Tokens=2,265,600, GradNorm=8.956\n",
            "Epoch 47/150, Step 17725: Train Loss=0.5757, Val Loss=1.5176, LR=6.37e-05, Tokens=2,268,800, GradNorm=10.124\n",
            "New best model saved! Val Loss: 1.5176 (improvement: 0.0123)\n",
            "Epoch 47/150, Step 17750: Train Loss=0.3536, Val Loss=1.5373, LR=6.37e-05, Tokens=2,272,000, GradNorm=12.696\n",
            "Epoch 47/150, Step 17775: Train Loss=0.6422, Val Loss=1.5386, LR=6.36e-05, Tokens=2,275,200, GradNorm=8.717\n",
            "Epoch 47/150, Step 17800: Train Loss=1.0547, Val Loss=1.5663, LR=6.36e-05, Tokens=2,278,400, GradNorm=14.655\n",
            "Epoch 47/150, Step 17825: Train Loss=0.5620, Val Loss=1.5806, LR=6.35e-05, Tokens=2,281,600, GradNorm=10.057\n",
            "Epoch 47/150, Step 17850: Train Loss=0.3953, Val Loss=1.4784, LR=6.35e-05, Tokens=2,284,800, GradNorm=14.501\n",
            "New best model saved! Val Loss: 1.4784 (improvement: 0.0391)\n",
            "Epoch 47/150, Step 17875: Train Loss=0.3447, Val Loss=1.4822, LR=6.34e-05, Tokens=2,288,000, GradNorm=9.911\n",
            "Epoch 47/150, Step 17900: Train Loss=0.5857, Val Loss=1.4854, LR=6.34e-05, Tokens=2,291,200, GradNorm=11.349\n",
            "Epoch 47/150, Step 17925: Train Loss=0.6508, Val Loss=1.4988, LR=6.34e-05, Tokens=2,294,400, GradNorm=9.546\n",
            "Epoch 47/150, Step 17950: Train Loss=0.3722, Val Loss=1.4938, LR=6.33e-05, Tokens=2,297,600, GradNorm=11.224\n",
            "Epoch 47/150, Step 17975: Train Loss=0.4036, Val Loss=1.5180, LR=6.33e-05, Tokens=2,300,800, GradNorm=14.465\n",
            "Epoch 47/150, Step 18000: Train Loss=0.5457, Val Loss=1.5032, LR=6.32e-05, Tokens=2,304,000, GradNorm=11.660\n",
            "Epoch 47/150, Step 18025: Train Loss=0.7118, Val Loss=1.4652, LR=6.32e-05, Tokens=2,307,200, GradNorm=12.453\n",
            "New best model saved! Val Loss: 1.4652 (improvement: 0.0132)\n",
            "Epoch 47 completed. Average loss: 0.7139\n",
            "Epoch 48/150, Step 18050: Train Loss=0.4451, Val Loss=1.5220, LR=6.31e-05, Tokens=2,310,400, GradNorm=11.883\n",
            "Epoch 48/150, Step 18075: Train Loss=0.5912, Val Loss=1.5126, LR=6.31e-05, Tokens=2,313,600, GradNorm=11.952\n",
            "Epoch 48/150, Step 18100: Train Loss=0.4793, Val Loss=1.5066, LR=6.30e-05, Tokens=2,316,800, GradNorm=14.300\n",
            "Epoch 48/150, Step 18125: Train Loss=0.2927, Val Loss=1.4914, LR=6.30e-05, Tokens=2,320,000, GradNorm=12.267\n",
            "Epoch 48/150, Step 18150: Train Loss=0.3474, Val Loss=1.4892, LR=6.30e-05, Tokens=2,323,200, GradNorm=8.873\n",
            "Epoch 48/150, Step 18175: Train Loss=0.6438, Val Loss=1.4930, LR=6.29e-05, Tokens=2,326,400, GradNorm=9.647\n",
            "Epoch 48/150, Step 18200: Train Loss=0.2760, Val Loss=1.4434, LR=6.29e-05, Tokens=2,329,600, GradNorm=8.850\n",
            "New best model saved! Val Loss: 1.4434 (improvement: 0.0219)\n",
            "Epoch 48/150, Step 18225: Train Loss=0.3360, Val Loss=1.4651, LR=6.28e-05, Tokens=2,332,800, GradNorm=12.161\n",
            "Epoch 48/150, Step 18250: Train Loss=0.3362, Val Loss=1.4479, LR=6.28e-05, Tokens=2,336,000, GradNorm=17.848\n",
            "Epoch 48/150, Step 18275: Train Loss=0.7601, Val Loss=1.4930, LR=6.27e-05, Tokens=2,339,200, GradNorm=9.902\n",
            "Epoch 48/150, Step 18300: Train Loss=0.4015, Val Loss=1.5146, LR=6.27e-05, Tokens=2,342,400, GradNorm=13.765\n",
            "Epoch 48/150, Step 18325: Train Loss=0.5213, Val Loss=1.4287, LR=6.26e-05, Tokens=2,345,600, GradNorm=8.762\n",
            "New best model saved! Val Loss: 1.4287 (improvement: 0.0147)\n",
            "Epoch 48/150, Step 18350: Train Loss=0.4271, Val Loss=1.4427, LR=6.26e-05, Tokens=2,348,800, GradNorm=12.093\n",
            "Epoch 48/150, Step 18375: Train Loss=0.4837, Val Loss=1.4397, LR=6.25e-05, Tokens=2,352,000, GradNorm=13.385\n",
            "Epoch 48/150, Step 18400: Train Loss=0.5221, Val Loss=1.4633, LR=6.25e-05, Tokens=2,355,200, GradNorm=11.814\n",
            "Epoch 48/150, Step 18425: Train Loss=0.3924, Val Loss=1.4721, LR=6.25e-05, Tokens=2,358,400, GradNorm=7.856\n",
            "Epoch 48 completed. Average loss: 0.6776\n",
            "Epoch 49/150, Step 18450: Train Loss=0.2887, Val Loss=1.4688, LR=6.24e-05, Tokens=2,361,600, GradNorm=13.826\n",
            "Epoch 49/150, Step 18475: Train Loss=0.3955, Val Loss=1.4261, LR=6.24e-05, Tokens=2,364,800, GradNorm=12.716\n",
            "New best model saved! Val Loss: 1.4261 (improvement: 0.0025)\n",
            "Epoch 49/150, Step 18500: Train Loss=0.3989, Val Loss=1.4480, LR=6.23e-05, Tokens=2,368,000, GradNorm=9.553\n",
            "Epoch 49/150, Step 18525: Train Loss=0.3666, Val Loss=1.4274, LR=6.23e-05, Tokens=2,371,200, GradNorm=8.134\n",
            "Epoch 49/150, Step 18550: Train Loss=0.5950, Val Loss=1.4774, LR=6.22e-05, Tokens=2,374,400, GradNorm=8.673\n",
            "Epoch 49/150, Step 18575: Train Loss=0.3611, Val Loss=1.4306, LR=6.22e-05, Tokens=2,377,600, GradNorm=12.328\n",
            "Epoch 49/150, Step 18600: Train Loss=0.3657, Val Loss=1.4679, LR=6.21e-05, Tokens=2,380,800, GradNorm=12.566\n",
            "Epoch 49/150, Step 18625: Train Loss=0.4515, Val Loss=1.4244, LR=6.21e-05, Tokens=2,384,000, GradNorm=8.928\n",
            "New best model saved! Val Loss: 1.4244 (improvement: 0.0017)\n",
            "Epoch 49/150, Step 18650: Train Loss=0.2824, Val Loss=1.4614, LR=6.20e-05, Tokens=2,387,200, GradNorm=10.899\n",
            "Epoch 49/150, Step 18675: Train Loss=0.8174, Val Loss=1.4418, LR=6.20e-05, Tokens=2,390,400, GradNorm=14.228\n",
            "Epoch 49/150, Step 18700: Train Loss=0.2963, Val Loss=1.4034, LR=6.19e-05, Tokens=2,393,600, GradNorm=18.314\n",
            "New best model saved! Val Loss: 1.4034 (improvement: 0.0210)\n",
            "Epoch 49/150, Step 18725: Train Loss=0.5156, Val Loss=1.4058, LR=6.19e-05, Tokens=2,396,800, GradNorm=14.467\n",
            "Epoch 49/150, Step 18750: Train Loss=0.4150, Val Loss=1.3971, LR=6.19e-05, Tokens=2,400,000, GradNorm=13.926\n",
            "New best model saved! Val Loss: 1.3971 (improvement: 0.0063)\n",
            "Epoch 49/150, Step 18775: Train Loss=0.3443, Val Loss=1.4022, LR=6.18e-05, Tokens=2,403,200, GradNorm=8.624\n",
            "Epoch 49/150, Step 18800: Train Loss=0.4123, Val Loss=1.3729, LR=6.18e-05, Tokens=2,406,400, GradNorm=10.880\n",
            "New best model saved! Val Loss: 1.3729 (improvement: 0.0243)\n",
            "Epoch 49 completed. Average loss: 0.6464\n",
            "Epoch 50/150, Step 18825: Train Loss=0.2655, Val Loss=1.3625, LR=6.17e-05, Tokens=2,409,600, GradNorm=14.316\n",
            "New best model saved! Val Loss: 1.3625 (improvement: 0.0103)\n",
            "Epoch 50/150, Step 18850: Train Loss=0.2753, Val Loss=1.4431, LR=6.17e-05, Tokens=2,412,800, GradNorm=8.113\n",
            "Epoch 50/150, Step 18875: Train Loss=0.5014, Val Loss=1.3820, LR=6.16e-05, Tokens=2,416,000, GradNorm=10.813\n",
            "Epoch 50/150, Step 18900: Train Loss=0.4289, Val Loss=1.4009, LR=6.16e-05, Tokens=2,419,200, GradNorm=12.576\n",
            "Epoch 50/150, Step 18925: Train Loss=0.4321, Val Loss=1.4380, LR=6.15e-05, Tokens=2,422,400, GradNorm=16.627\n",
            "Epoch 50/150, Step 18950: Train Loss=0.3735, Val Loss=1.4405, LR=6.15e-05, Tokens=2,425,600, GradNorm=11.010\n",
            "Epoch 50/150, Step 18975: Train Loss=0.2995, Val Loss=1.4150, LR=6.14e-05, Tokens=2,428,800, GradNorm=11.727\n",
            "Epoch 50/150, Step 19000: Train Loss=0.5696, Val Loss=1.4076, LR=6.14e-05, Tokens=2,432,000, GradNorm=7.483\n",
            "Epoch 50/150, Step 19025: Train Loss=0.4755, Val Loss=1.4314, LR=6.13e-05, Tokens=2,435,200, GradNorm=11.857\n",
            "Epoch 50/150, Step 19050: Train Loss=0.4140, Val Loss=1.4634, LR=6.13e-05, Tokens=2,438,400, GradNorm=11.842\n",
            "Epoch 50/150, Step 19075: Train Loss=0.7113, Val Loss=1.4184, LR=6.12e-05, Tokens=2,441,600, GradNorm=10.936\n",
            "Epoch 50/150, Step 19100: Train Loss=0.5576, Val Loss=1.4363, LR=6.12e-05, Tokens=2,444,800, GradNorm=8.936\n",
            "Epoch 50/150, Step 19125: Train Loss=0.4913, Val Loss=1.4321, LR=6.12e-05, Tokens=2,448,000, GradNorm=9.242\n",
            "Epoch 50/150, Step 19150: Train Loss=0.4203, Val Loss=1.3814, LR=6.11e-05, Tokens=2,451,200, GradNorm=9.920\n",
            "Epoch 50/150, Step 19175: Train Loss=0.3533, Val Loss=1.3800, LR=6.11e-05, Tokens=2,454,400, GradNorm=7.163\n",
            "Epoch 50/150, Step 19200: Train Loss=0.7895, Val Loss=1.4231, LR=6.10e-05, Tokens=2,457,600, GradNorm=9.331\n",
            "Epoch 50 completed. Average loss: 0.6183\n",
            "Sample generation after epoch 50:\n",
            "This movie raise her wear heradise  me This could have I dreamed of para night Para-para-para-para-paradise Para-para-paradise Oh,ise Para-para-raya-paradise Paraa-paraparadise Paraaoh-para-par\n",
            "------------------------------------------------------------\n",
            "Epoch 51/150, Step 19225: Train Loss=0.3607, Val Loss=1.4199, LR=6.10e-05, Tokens=2,460,800, GradNorm=11.128\n",
            "Epoch 51/150, Step 19250: Train Loss=0.5587, Val Loss=1.3785, LR=6.09e-05, Tokens=2,464,000, GradNorm=9.686\n",
            "Epoch 51/150, Step 19275: Train Loss=0.4176, Val Loss=1.3644, LR=6.09e-05, Tokens=2,467,200, GradNorm=10.759\n",
            "Epoch 51/150, Step 19300: Train Loss=0.5461, Val Loss=1.3487, LR=6.08e-05, Tokens=2,470,400, GradNorm=10.434\n",
            "New best model saved! Val Loss: 1.3487 (improvement: 0.0138)\n",
            "Epoch 51/150, Step 19325: Train Loss=0.5864, Val Loss=1.3945, LR=6.08e-05, Tokens=2,473,600, GradNorm=9.319\n",
            "Epoch 51/150, Step 19350: Train Loss=0.4198, Val Loss=1.3843, LR=6.07e-05, Tokens=2,476,800, GradNorm=9.532\n",
            "Epoch 51/150, Step 19375: Train Loss=0.5664, Val Loss=1.3633, LR=6.07e-05, Tokens=2,480,000, GradNorm=10.271\n",
            "Epoch 51/150, Step 19400: Train Loss=0.3622, Val Loss=1.3948, LR=6.06e-05, Tokens=2,483,200, GradNorm=7.504\n",
            "Epoch 51/150, Step 19425: Train Loss=0.3952, Val Loss=1.4290, LR=6.06e-05, Tokens=2,486,400, GradNorm=11.487\n",
            "Epoch 51/150, Step 19450: Train Loss=0.2992, Val Loss=1.3766, LR=6.05e-05, Tokens=2,489,600, GradNorm=9.267\n",
            "Epoch 51/150, Step 19475: Train Loss=0.3506, Val Loss=1.3394, LR=6.05e-05, Tokens=2,492,800, GradNorm=13.651\n",
            "New best model saved! Val Loss: 1.3394 (improvement: 0.0093)\n",
            "Epoch 51/150, Step 19500: Train Loss=0.4021, Val Loss=1.3609, LR=6.04e-05, Tokens=2,496,000, GradNorm=10.695\n",
            "Epoch 51/150, Step 19525: Train Loss=0.4460, Val Loss=1.3443, LR=6.04e-05, Tokens=2,499,200, GradNorm=10.179\n",
            "Epoch 51/150, Step 19550: Train Loss=0.4448, Val Loss=1.3304, LR=6.04e-05, Tokens=2,502,400, GradNorm=14.436\n",
            "New best model saved! Val Loss: 1.3304 (improvement: 0.0091)\n",
            "Epoch 51/150, Step 19575: Train Loss=0.3503, Val Loss=1.3288, LR=6.03e-05, Tokens=2,505,600, GradNorm=8.011\n",
            "New best model saved! Val Loss: 1.3288 (improvement: 0.0015)\n",
            "Epoch 51 completed. Average loss: 0.5873\n",
            "Epoch 52/150, Step 19600: Train Loss=0.3053, Val Loss=1.3246, LR=6.03e-05, Tokens=2,508,800, GradNorm=8.921\n",
            "New best model saved! Val Loss: 1.3246 (improvement: 0.0042)\n",
            "Epoch 52/150, Step 19625: Train Loss=0.2865, Val Loss=1.3166, LR=6.02e-05, Tokens=2,512,000, GradNorm=16.830\n",
            "New best model saved! Val Loss: 1.3166 (improvement: 0.0080)\n",
            "Epoch 52/150, Step 19650: Train Loss=0.3599, Val Loss=1.3111, LR=6.02e-05, Tokens=2,515,200, GradNorm=13.359\n",
            "New best model saved! Val Loss: 1.3111 (improvement: 0.0055)\n",
            "Epoch 52/150, Step 19675: Train Loss=0.2298, Val Loss=1.3017, LR=6.01e-05, Tokens=2,518,400, GradNorm=14.873\n",
            "New best model saved! Val Loss: 1.3017 (improvement: 0.0094)\n",
            "Epoch 52/150, Step 19700: Train Loss=0.4039, Val Loss=1.2786, LR=6.01e-05, Tokens=2,521,600, GradNorm=8.952\n",
            "New best model saved! Val Loss: 1.2786 (improvement: 0.0230)\n",
            "Epoch 52/150, Step 19725: Train Loss=0.4864, Val Loss=1.3130, LR=6.00e-05, Tokens=2,524,800, GradNorm=6.741\n",
            "Epoch 52/150, Step 19750: Train Loss=0.4787, Val Loss=1.3264, LR=6.00e-05, Tokens=2,528,000, GradNorm=13.863\n",
            "Epoch 52/150, Step 19775: Train Loss=0.3482, Val Loss=1.2997, LR=5.99e-05, Tokens=2,531,200, GradNorm=7.575\n",
            "Epoch 52/150, Step 19800: Train Loss=0.5139, Val Loss=1.2790, LR=5.99e-05, Tokens=2,534,400, GradNorm=5.229\n",
            "Epoch 52/150, Step 19825: Train Loss=0.7479, Val Loss=1.2887, LR=5.98e-05, Tokens=2,537,600, GradNorm=11.887\n",
            "Epoch 52/150, Step 19850: Train Loss=0.4326, Val Loss=1.2786, LR=5.98e-05, Tokens=2,540,800, GradNorm=10.760\n",
            "Epoch 52/150, Step 19875: Train Loss=0.4475, Val Loss=1.2954, LR=5.97e-05, Tokens=2,544,000, GradNorm=12.483\n",
            "Epoch 52/150, Step 19900: Train Loss=0.4324, Val Loss=1.2965, LR=5.97e-05, Tokens=2,547,200, GradNorm=11.087\n",
            "Epoch 52/150, Step 19925: Train Loss=0.2992, Val Loss=1.3274, LR=5.96e-05, Tokens=2,550,400, GradNorm=7.891\n",
            "Epoch 52/150, Step 19950: Train Loss=0.3435, Val Loss=1.3125, LR=5.96e-05, Tokens=2,553,600, GradNorm=9.407\n",
            "Epoch 52 completed. Average loss: 0.5600\n",
            "Epoch 53/150, Step 19975: Train Loss=0.4626, Val Loss=1.2954, LR=5.95e-05, Tokens=2,556,800, GradNorm=6.986\n",
            "Epoch 53/150, Step 20000: Train Loss=0.2503, Val Loss=1.3017, LR=5.95e-05, Tokens=2,560,000, GradNorm=8.656\n",
            "Epoch 53/150, Step 20025: Train Loss=0.3590, Val Loss=1.3006, LR=5.94e-05, Tokens=2,563,200, GradNorm=9.099\n",
            "Epoch 53/150, Step 20050: Train Loss=0.3212, Val Loss=1.3220, LR=5.94e-05, Tokens=2,566,400, GradNorm=13.736\n",
            "Epoch 53/150, Step 20075: Train Loss=0.4474, Val Loss=1.2702, LR=5.93e-05, Tokens=2,569,600, GradNorm=8.275\n",
            "New best model saved! Val Loss: 1.2702 (improvement: 0.0084)\n",
            "Epoch 53/150, Step 20100: Train Loss=0.3622, Val Loss=1.3049, LR=5.93e-05, Tokens=2,572,800, GradNorm=12.978\n",
            "Epoch 53/150, Step 20125: Train Loss=0.2697, Val Loss=1.2955, LR=5.92e-05, Tokens=2,576,000, GradNorm=12.430\n",
            "Epoch 53/150, Step 20150: Train Loss=0.6346, Val Loss=1.3137, LR=5.92e-05, Tokens=2,579,200, GradNorm=10.966\n",
            "Epoch 53/150, Step 20175: Train Loss=0.4133, Val Loss=1.2699, LR=5.91e-05, Tokens=2,582,400, GradNorm=9.061\n",
            "New best model saved! Val Loss: 1.2699 (improvement: 0.0003)\n",
            "Epoch 53/150, Step 20200: Train Loss=0.3890, Val Loss=1.2590, LR=5.91e-05, Tokens=2,585,600, GradNorm=10.489\n",
            "New best model saved! Val Loss: 1.2590 (improvement: 0.0108)\n",
            "Epoch 53/150, Step 20225: Train Loss=0.3713, Val Loss=1.2652, LR=5.91e-05, Tokens=2,588,800, GradNorm=8.358\n",
            "Epoch 53/150, Step 20250: Train Loss=0.2751, Val Loss=1.2683, LR=5.90e-05, Tokens=2,592,000, GradNorm=6.948\n",
            "Epoch 53/150, Step 20275: Train Loss=0.2815, Val Loss=1.2936, LR=5.90e-05, Tokens=2,595,200, GradNorm=12.734\n",
            "Epoch 53/150, Step 20300: Train Loss=0.4287, Val Loss=1.3802, LR=5.89e-05, Tokens=2,598,400, GradNorm=10.748\n",
            "Epoch 53/150, Step 20325: Train Loss=0.4677, Val Loss=1.3323, LR=5.89e-05, Tokens=2,601,600, GradNorm=9.733\n",
            "Epoch 53/150, Step 20350: Train Loss=0.2232, Val Loss=1.2848, LR=5.88e-05, Tokens=2,604,800, GradNorm=11.708\n",
            "Epoch 53 completed. Average loss: 0.5382\n",
            "Epoch 54/150, Step 20375: Train Loss=0.4229, Val Loss=1.3025, LR=5.88e-05, Tokens=2,608,000, GradNorm=9.294\n",
            "Epoch 54/150, Step 20400: Train Loss=0.4038, Val Loss=1.2542, LR=5.87e-05, Tokens=2,611,200, GradNorm=12.955\n",
            "New best model saved! Val Loss: 1.2542 (improvement: 0.0048)\n",
            "Epoch 54/150, Step 20425: Train Loss=0.3145, Val Loss=1.2577, LR=5.87e-05, Tokens=2,614,400, GradNorm=5.812\n",
            "Epoch 54/150, Step 20450: Train Loss=0.4913, Val Loss=1.2769, LR=5.86e-05, Tokens=2,617,600, GradNorm=20.135\n",
            "Epoch 54/150, Step 20475: Train Loss=0.5427, Val Loss=1.2954, LR=5.86e-05, Tokens=2,620,800, GradNorm=13.303\n",
            "Epoch 54/150, Step 20500: Train Loss=0.2879, Val Loss=1.2342, LR=5.85e-05, Tokens=2,624,000, GradNorm=11.354\n",
            "New best model saved! Val Loss: 1.2342 (improvement: 0.0201)\n",
            "Epoch 54/150, Step 20525: Train Loss=0.3180, Val Loss=1.2714, LR=5.85e-05, Tokens=2,627,200, GradNorm=10.863\n",
            "Epoch 54/150, Step 20550: Train Loss=0.5221, Val Loss=1.2592, LR=5.84e-05, Tokens=2,630,400, GradNorm=15.600\n",
            "Epoch 54/150, Step 20575: Train Loss=0.4354, Val Loss=1.2271, LR=5.84e-05, Tokens=2,633,600, GradNorm=11.948\n",
            "New best model saved! Val Loss: 1.2271 (improvement: 0.0071)\n",
            "Epoch 54/150, Step 20600: Train Loss=0.2934, Val Loss=1.1922, LR=5.83e-05, Tokens=2,636,800, GradNorm=12.882\n",
            "New best model saved! Val Loss: 1.1922 (improvement: 0.0349)\n",
            "Epoch 54/150, Step 20625: Train Loss=0.3617, Val Loss=1.2085, LR=5.83e-05, Tokens=2,640,000, GradNorm=11.692\n",
            "Epoch 54/150, Step 20650: Train Loss=0.6050, Val Loss=1.2264, LR=5.82e-05, Tokens=2,643,200, GradNorm=7.938\n",
            "Epoch 54/150, Step 20675: Train Loss=0.3234, Val Loss=1.2310, LR=5.82e-05, Tokens=2,646,400, GradNorm=6.901\n",
            "Epoch 54/150, Step 20700: Train Loss=0.3355, Val Loss=1.2360, LR=5.81e-05, Tokens=2,649,600, GradNorm=7.119\n",
            "Epoch 54/150, Step 20725: Train Loss=0.3506, Val Loss=1.2055, LR=5.81e-05, Tokens=2,652,800, GradNorm=9.671\n",
            "Epoch 54 completed. Average loss: 0.5131\n",
            "Epoch 55/150, Step 20750: Train Loss=0.3582, Val Loss=1.2399, LR=5.80e-05, Tokens=2,656,000, GradNorm=12.562\n",
            "Epoch 55/150, Step 20775: Train Loss=0.3207, Val Loss=1.2248, LR=5.80e-05, Tokens=2,659,200, GradNorm=13.194\n",
            "Epoch 55/150, Step 20800: Train Loss=0.1998, Val Loss=1.2529, LR=5.79e-05, Tokens=2,662,400, GradNorm=8.768\n",
            "Epoch 55/150, Step 20825: Train Loss=0.4860, Val Loss=1.2336, LR=5.79e-05, Tokens=2,665,600, GradNorm=13.718\n",
            "Epoch 55/150, Step 20850: Train Loss=0.4135, Val Loss=1.2135, LR=5.78e-05, Tokens=2,668,800, GradNorm=11.359\n",
            "Epoch 55/150, Step 20875: Train Loss=0.4996, Val Loss=1.2371, LR=5.78e-05, Tokens=2,672,000, GradNorm=12.507\n",
            "Epoch 55/150, Step 20900: Train Loss=0.3377, Val Loss=1.2230, LR=5.77e-05, Tokens=2,675,200, GradNorm=8.211\n",
            "Epoch 55/150, Step 20925: Train Loss=0.3041, Val Loss=1.2579, LR=5.77e-05, Tokens=2,678,400, GradNorm=9.624\n",
            "Epoch 55/150, Step 20950: Train Loss=0.2824, Val Loss=1.2264, LR=5.76e-05, Tokens=2,681,600, GradNorm=11.959\n",
            "Epoch 55/150, Step 20975: Train Loss=0.3216, Val Loss=1.2380, LR=5.76e-05, Tokens=2,684,800, GradNorm=7.129\n",
            "Epoch 55/150, Step 21000: Train Loss=0.4699, Val Loss=1.2507, LR=5.75e-05, Tokens=2,688,000, GradNorm=9.766\n",
            "Epoch 55/150, Step 21025: Train Loss=0.3556, Val Loss=1.2223, LR=5.75e-05, Tokens=2,691,200, GradNorm=11.040\n",
            "Epoch 55/150, Step 21050: Train Loss=0.2866, Val Loss=1.2544, LR=5.74e-05, Tokens=2,694,400, GradNorm=10.756\n",
            "Epoch 55/150, Step 21075: Train Loss=0.4139, Val Loss=1.2793, LR=5.74e-05, Tokens=2,697,600, GradNorm=13.200\n",
            "Epoch 55/150, Step 21100: Train Loss=0.3654, Val Loss=1.2355, LR=5.73e-05, Tokens=2,700,800, GradNorm=9.111\n",
            "Epoch 55 completed. Average loss: 0.4862\n",
            "Epoch 56/150, Step 21125: Train Loss=0.2753, Val Loss=1.2058, LR=5.73e-05, Tokens=2,704,000, GradNorm=9.464\n",
            "Epoch 56/150, Step 21150: Train Loss=0.2155, Val Loss=1.2055, LR=5.72e-05, Tokens=2,707,200, GradNorm=11.483\n",
            "Epoch 56/150, Step 21175: Train Loss=0.4202, Val Loss=1.2448, LR=5.72e-05, Tokens=2,710,400, GradNorm=10.816\n",
            "Epoch 56/150, Step 21200: Train Loss=0.5073, Val Loss=1.2345, LR=5.71e-05, Tokens=2,713,600, GradNorm=8.368\n",
            "Epoch 56/150, Step 21225: Train Loss=0.2779, Val Loss=1.2240, LR=5.71e-05, Tokens=2,716,800, GradNorm=14.774\n",
            "Epoch 56/150, Step 21250: Train Loss=0.2022, Val Loss=1.2603, LR=5.70e-05, Tokens=2,720,000, GradNorm=7.800\n",
            "Epoch 56/150, Step 21275: Train Loss=0.4816, Val Loss=1.2012, LR=5.70e-05, Tokens=2,723,200, GradNorm=10.774\n",
            "Epoch 56/150, Step 21300: Train Loss=0.1292, Val Loss=1.1921, LR=5.69e-05, Tokens=2,726,400, GradNorm=10.281\n",
            "Epoch 56/150, Step 21325: Train Loss=0.2978, Val Loss=1.1898, LR=5.69e-05, Tokens=2,729,600, GradNorm=7.692\n",
            "New best model saved! Val Loss: 1.1898 (improvement: 0.0023)\n",
            "Epoch 56/150, Step 21350: Train Loss=0.3086, Val Loss=1.1861, LR=5.68e-05, Tokens=2,732,800, GradNorm=11.083\n",
            "New best model saved! Val Loss: 1.1861 (improvement: 0.0038)\n",
            "Epoch 56/150, Step 21375: Train Loss=0.5001, Val Loss=1.1688, LR=5.68e-05, Tokens=2,736,000, GradNorm=10.349\n",
            "New best model saved! Val Loss: 1.1688 (improvement: 0.0172)\n",
            "Epoch 56/150, Step 21400: Train Loss=0.4819, Val Loss=1.1649, LR=5.67e-05, Tokens=2,739,200, GradNorm=7.463\n",
            "New best model saved! Val Loss: 1.1649 (improvement: 0.0039)\n",
            "Epoch 56/150, Step 21425: Train Loss=0.3552, Val Loss=1.1998, LR=5.67e-05, Tokens=2,742,400, GradNorm=13.026\n",
            "Epoch 56/150, Step 21450: Train Loss=0.3127, Val Loss=1.1953, LR=5.66e-05, Tokens=2,745,600, GradNorm=9.351\n",
            "Epoch 56/150, Step 21475: Train Loss=0.3317, Val Loss=1.2023, LR=5.66e-05, Tokens=2,748,800, GradNorm=7.330\n",
            "Epoch 56/150, Step 21500: Train Loss=0.2264, Val Loss=1.2044, LR=5.65e-05, Tokens=2,752,000, GradNorm=12.651\n",
            "Epoch 56 completed. Average loss: 0.4628\n",
            "Epoch 57/150, Step 21525: Train Loss=0.3520, Val Loss=1.1889, LR=5.65e-05, Tokens=2,755,200, GradNorm=7.630\n",
            "Epoch 57/150, Step 21550: Train Loss=0.3200, Val Loss=1.2085, LR=5.64e-05, Tokens=2,758,400, GradNorm=7.074\n",
            "Epoch 57/150, Step 21575: Train Loss=0.4553, Val Loss=1.2205, LR=5.64e-05, Tokens=2,761,600, GradNorm=11.272\n",
            "Epoch 57/150, Step 21600: Train Loss=0.4623, Val Loss=1.2007, LR=5.63e-05, Tokens=2,764,800, GradNorm=10.298\n",
            "Epoch 57/150, Step 21625: Train Loss=0.2561, Val Loss=1.1718, LR=5.63e-05, Tokens=2,768,000, GradNorm=6.011\n",
            "Epoch 57/150, Step 21650: Train Loss=0.3218, Val Loss=1.2191, LR=5.62e-05, Tokens=2,771,200, GradNorm=6.911\n",
            "Epoch 57/150, Step 21675: Train Loss=0.2676, Val Loss=1.1438, LR=5.62e-05, Tokens=2,774,400, GradNorm=9.324\n",
            "New best model saved! Val Loss: 1.1438 (improvement: 0.0212)\n",
            "Epoch 57/150, Step 21700: Train Loss=0.3494, Val Loss=1.1747, LR=5.61e-05, Tokens=2,777,600, GradNorm=8.352\n",
            "Epoch 57/150, Step 21725: Train Loss=0.1912, Val Loss=1.1403, LR=5.61e-05, Tokens=2,780,800, GradNorm=10.176\n",
            "New best model saved! Val Loss: 1.1403 (improvement: 0.0035)\n",
            "Epoch 57/150, Step 21750: Train Loss=0.2662, Val Loss=1.1733, LR=5.60e-05, Tokens=2,784,000, GradNorm=8.909\n",
            "Epoch 57/150, Step 21775: Train Loss=0.4343, Val Loss=1.1705, LR=5.60e-05, Tokens=2,787,200, GradNorm=10.100\n",
            "Epoch 57/150, Step 21800: Train Loss=0.2092, Val Loss=1.1581, LR=5.59e-05, Tokens=2,790,400, GradNorm=9.929\n",
            "Epoch 57/150, Step 21825: Train Loss=0.2691, Val Loss=1.1573, LR=5.59e-05, Tokens=2,793,600, GradNorm=13.120\n",
            "Epoch 57/150, Step 21850: Train Loss=0.2025, Val Loss=1.1680, LR=5.58e-05, Tokens=2,796,800, GradNorm=8.154\n",
            "Epoch 57/150, Step 21875: Train Loss=0.1644, Val Loss=1.1908, LR=5.58e-05, Tokens=2,800,000, GradNorm=12.011\n",
            "Epoch 57 completed. Average loss: 0.4433\n",
            "Epoch 58/150, Step 21900: Train Loss=0.3006, Val Loss=1.1742, LR=5.57e-05, Tokens=2,803,200, GradNorm=15.004\n",
            "Epoch 58/150, Step 21925: Train Loss=0.3422, Val Loss=1.1772, LR=5.57e-05, Tokens=2,806,400, GradNorm=11.567\n",
            "Epoch 58/150, Step 21950: Train Loss=0.4834, Val Loss=1.1936, LR=5.56e-05, Tokens=2,809,600, GradNorm=6.303\n",
            "Epoch 58/150, Step 21975: Train Loss=0.3241, Val Loss=1.1961, LR=5.56e-05, Tokens=2,812,800, GradNorm=8.093\n",
            "Epoch 58/150, Step 22000: Train Loss=0.1687, Val Loss=1.1872, LR=5.55e-05, Tokens=2,816,000, GradNorm=10.706\n",
            "Epoch 58/150, Step 22025: Train Loss=0.1785, Val Loss=1.1576, LR=5.55e-05, Tokens=2,819,200, GradNorm=16.777\n",
            "Epoch 58/150, Step 22050: Train Loss=0.4510, Val Loss=1.1445, LR=5.54e-05, Tokens=2,822,400, GradNorm=12.210\n",
            "Epoch 58/150, Step 22075: Train Loss=0.2810, Val Loss=1.1691, LR=5.54e-05, Tokens=2,825,600, GradNorm=13.055\n",
            "Epoch 58/150, Step 22100: Train Loss=0.3606, Val Loss=1.2018, LR=5.53e-05, Tokens=2,828,800, GradNorm=7.300\n",
            "Epoch 58/150, Step 22125: Train Loss=0.4555, Val Loss=1.1415, LR=5.53e-05, Tokens=2,832,000, GradNorm=5.893\n",
            "Epoch 58/150, Step 22150: Train Loss=0.3579, Val Loss=1.1578, LR=5.52e-05, Tokens=2,835,200, GradNorm=8.642\n",
            "Epoch 58/150, Step 22175: Train Loss=0.2137, Val Loss=1.1160, LR=5.52e-05, Tokens=2,838,400, GradNorm=9.199\n",
            "New best model saved! Val Loss: 1.1160 (improvement: 0.0243)\n",
            "Epoch 58/150, Step 22200: Train Loss=0.3436, Val Loss=1.1469, LR=5.51e-05, Tokens=2,841,600, GradNorm=6.599\n",
            "Epoch 58/150, Step 22225: Train Loss=0.3767, Val Loss=1.1811, LR=5.51e-05, Tokens=2,844,800, GradNorm=4.683\n",
            "Epoch 58/150, Step 22250: Train Loss=0.3734, Val Loss=1.1475, LR=5.50e-05, Tokens=2,848,000, GradNorm=5.248\n",
            "Epoch 58 completed. Average loss: 0.4270\n",
            "Epoch 59/150, Step 22275: Train Loss=0.1906, Val Loss=1.1247, LR=5.49e-05, Tokens=2,851,200, GradNorm=6.583\n",
            "Epoch 59/150, Step 22300: Train Loss=0.4893, Val Loss=1.1162, LR=5.49e-05, Tokens=2,854,400, GradNorm=7.033\n",
            "Epoch 59/150, Step 22325: Train Loss=0.3355, Val Loss=1.1315, LR=5.48e-05, Tokens=2,857,600, GradNorm=9.877\n",
            "Epoch 59/150, Step 22350: Train Loss=0.2770, Val Loss=1.1083, LR=5.48e-05, Tokens=2,860,800, GradNorm=8.826\n",
            "New best model saved! Val Loss: 1.1083 (improvement: 0.0077)\n",
            "Epoch 59/150, Step 22375: Train Loss=0.3172, Val Loss=1.1039, LR=5.47e-05, Tokens=2,864,000, GradNorm=5.198\n",
            "New best model saved! Val Loss: 1.1039 (improvement: 0.0044)\n",
            "Epoch 59/150, Step 22400: Train Loss=0.1926, Val Loss=1.1077, LR=5.47e-05, Tokens=2,867,200, GradNorm=6.280\n",
            "Epoch 59/150, Step 22425: Train Loss=0.1437, Val Loss=1.1335, LR=5.46e-05, Tokens=2,870,400, GradNorm=10.269\n",
            "Epoch 59/150, Step 22450: Train Loss=0.3166, Val Loss=1.1318, LR=5.46e-05, Tokens=2,873,600, GradNorm=9.365\n",
            "Epoch 59/150, Step 22475: Train Loss=0.1514, Val Loss=1.1397, LR=5.45e-05, Tokens=2,876,800, GradNorm=6.908\n",
            "Epoch 59/150, Step 22500: Train Loss=0.1815, Val Loss=1.1668, LR=5.45e-05, Tokens=2,880,000, GradNorm=11.577\n",
            "Epoch 59/150, Step 22525: Train Loss=0.4733, Val Loss=1.1558, LR=5.44e-05, Tokens=2,883,200, GradNorm=15.302\n",
            "Epoch 59/150, Step 22550: Train Loss=0.3710, Val Loss=1.1201, LR=5.44e-05, Tokens=2,886,400, GradNorm=12.268\n",
            "Epoch 59/150, Step 22575: Train Loss=0.4041, Val Loss=1.1415, LR=5.43e-05, Tokens=2,889,600, GradNorm=12.273\n",
            "Epoch 59/150, Step 22600: Train Loss=0.1689, Val Loss=1.1410, LR=5.43e-05, Tokens=2,892,800, GradNorm=9.128\n",
            "Epoch 59/150, Step 22625: Train Loss=0.2607, Val Loss=1.0870, LR=5.42e-05, Tokens=2,896,000, GradNorm=8.210\n",
            "New best model saved! Val Loss: 1.0870 (improvement: 0.0169)\n",
            "Epoch 59/150, Step 22650: Train Loss=0.4310, Val Loss=1.1036, LR=5.42e-05, Tokens=2,899,200, GradNorm=10.467\n",
            "Epoch 59 completed. Average loss: 0.4095\n",
            "Epoch 60/150, Step 22675: Train Loss=0.2831, Val Loss=1.0870, LR=5.41e-05, Tokens=2,902,400, GradNorm=7.922\n",
            "Epoch 60/150, Step 22700: Train Loss=0.3573, Val Loss=1.1024, LR=5.41e-05, Tokens=2,905,600, GradNorm=13.729\n",
            "Epoch 60/150, Step 22725: Train Loss=0.2222, Val Loss=1.1096, LR=5.40e-05, Tokens=2,908,800, GradNorm=5.979\n",
            "Epoch 60/150, Step 22750: Train Loss=0.4267, Val Loss=1.1290, LR=5.40e-05, Tokens=2,912,000, GradNorm=13.339\n",
            "Epoch 60/150, Step 22775: Train Loss=0.2196, Val Loss=1.1187, LR=5.39e-05, Tokens=2,915,200, GradNorm=11.129\n",
            "Epoch 60/150, Step 22800: Train Loss=0.3573, Val Loss=1.1097, LR=5.39e-05, Tokens=2,918,400, GradNorm=8.054\n",
            "Epoch 60/150, Step 22825: Train Loss=0.2458, Val Loss=1.1164, LR=5.38e-05, Tokens=2,921,600, GradNorm=8.067\n",
            "Epoch 60/150, Step 22850: Train Loss=0.1130, Val Loss=1.0710, LR=5.38e-05, Tokens=2,924,800, GradNorm=14.012\n",
            "New best model saved! Val Loss: 1.0710 (improvement: 0.0160)\n",
            "Epoch 60/150, Step 22875: Train Loss=0.1266, Val Loss=1.0865, LR=5.37e-05, Tokens=2,928,000, GradNorm=13.831\n",
            "Epoch 60/150, Step 22900: Train Loss=0.3188, Val Loss=1.1068, LR=5.37e-05, Tokens=2,931,200, GradNorm=7.979\n",
            "Epoch 60/150, Step 22925: Train Loss=0.3724, Val Loss=1.1082, LR=5.36e-05, Tokens=2,934,400, GradNorm=16.583\n",
            "Epoch 60/150, Step 22950: Train Loss=0.2825, Val Loss=1.1202, LR=5.36e-05, Tokens=2,937,600, GradNorm=7.374\n",
            "Epoch 60/150, Step 22975: Train Loss=0.2150, Val Loss=1.1319, LR=5.35e-05, Tokens=2,940,800, GradNorm=7.792\n",
            "Epoch 60/150, Step 23000: Train Loss=0.2598, Val Loss=1.0892, LR=5.34e-05, Tokens=2,944,000, GradNorm=5.658\n",
            "Epoch 60/150, Step 23025: Train Loss=0.2789, Val Loss=1.0628, LR=5.34e-05, Tokens=2,947,200, GradNorm=9.997\n",
            "New best model saved! Val Loss: 1.0628 (improvement: 0.0082)\n",
            "Epoch 60 completed. Average loss: 0.3914\n",
            "Sample generation after epoch 60:\n",
            "This moviea-paradise This could be para-para-paradise Oh, oh-oh, oh-oh, oh-oh-oh This could be para-para-paradise This could be para-para-para-paradise This could be para-para-para-para-para-para\n",
            "------------------------------------------------------------\n",
            "Epoch 61/150, Step 23050: Train Loss=0.2985, Val Loss=1.0832, LR=5.33e-05, Tokens=2,950,400, GradNorm=13.528\n",
            "Epoch 61/150, Step 23075: Train Loss=0.3056, Val Loss=1.0836, LR=5.33e-05, Tokens=2,953,600, GradNorm=7.802\n",
            "Epoch 61/150, Step 23100: Train Loss=0.3209, Val Loss=1.0749, LR=5.32e-05, Tokens=2,956,800, GradNorm=5.117\n",
            "Epoch 61/150, Step 23125: Train Loss=0.2520, Val Loss=1.0815, LR=5.32e-05, Tokens=2,960,000, GradNorm=6.361\n",
            "Epoch 61/150, Step 23150: Train Loss=0.2609, Val Loss=1.0984, LR=5.31e-05, Tokens=2,963,200, GradNorm=5.503\n",
            "Epoch 61/150, Step 23175: Train Loss=0.2104, Val Loss=1.1296, LR=5.31e-05, Tokens=2,966,400, GradNorm=7.099\n",
            "Epoch 61/150, Step 23200: Train Loss=0.3524, Val Loss=1.0934, LR=5.30e-05, Tokens=2,969,600, GradNorm=10.714\n",
            "Epoch 61/150, Step 23225: Train Loss=0.3038, Val Loss=1.0921, LR=5.30e-05, Tokens=2,972,800, GradNorm=11.400\n",
            "Epoch 61/150, Step 23250: Train Loss=0.2761, Val Loss=1.1044, LR=5.29e-05, Tokens=2,976,000, GradNorm=10.244\n",
            "Epoch 61/150, Step 23275: Train Loss=0.3619, Val Loss=1.0678, LR=5.29e-05, Tokens=2,979,200, GradNorm=14.083\n",
            "Epoch 61/150, Step 23300: Train Loss=0.3012, Val Loss=1.1046, LR=5.28e-05, Tokens=2,982,400, GradNorm=11.943\n",
            "Epoch 61/150, Step 23325: Train Loss=0.2336, Val Loss=1.0572, LR=5.28e-05, Tokens=2,985,600, GradNorm=7.162\n",
            "New best model saved! Val Loss: 1.0572 (improvement: 0.0056)\n",
            "Epoch 61/150, Step 23350: Train Loss=0.3439, Val Loss=1.0425, LR=5.27e-05, Tokens=2,988,800, GradNorm=12.140\n",
            "New best model saved! Val Loss: 1.0425 (improvement: 0.0147)\n",
            "Epoch 61/150, Step 23375: Train Loss=0.2023, Val Loss=1.0680, LR=5.27e-05, Tokens=2,992,000, GradNorm=8.670\n",
            "Epoch 61/150, Step 23400: Train Loss=0.2191, Val Loss=1.0907, LR=5.26e-05, Tokens=2,995,200, GradNorm=9.850\n",
            "Epoch 61 completed. Average loss: 0.3784\n",
            "Epoch 62/150, Step 23425: Train Loss=0.2639, Val Loss=1.0516, LR=5.26e-05, Tokens=2,998,400, GradNorm=5.870\n",
            "Epoch 62/150, Step 23450: Train Loss=0.1188, Val Loss=1.0692, LR=5.25e-05, Tokens=3,001,600, GradNorm=7.075\n",
            "Epoch 62/150, Step 23475: Train Loss=0.3143, Val Loss=1.0457, LR=5.25e-05, Tokens=3,004,800, GradNorm=9.104\n",
            "Epoch 62/150, Step 23500: Train Loss=0.1435, Val Loss=1.0498, LR=5.24e-05, Tokens=3,008,000, GradNorm=7.501\n",
            "Epoch 62/150, Step 23525: Train Loss=0.3691, Val Loss=1.0627, LR=5.24e-05, Tokens=3,011,200, GradNorm=9.703\n",
            "Epoch 62/150, Step 23550: Train Loss=0.3651, Val Loss=1.0556, LR=5.23e-05, Tokens=3,014,400, GradNorm=12.616\n",
            "Epoch 62/150, Step 23575: Train Loss=0.1109, Val Loss=1.0710, LR=5.22e-05, Tokens=3,017,600, GradNorm=8.908\n",
            "Epoch 62/150, Step 23600: Train Loss=0.3175, Val Loss=1.0940, LR=5.22e-05, Tokens=3,020,800, GradNorm=13.362\n",
            "Epoch 62/150, Step 23625: Train Loss=0.2269, Val Loss=1.0603, LR=5.21e-05, Tokens=3,024,000, GradNorm=5.858\n",
            "Epoch 62/150, Step 23650: Train Loss=0.2408, Val Loss=0.9975, LR=5.21e-05, Tokens=3,027,200, GradNorm=7.550\n",
            "New best model saved! Val Loss: 0.9975 (improvement: 0.0450)\n",
            "Epoch 62/150, Step 23675: Train Loss=0.2039, Val Loss=1.0120, LR=5.20e-05, Tokens=3,030,400, GradNorm=13.406\n",
            "Epoch 62/150, Step 23700: Train Loss=0.1499, Val Loss=1.0265, LR=5.20e-05, Tokens=3,033,600, GradNorm=10.347\n",
            "Epoch 62/150, Step 23725: Train Loss=0.2934, Val Loss=1.0552, LR=5.19e-05, Tokens=3,036,800, GradNorm=9.269\n",
            "Epoch 62/150, Step 23750: Train Loss=0.2994, Val Loss=1.0881, LR=5.19e-05, Tokens=3,040,000, GradNorm=11.617\n",
            "Epoch 62/150, Step 23775: Train Loss=0.4595, Val Loss=1.0597, LR=5.18e-05, Tokens=3,043,200, GradNorm=3.927\n",
            "Epoch 62/150, Step 23800: Train Loss=0.1647, Val Loss=1.0129, LR=5.18e-05, Tokens=3,046,400, GradNorm=5.467\n",
            "Epoch 62 completed. Average loss: 0.3611\n",
            "Epoch 63/150, Step 23825: Train Loss=0.3099, Val Loss=1.0238, LR=5.17e-05, Tokens=3,049,600, GradNorm=9.406\n",
            "Epoch 63/150, Step 23850: Train Loss=0.1269, Val Loss=1.0212, LR=5.17e-05, Tokens=3,052,800, GradNorm=4.836\n",
            "Epoch 63/150, Step 23875: Train Loss=0.1903, Val Loss=1.0215, LR=5.16e-05, Tokens=3,056,000, GradNorm=7.416\n",
            "Epoch 63/150, Step 23900: Train Loss=0.2944, Val Loss=1.0448, LR=5.16e-05, Tokens=3,059,200, GradNorm=9.650\n",
            "Epoch 63/150, Step 23925: Train Loss=0.2936, Val Loss=1.0608, LR=5.15e-05, Tokens=3,062,400, GradNorm=12.285\n",
            "Epoch 63/150, Step 23950: Train Loss=0.3757, Val Loss=1.0257, LR=5.15e-05, Tokens=3,065,600, GradNorm=7.971\n",
            "Epoch 63/150, Step 23975: Train Loss=0.3027, Val Loss=1.0374, LR=5.14e-05, Tokens=3,068,800, GradNorm=8.869\n",
            "Epoch 63/150, Step 24000: Train Loss=0.2425, Val Loss=1.0137, LR=5.13e-05, Tokens=3,072,000, GradNorm=5.770\n",
            "Epoch 63/150, Step 24025: Train Loss=0.2710, Val Loss=0.9928, LR=5.13e-05, Tokens=3,075,200, GradNorm=8.411\n",
            "New best model saved! Val Loss: 0.9928 (improvement: 0.0047)\n",
            "Epoch 63/150, Step 24050: Train Loss=0.3211, Val Loss=1.0185, LR=5.12e-05, Tokens=3,078,400, GradNorm=9.727\n",
            "Epoch 63/150, Step 24075: Train Loss=0.1224, Val Loss=1.0296, LR=5.12e-05, Tokens=3,081,600, GradNorm=7.636\n",
            "Epoch 63/150, Step 24100: Train Loss=0.2322, Val Loss=1.0002, LR=5.11e-05, Tokens=3,084,800, GradNorm=5.395\n",
            "Epoch 63/150, Step 24125: Train Loss=0.3396, Val Loss=1.0006, LR=5.11e-05, Tokens=3,088,000, GradNorm=6.495\n",
            "Epoch 63/150, Step 24150: Train Loss=0.1408, Val Loss=0.9742, LR=5.10e-05, Tokens=3,091,200, GradNorm=9.710\n",
            "New best model saved! Val Loss: 0.9742 (improvement: 0.0186)\n",
            "Epoch 63/150, Step 24175: Train Loss=0.2104, Val Loss=1.0056, LR=5.10e-05, Tokens=3,094,400, GradNorm=11.827\n",
            "Epoch 63 completed. Average loss: 0.3457\n",
            "Epoch 64/150, Step 24200: Train Loss=0.3540, Val Loss=1.0016, LR=5.09e-05, Tokens=3,097,600, GradNorm=6.018\n",
            "Epoch 64/150, Step 24225: Train Loss=0.5032, Val Loss=0.9898, LR=5.09e-05, Tokens=3,100,800, GradNorm=6.171\n",
            "Epoch 64/150, Step 24250: Train Loss=0.1763, Val Loss=1.0145, LR=5.08e-05, Tokens=3,104,000, GradNorm=7.156\n",
            "Epoch 64/150, Step 24275: Train Loss=0.2229, Val Loss=1.0092, LR=5.08e-05, Tokens=3,107,200, GradNorm=10.011\n",
            "Epoch 64/150, Step 24300: Train Loss=0.1951, Val Loss=0.9946, LR=5.07e-05, Tokens=3,110,400, GradNorm=9.565\n",
            "Epoch 64/150, Step 24325: Train Loss=0.1304, Val Loss=0.9926, LR=5.07e-05, Tokens=3,113,600, GradNorm=6.020\n",
            "Epoch 64/150, Step 24350: Train Loss=0.2246, Val Loss=0.9810, LR=5.06e-05, Tokens=3,116,800, GradNorm=5.760\n",
            "Epoch 64/150, Step 24375: Train Loss=0.1765, Val Loss=1.0075, LR=5.05e-05, Tokens=3,120,000, GradNorm=4.633\n",
            "Epoch 64/150, Step 24400: Train Loss=0.1617, Val Loss=1.0214, LR=5.05e-05, Tokens=3,123,200, GradNorm=8.837\n",
            "Epoch 64/150, Step 24425: Train Loss=0.1933, Val Loss=1.0050, LR=5.04e-05, Tokens=3,126,400, GradNorm=6.721\n",
            "Epoch 64/150, Step 24450: Train Loss=0.3474, Val Loss=0.9798, LR=5.04e-05, Tokens=3,129,600, GradNorm=8.130\n",
            "Epoch 64/150, Step 24475: Train Loss=0.1850, Val Loss=0.9681, LR=5.03e-05, Tokens=3,132,800, GradNorm=11.237\n",
            "New best model saved! Val Loss: 0.9681 (improvement: 0.0061)\n",
            "Epoch 64/150, Step 24500: Train Loss=0.1082, Val Loss=0.9803, LR=5.03e-05, Tokens=3,136,000, GradNorm=10.770\n",
            "Epoch 64/150, Step 24525: Train Loss=0.1471, Val Loss=0.9943, LR=5.02e-05, Tokens=3,139,200, GradNorm=11.101\n",
            "Epoch 64/150, Step 24550: Train Loss=0.2206, Val Loss=1.0118, LR=5.02e-05, Tokens=3,142,400, GradNorm=7.245\n",
            "Epoch 64/150, Step 24575: Train Loss=0.4439, Val Loss=0.9976, LR=5.01e-05, Tokens=3,145,600, GradNorm=9.885\n",
            "Epoch 64 completed. Average loss: 0.3372\n",
            "Epoch 65/150, Step 24600: Train Loss=0.1657, Val Loss=1.0069, LR=5.01e-05, Tokens=3,148,800, GradNorm=7.372\n",
            "Epoch 65/150, Step 24625: Train Loss=0.3750, Val Loss=0.9982, LR=5.00e-05, Tokens=3,152,000, GradNorm=7.102\n",
            "Epoch 65/150, Step 24650: Train Loss=0.1972, Val Loss=0.9772, LR=5.00e-05, Tokens=3,155,200, GradNorm=7.663\n",
            "Epoch 65/150, Step 24675: Train Loss=0.1633, Val Loss=0.9708, LR=4.99e-05, Tokens=3,158,400, GradNorm=4.055\n",
            "Epoch 65/150, Step 24700: Train Loss=0.1764, Val Loss=0.9774, LR=4.99e-05, Tokens=3,161,600, GradNorm=3.642\n",
            "Epoch 65/150, Step 24725: Train Loss=0.3271, Val Loss=0.9776, LR=4.98e-05, Tokens=3,164,800, GradNorm=6.724\n",
            "Epoch 65/150, Step 24750: Train Loss=0.1608, Val Loss=0.9645, LR=4.97e-05, Tokens=3,168,000, GradNorm=9.545\n",
            "New best model saved! Val Loss: 0.9645 (improvement: 0.0036)\n",
            "Epoch 65/150, Step 24775: Train Loss=0.2159, Val Loss=0.9716, LR=4.97e-05, Tokens=3,171,200, GradNorm=8.426\n",
            "Epoch 65/150, Step 24800: Train Loss=0.2041, Val Loss=0.9253, LR=4.96e-05, Tokens=3,174,400, GradNorm=4.643\n",
            "New best model saved! Val Loss: 0.9253 (improvement: 0.0391)\n",
            "Epoch 65/150, Step 24825: Train Loss=0.1314, Val Loss=0.9392, LR=4.96e-05, Tokens=3,177,600, GradNorm=6.216\n",
            "Epoch 65/150, Step 24850: Train Loss=0.1637, Val Loss=0.9328, LR=4.95e-05, Tokens=3,180,800, GradNorm=5.673\n",
            "Epoch 65/150, Step 24875: Train Loss=0.4051, Val Loss=0.9913, LR=4.95e-05, Tokens=3,184,000, GradNorm=4.321\n",
            "Epoch 65/150, Step 24900: Train Loss=0.1675, Val Loss=0.9130, LR=4.94e-05, Tokens=3,187,200, GradNorm=7.847\n",
            "New best model saved! Val Loss: 0.9130 (improvement: 0.0123)\n",
            "Epoch 65/150, Step 24925: Train Loss=0.2625, Val Loss=0.9371, LR=4.94e-05, Tokens=3,190,400, GradNorm=5.959\n",
            "Epoch 65/150, Step 24950: Train Loss=0.2332, Val Loss=0.9396, LR=4.93e-05, Tokens=3,193,600, GradNorm=12.186\n",
            "Epoch 65 completed. Average loss: 0.3221\n",
            "Epoch 66/150, Step 24975: Train Loss=0.1628, Val Loss=0.9208, LR=4.93e-05, Tokens=3,196,800, GradNorm=5.663\n",
            "Epoch 66/150, Step 25000: Train Loss=0.2826, Val Loss=0.9273, LR=4.92e-05, Tokens=3,200,000, GradNorm=5.378\n",
            "Epoch 66/150, Step 25025: Train Loss=0.1012, Val Loss=0.9090, LR=4.92e-05, Tokens=3,203,200, GradNorm=9.563\n",
            "New best model saved! Val Loss: 0.9090 (improvement: 0.0040)\n",
            "Epoch 66/150, Step 25050: Train Loss=0.1517, Val Loss=0.9172, LR=4.91e-05, Tokens=3,206,400, GradNorm=8.422\n",
            "Epoch 66/150, Step 25075: Train Loss=0.2920, Val Loss=0.9783, LR=4.90e-05, Tokens=3,209,600, GradNorm=5.389\n",
            "Epoch 66/150, Step 25100: Train Loss=0.2800, Val Loss=0.9316, LR=4.90e-05, Tokens=3,212,800, GradNorm=4.968\n",
            "Epoch 66/150, Step 25125: Train Loss=0.1509, Val Loss=0.9698, LR=4.89e-05, Tokens=3,216,000, GradNorm=11.427\n",
            "Epoch 66/150, Step 25150: Train Loss=0.1632, Val Loss=0.9403, LR=4.89e-05, Tokens=3,219,200, GradNorm=5.934\n",
            "Epoch 66/150, Step 25175: Train Loss=0.3129, Val Loss=0.9381, LR=4.88e-05, Tokens=3,222,400, GradNorm=5.083\n",
            "Epoch 66/150, Step 25200: Train Loss=0.3169, Val Loss=0.9643, LR=4.88e-05, Tokens=3,225,600, GradNorm=7.307\n",
            "Epoch 66/150, Step 25225: Train Loss=0.1706, Val Loss=0.9646, LR=4.87e-05, Tokens=3,228,800, GradNorm=12.407\n",
            "Epoch 66/150, Step 25250: Train Loss=0.1746, Val Loss=0.9354, LR=4.87e-05, Tokens=3,232,000, GradNorm=6.262\n",
            "Epoch 66/150, Step 25275: Train Loss=0.1672, Val Loss=0.9497, LR=4.86e-05, Tokens=3,235,200, GradNorm=6.710\n",
            "Epoch 66/150, Step 25300: Train Loss=0.2882, Val Loss=0.9406, LR=4.86e-05, Tokens=3,238,400, GradNorm=8.317\n",
            "Epoch 66/150, Step 25325: Train Loss=0.2123, Val Loss=0.9218, LR=4.85e-05, Tokens=3,241,600, GradNorm=7.170\n",
            "Epoch 66 completed. Average loss: 0.3066\n",
            "Epoch 67/150, Step 25350: Train Loss=0.2404, Val Loss=0.9524, LR=4.85e-05, Tokens=3,244,800, GradNorm=10.186\n",
            "Epoch 67/150, Step 25375: Train Loss=0.1191, Val Loss=0.9651, LR=4.84e-05, Tokens=3,248,000, GradNorm=6.266\n",
            "Epoch 67/150, Step 25400: Train Loss=0.1288, Val Loss=0.9215, LR=4.83e-05, Tokens=3,251,200, GradNorm=11.122\n",
            "Epoch 67/150, Step 25425: Train Loss=0.2881, Val Loss=0.9162, LR=4.83e-05, Tokens=3,254,400, GradNorm=8.168\n",
            "Epoch 67/150, Step 25450: Train Loss=0.2191, Val Loss=0.8958, LR=4.82e-05, Tokens=3,257,600, GradNorm=7.644\n",
            "New best model saved! Val Loss: 0.8958 (improvement: 0.0133)\n",
            "Epoch 67/150, Step 25475: Train Loss=0.1372, Val Loss=0.9227, LR=4.82e-05, Tokens=3,260,800, GradNorm=9.876\n",
            "Epoch 67/150, Step 25500: Train Loss=0.1412, Val Loss=0.9065, LR=4.81e-05, Tokens=3,264,000, GradNorm=6.314\n",
            "Epoch 67/150, Step 25525: Train Loss=0.0853, Val Loss=0.9213, LR=4.81e-05, Tokens=3,267,200, GradNorm=6.897\n",
            "Epoch 67/150, Step 25550: Train Loss=0.0973, Val Loss=0.9168, LR=4.80e-05, Tokens=3,270,400, GradNorm=6.372\n",
            "Epoch 67/150, Step 25575: Train Loss=0.1388, Val Loss=0.9262, LR=4.80e-05, Tokens=3,273,600, GradNorm=4.310\n",
            "Epoch 67/150, Step 25600: Train Loss=0.2040, Val Loss=0.8980, LR=4.79e-05, Tokens=3,276,800, GradNorm=5.705\n",
            "Epoch 67/150, Step 25625: Train Loss=0.2417, Val Loss=0.9409, LR=4.79e-05, Tokens=3,280,000, GradNorm=11.748\n",
            "Epoch 67/150, Step 25650: Train Loss=0.2453, Val Loss=0.9245, LR=4.78e-05, Tokens=3,283,200, GradNorm=9.090\n",
            "Epoch 67/150, Step 25675: Train Loss=0.1949, Val Loss=0.9465, LR=4.77e-05, Tokens=3,286,400, GradNorm=7.342\n",
            "Epoch 67/150, Step 25700: Train Loss=0.4317, Val Loss=0.9517, LR=4.77e-05, Tokens=3,289,600, GradNorm=6.753\n",
            "Epoch 67/150, Step 25725: Train Loss=0.2200, Val Loss=0.9473, LR=4.76e-05, Tokens=3,292,800, GradNorm=6.911\n",
            "Epoch 67 completed. Average loss: 0.2962\n",
            "Epoch 68/150, Step 25750: Train Loss=0.1867, Val Loss=0.9531, LR=4.76e-05, Tokens=3,296,000, GradNorm=8.040\n",
            "Epoch 68/150, Step 25775: Train Loss=0.0858, Val Loss=0.9315, LR=4.75e-05, Tokens=3,299,200, GradNorm=9.069\n",
            "Epoch 68/150, Step 25800: Train Loss=0.0904, Val Loss=0.9254, LR=4.75e-05, Tokens=3,302,400, GradNorm=11.694\n",
            "Epoch 68/150, Step 25825: Train Loss=0.2822, Val Loss=0.9326, LR=4.74e-05, Tokens=3,305,600, GradNorm=10.941\n",
            "Epoch 68/150, Step 25850: Train Loss=0.1556, Val Loss=0.9665, LR=4.74e-05, Tokens=3,308,800, GradNorm=9.551\n",
            "Epoch 68/150, Step 25875: Train Loss=0.1785, Val Loss=0.9571, LR=4.73e-05, Tokens=3,312,000, GradNorm=7.550\n",
            "Epoch 68/150, Step 25900: Train Loss=0.2030, Val Loss=0.9123, LR=4.73e-05, Tokens=3,315,200, GradNorm=9.565\n",
            "Epoch 68/150, Step 25925: Train Loss=0.1552, Val Loss=0.9274, LR=4.72e-05, Tokens=3,318,400, GradNorm=7.579\n",
            "Epoch 68/150, Step 25950: Train Loss=0.1425, Val Loss=0.8922, LR=4.72e-05, Tokens=3,321,600, GradNorm=8.830\n",
            "New best model saved! Val Loss: 0.8922 (improvement: 0.0035)\n",
            "Epoch 68/150, Step 25975: Train Loss=0.2992, Val Loss=0.9024, LR=4.71e-05, Tokens=3,324,800, GradNorm=9.881\n",
            "Epoch 68/150, Step 26000: Train Loss=0.3185, Val Loss=0.9298, LR=4.70e-05, Tokens=3,328,000, GradNorm=4.714\n",
            "Epoch 68/150, Step 26025: Train Loss=0.1362, Val Loss=0.9472, LR=4.70e-05, Tokens=3,331,200, GradNorm=7.347\n",
            "Epoch 68/150, Step 26050: Train Loss=0.1521, Val Loss=0.9043, LR=4.69e-05, Tokens=3,334,400, GradNorm=9.509\n",
            "Epoch 68/150, Step 26075: Train Loss=0.2140, Val Loss=0.8793, LR=4.69e-05, Tokens=3,337,600, GradNorm=5.387\n",
            "New best model saved! Val Loss: 0.8793 (improvement: 0.0130)\n",
            "Epoch 68/150, Step 26100: Train Loss=0.1462, Val Loss=0.9037, LR=4.68e-05, Tokens=3,340,800, GradNorm=8.805\n",
            "Epoch 68 completed. Average loss: 0.2845\n",
            "Epoch 69/150, Step 26125: Train Loss=0.1856, Val Loss=0.9040, LR=4.68e-05, Tokens=3,344,000, GradNorm=3.958\n",
            "Epoch 69/150, Step 26150: Train Loss=0.2869, Val Loss=0.9269, LR=4.67e-05, Tokens=3,347,200, GradNorm=6.601\n",
            "Epoch 69/150, Step 26175: Train Loss=0.2579, Val Loss=0.9047, LR=4.67e-05, Tokens=3,350,400, GradNorm=7.079\n",
            "Epoch 69/150, Step 26200: Train Loss=0.1791, Val Loss=0.9016, LR=4.66e-05, Tokens=3,353,600, GradNorm=9.367\n",
            "Epoch 69/150, Step 26225: Train Loss=0.1001, Val Loss=0.9242, LR=4.66e-05, Tokens=3,356,800, GradNorm=2.713\n",
            "Epoch 69/150, Step 26250: Train Loss=0.1794, Val Loss=0.9102, LR=4.65e-05, Tokens=3,360,000, GradNorm=4.683\n",
            "Epoch 69/150, Step 26275: Train Loss=0.1474, Val Loss=0.9146, LR=4.64e-05, Tokens=3,363,200, GradNorm=6.195\n",
            "Epoch 69/150, Step 26300: Train Loss=0.1597, Val Loss=0.8956, LR=4.64e-05, Tokens=3,366,400, GradNorm=12.052\n",
            "Epoch 69/150, Step 26325: Train Loss=0.1497, Val Loss=0.8976, LR=4.63e-05, Tokens=3,369,600, GradNorm=11.753\n",
            "Epoch 69/150, Step 26350: Train Loss=0.2901, Val Loss=0.9078, LR=4.63e-05, Tokens=3,372,800, GradNorm=5.439\n",
            "Epoch 69/150, Step 26375: Train Loss=0.2064, Val Loss=0.9112, LR=4.62e-05, Tokens=3,376,000, GradNorm=15.520\n",
            "Epoch 69/150, Step 26400: Train Loss=0.1936, Val Loss=0.9008, LR=4.62e-05, Tokens=3,379,200, GradNorm=6.976\n",
            "Epoch 69/150, Step 26425: Train Loss=0.1818, Val Loss=0.9150, LR=4.61e-05, Tokens=3,382,400, GradNorm=2.853\n",
            "Epoch 69/150, Step 26450: Train Loss=0.1999, Val Loss=0.9300, LR=4.61e-05, Tokens=3,385,600, GradNorm=5.579\n",
            "Epoch 69/150, Step 26475: Train Loss=0.1630, Val Loss=0.8900, LR=4.60e-05, Tokens=3,388,800, GradNorm=8.479\n",
            "Epoch 69 completed. Average loss: 0.2746\n",
            "Epoch 70/150, Step 26500: Train Loss=0.1831, Val Loss=0.8699, LR=4.60e-05, Tokens=3,392,000, GradNorm=3.773\n",
            "New best model saved! Val Loss: 0.8699 (improvement: 0.0094)\n",
            "Epoch 70/150, Step 26525: Train Loss=0.2615, Val Loss=0.8777, LR=4.59e-05, Tokens=3,395,200, GradNorm=11.723\n",
            "Epoch 70/150, Step 26550: Train Loss=0.1730, Val Loss=0.9195, LR=4.58e-05, Tokens=3,398,400, GradNorm=9.099\n",
            "Epoch 70/150, Step 26575: Train Loss=0.2111, Val Loss=0.8795, LR=4.58e-05, Tokens=3,401,600, GradNorm=6.640\n",
            "Epoch 70/150, Step 26600: Train Loss=0.1581, Val Loss=0.8769, LR=4.57e-05, Tokens=3,404,800, GradNorm=5.209\n",
            "Epoch 70/150, Step 26625: Train Loss=0.2353, Val Loss=0.8489, LR=4.57e-05, Tokens=3,408,000, GradNorm=4.235\n",
            "New best model saved! Val Loss: 0.8489 (improvement: 0.0210)\n",
            "Epoch 70/150, Step 26650: Train Loss=0.1130, Val Loss=0.8577, LR=4.56e-05, Tokens=3,411,200, GradNorm=3.251\n",
            "Epoch 70/150, Step 26675: Train Loss=0.3073, Val Loss=0.9010, LR=4.56e-05, Tokens=3,414,400, GradNorm=14.863\n",
            "Epoch 70/150, Step 26700: Train Loss=0.3700, Val Loss=0.8758, LR=4.55e-05, Tokens=3,417,600, GradNorm=11.013\n",
            "Epoch 70/150, Step 26725: Train Loss=0.2246, Val Loss=0.9028, LR=4.55e-05, Tokens=3,420,800, GradNorm=4.689\n",
            "Epoch 70/150, Step 26750: Train Loss=0.1471, Val Loss=0.9039, LR=4.54e-05, Tokens=3,424,000, GradNorm=7.079\n",
            "Epoch 70/150, Step 26775: Train Loss=0.2010, Val Loss=0.9101, LR=4.53e-05, Tokens=3,427,200, GradNorm=7.878\n",
            "Epoch 70/150, Step 26800: Train Loss=0.2617, Val Loss=0.8941, LR=4.53e-05, Tokens=3,430,400, GradNorm=8.005\n",
            "Epoch 70/150, Step 26825: Train Loss=0.0943, Val Loss=0.8843, LR=4.52e-05, Tokens=3,433,600, GradNorm=5.729\n",
            "Epoch 70/150, Step 26850: Train Loss=0.2312, Val Loss=0.8876, LR=4.52e-05, Tokens=3,436,800, GradNorm=5.314\n",
            "Epoch 70/150, Step 26875: Train Loss=0.1601, Val Loss=0.8735, LR=4.51e-05, Tokens=3,440,000, GradNorm=4.758\n",
            "Epoch 70 completed. Average loss: 0.2651\n",
            "Sample generation after epoch 70:\n",
            "This moviearry use joy isens This phrase a disguised as a waterfall And the talk So I all that Ive allees thatJack! I know, Ill cut the things that my heart  like the heartBel inside you so book  aX meansed to do you down and you  In the, ASICu souven is soACP But you putSing it dark O\n",
            "------------------------------------------------------------\n",
            "Epoch 71/150, Step 26900: Train Loss=0.1278, Val Loss=0.8681, LR=4.51e-05, Tokens=3,443,200, GradNorm=4.871\n",
            "Epoch 71/150, Step 26925: Train Loss=0.0894, Val Loss=0.8924, LR=4.50e-05, Tokens=3,446,400, GradNorm=5.939\n",
            "Epoch 71/150, Step 26950: Train Loss=0.1357, Val Loss=0.8936, LR=4.50e-05, Tokens=3,449,600, GradNorm=2.529\n",
            "Epoch 71/150, Step 26975: Train Loss=0.1831, Val Loss=0.9151, LR=4.49e-05, Tokens=3,452,800, GradNorm=4.184\n",
            "Epoch 71/150, Step 27000: Train Loss=0.0942, Val Loss=0.8719, LR=4.49e-05, Tokens=3,456,000, GradNorm=8.329\n",
            "Epoch 71/150, Step 27025: Train Loss=0.1332, Val Loss=0.8826, LR=4.48e-05, Tokens=3,459,200, GradNorm=9.879\n",
            "Epoch 71/150, Step 27050: Train Loss=0.2369, Val Loss=0.8749, LR=4.47e-05, Tokens=3,462,400, GradNorm=6.942\n",
            "Epoch 71/150, Step 27075: Train Loss=0.2180, Val Loss=0.8681, LR=4.47e-05, Tokens=3,465,600, GradNorm=7.196\n",
            "Epoch 71/150, Step 27100: Train Loss=0.1997, Val Loss=0.8532, LR=4.46e-05, Tokens=3,468,800, GradNorm=7.665\n",
            "Epoch 71/150, Step 27125: Train Loss=0.2708, Val Loss=0.9041, LR=4.46e-05, Tokens=3,472,000, GradNorm=7.953\n",
            "Epoch 71/150, Step 27150: Train Loss=0.1750, Val Loss=0.8479, LR=4.45e-05, Tokens=3,475,200, GradNorm=5.783\n",
            "New best model saved! Val Loss: 0.8479 (improvement: 0.0010)\n",
            "Epoch 71/150, Step 27175: Train Loss=0.2515, Val Loss=0.8692, LR=4.45e-05, Tokens=3,478,400, GradNorm=11.367\n",
            "Epoch 71/150, Step 27200: Train Loss=0.1876, Val Loss=0.8880, LR=4.44e-05, Tokens=3,481,600, GradNorm=6.781\n",
            "Epoch 71/150, Step 27225: Train Loss=0.1742, Val Loss=0.8395, LR=4.44e-05, Tokens=3,484,800, GradNorm=5.247\n",
            "New best model saved! Val Loss: 0.8395 (improvement: 0.0084)\n",
            "Epoch 71/150, Step 27250: Train Loss=0.2276, Val Loss=0.8300, LR=4.43e-05, Tokens=3,488,000, GradNorm=7.125\n",
            "New best model saved! Val Loss: 0.8300 (improvement: 0.0095)\n",
            "Epoch 71 completed. Average loss: 0.2563\n",
            "Epoch 72/150, Step 27275: Train Loss=0.2292, Val Loss=0.8274, LR=4.43e-05, Tokens=3,491,200, GradNorm=9.215\n",
            "New best model saved! Val Loss: 0.8274 (improvement: 0.0026)\n",
            "Epoch 72/150, Step 27300: Train Loss=0.3699, Val Loss=0.8365, LR=4.42e-05, Tokens=3,494,400, GradNorm=9.503\n",
            "Epoch 72/150, Step 27325: Train Loss=0.1310, Val Loss=0.8476, LR=4.41e-05, Tokens=3,497,600, GradNorm=6.749\n",
            "Epoch 72/150, Step 27350: Train Loss=0.2160, Val Loss=0.8397, LR=4.41e-05, Tokens=3,500,800, GradNorm=6.205\n",
            "Epoch 72/150, Step 27375: Train Loss=0.1627, Val Loss=0.8338, LR=4.40e-05, Tokens=3,504,000, GradNorm=8.700\n",
            "Epoch 72/150, Step 27400: Train Loss=0.0879, Val Loss=0.8600, LR=4.40e-05, Tokens=3,507,200, GradNorm=5.747\n",
            "Epoch 72/150, Step 27425: Train Loss=0.1285, Val Loss=0.8492, LR=4.39e-05, Tokens=3,510,400, GradNorm=4.528\n",
            "Epoch 72/150, Step 27450: Train Loss=0.0703, Val Loss=0.8385, LR=4.39e-05, Tokens=3,513,600, GradNorm=7.226\n",
            "Epoch 72/150, Step 27475: Train Loss=0.1424, Val Loss=0.8557, LR=4.38e-05, Tokens=3,516,800, GradNorm=8.213\n",
            "Epoch 72/150, Step 27500: Train Loss=0.1592, Val Loss=0.8418, LR=4.38e-05, Tokens=3,520,000, GradNorm=7.086\n",
            "Epoch 72/150, Step 27525: Train Loss=0.2130, Val Loss=0.8896, LR=4.37e-05, Tokens=3,523,200, GradNorm=6.043\n",
            "Epoch 72/150, Step 27550: Train Loss=0.1292, Val Loss=0.8572, LR=4.36e-05, Tokens=3,526,400, GradNorm=6.581\n",
            "Epoch 72/150, Step 27575: Train Loss=0.2413, Val Loss=0.8413, LR=4.36e-05, Tokens=3,529,600, GradNorm=3.709\n",
            "Epoch 72/150, Step 27600: Train Loss=0.0819, Val Loss=0.8328, LR=4.35e-05, Tokens=3,532,800, GradNorm=3.780\n",
            "Epoch 72/150, Step 27625: Train Loss=0.1050, Val Loss=0.8537, LR=4.35e-05, Tokens=3,536,000, GradNorm=8.003\n",
            "Epoch 72 completed. Average loss: 0.2474\n",
            "Epoch 73/150, Step 27650: Train Loss=0.1785, Val Loss=0.8445, LR=4.34e-05, Tokens=3,539,200, GradNorm=6.992\n",
            "Epoch 73/150, Step 27675: Train Loss=0.1733, Val Loss=0.8457, LR=4.34e-05, Tokens=3,542,400, GradNorm=7.455\n",
            "Epoch 73/150, Step 27700: Train Loss=0.1219, Val Loss=0.8512, LR=4.33e-05, Tokens=3,545,600, GradNorm=6.347\n",
            "Epoch 73/150, Step 27725: Train Loss=0.2046, Val Loss=0.8633, LR=4.33e-05, Tokens=3,548,800, GradNorm=6.268\n",
            "Epoch 73/150, Step 27750: Train Loss=0.1267, Val Loss=0.8365, LR=4.32e-05, Tokens=3,552,000, GradNorm=3.726\n",
            "Epoch 73/150, Step 27775: Train Loss=0.1676, Val Loss=0.8594, LR=4.31e-05, Tokens=3,555,200, GradNorm=3.613\n",
            "Epoch 73/150, Step 27800: Train Loss=0.1839, Val Loss=0.8488, LR=4.31e-05, Tokens=3,558,400, GradNorm=9.883\n",
            "Epoch 73/150, Step 27825: Train Loss=0.2725, Val Loss=0.8253, LR=4.30e-05, Tokens=3,561,600, GradNorm=8.056\n",
            "New best model saved! Val Loss: 0.8253 (improvement: 0.0021)\n",
            "Epoch 73/150, Step 27850: Train Loss=0.1140, Val Loss=0.8359, LR=4.30e-05, Tokens=3,564,800, GradNorm=7.625\n",
            "Epoch 73/150, Step 27875: Train Loss=0.1726, Val Loss=0.8568, LR=4.29e-05, Tokens=3,568,000, GradNorm=7.373\n",
            "Epoch 73/150, Step 27900: Train Loss=0.2830, Val Loss=0.8443, LR=4.29e-05, Tokens=3,571,200, GradNorm=5.803\n",
            "Epoch 73/150, Step 27925: Train Loss=0.2203, Val Loss=0.8182, LR=4.28e-05, Tokens=3,574,400, GradNorm=4.636\n",
            "New best model saved! Val Loss: 0.8182 (improvement: 0.0071)\n",
            "Epoch 73/150, Step 27950: Train Loss=0.1462, Val Loss=0.8231, LR=4.28e-05, Tokens=3,577,600, GradNorm=7.721\n",
            "Epoch 73/150, Step 27975: Train Loss=0.2085, Val Loss=0.8122, LR=4.27e-05, Tokens=3,580,800, GradNorm=5.711\n",
            "New best model saved! Val Loss: 0.8122 (improvement: 0.0061)\n",
            "Epoch 73/150, Step 28000: Train Loss=0.2156, Val Loss=0.7879, LR=4.27e-05, Tokens=3,584,000, GradNorm=6.506\n",
            "New best model saved! Val Loss: 0.7879 (improvement: 0.0242)\n",
            "Epoch 73/150, Step 28025: Train Loss=0.1494, Val Loss=0.8187, LR=4.26e-05, Tokens=3,587,200, GradNorm=3.780\n",
            "Epoch 73 completed. Average loss: 0.2388\n",
            "Epoch 74/150, Step 28050: Train Loss=0.1112, Val Loss=0.8674, LR=4.25e-05, Tokens=3,590,400, GradNorm=6.217\n",
            "Epoch 74/150, Step 28075: Train Loss=0.1678, Val Loss=0.8671, LR=4.25e-05, Tokens=3,593,600, GradNorm=5.293\n",
            "Epoch 74/150, Step 28100: Train Loss=0.0943, Val Loss=0.8838, LR=4.24e-05, Tokens=3,596,800, GradNorm=5.253\n",
            "Epoch 74/150, Step 28125: Train Loss=0.1442, Val Loss=0.8614, LR=4.24e-05, Tokens=3,600,000, GradNorm=9.381\n",
            "Epoch 74/150, Step 28150: Train Loss=0.1120, Val Loss=0.8462, LR=4.23e-05, Tokens=3,603,200, GradNorm=5.704\n",
            "Epoch 74/150, Step 28175: Train Loss=0.1128, Val Loss=0.8457, LR=4.23e-05, Tokens=3,606,400, GradNorm=9.445\n",
            "Epoch 74/150, Step 28200: Train Loss=0.0812, Val Loss=0.8621, LR=4.22e-05, Tokens=3,609,600, GradNorm=6.613\n",
            "Epoch 74/150, Step 28225: Train Loss=0.1375, Val Loss=0.8288, LR=4.22e-05, Tokens=3,612,800, GradNorm=3.904\n",
            "Epoch 74/150, Step 28250: Train Loss=0.0689, Val Loss=0.8176, LR=4.21e-05, Tokens=3,616,000, GradNorm=7.742\n",
            "Epoch 74/150, Step 28275: Train Loss=0.1130, Val Loss=0.8239, LR=4.20e-05, Tokens=3,619,200, GradNorm=5.617\n",
            "Epoch 74/150, Step 28300: Train Loss=0.2464, Val Loss=0.8169, LR=4.20e-05, Tokens=3,622,400, GradNorm=3.139\n",
            "Epoch 74/150, Step 28325: Train Loss=0.1823, Val Loss=0.8327, LR=4.19e-05, Tokens=3,625,600, GradNorm=7.966\n",
            "Epoch 74/150, Step 28350: Train Loss=0.2623, Val Loss=0.8453, LR=4.19e-05, Tokens=3,628,800, GradNorm=9.525\n",
            "Epoch 74/150, Step 28375: Train Loss=0.1878, Val Loss=0.8186, LR=4.18e-05, Tokens=3,632,000, GradNorm=6.122\n",
            "Epoch 74/150, Step 28400: Train Loss=0.1760, Val Loss=0.8145, LR=4.18e-05, Tokens=3,635,200, GradNorm=7.525\n",
            "Epoch 74 completed. Average loss: 0.2282\n",
            "Epoch 75/150, Step 28425: Train Loss=0.0631, Val Loss=0.7964, LR=4.17e-05, Tokens=3,638,400, GradNorm=8.158\n",
            "Epoch 75/150, Step 28450: Train Loss=0.0885, Val Loss=0.8170, LR=4.17e-05, Tokens=3,641,600, GradNorm=6.956\n",
            "Epoch 75/150, Step 28475: Train Loss=0.1370, Val Loss=0.8196, LR=4.16e-05, Tokens=3,644,800, GradNorm=8.386\n",
            "Epoch 75/150, Step 28500: Train Loss=0.2074, Val Loss=0.8246, LR=4.15e-05, Tokens=3,648,000, GradNorm=6.840\n",
            "Epoch 75/150, Step 28525: Train Loss=0.2104, Val Loss=0.7865, LR=4.15e-05, Tokens=3,651,200, GradNorm=3.832\n",
            "New best model saved! Val Loss: 0.7865 (improvement: 0.0014)\n",
            "Epoch 75/150, Step 28550: Train Loss=0.0794, Val Loss=0.8043, LR=4.14e-05, Tokens=3,654,400, GradNorm=6.136\n",
            "Epoch 75/150, Step 28575: Train Loss=0.1151, Val Loss=0.7973, LR=4.14e-05, Tokens=3,657,600, GradNorm=5.216\n",
            "Epoch 75/150, Step 28600: Train Loss=0.1481, Val Loss=0.7614, LR=4.13e-05, Tokens=3,660,800, GradNorm=5.676\n",
            "New best model saved! Val Loss: 0.7614 (improvement: 0.0251)\n",
            "Epoch 75/150, Step 28625: Train Loss=0.1575, Val Loss=0.8013, LR=4.13e-05, Tokens=3,664,000, GradNorm=3.997\n",
            "Epoch 75/150, Step 28650: Train Loss=0.1335, Val Loss=0.8170, LR=4.12e-05, Tokens=3,667,200, GradNorm=8.028\n",
            "Epoch 75/150, Step 28675: Train Loss=0.0940, Val Loss=0.8164, LR=4.12e-05, Tokens=3,670,400, GradNorm=4.674\n",
            "Epoch 75/150, Step 28700: Train Loss=0.1327, Val Loss=0.8155, LR=4.11e-05, Tokens=3,673,600, GradNorm=9.126\n",
            "Epoch 75/150, Step 28725: Train Loss=0.2962, Val Loss=0.8239, LR=4.11e-05, Tokens=3,676,800, GradNorm=8.071\n",
            "Epoch 75/150, Step 28750: Train Loss=0.1808, Val Loss=0.8130, LR=4.10e-05, Tokens=3,680,000, GradNorm=10.734\n",
            "Epoch 75/150, Step 28775: Train Loss=0.2309, Val Loss=0.7958, LR=4.09e-05, Tokens=3,683,200, GradNorm=10.890\n",
            "Epoch 75/150, Step 28800: Train Loss=0.2097, Val Loss=0.8004, LR=4.09e-05, Tokens=3,686,400, GradNorm=5.637\n",
            "Epoch 75 completed. Average loss: 0.2235\n",
            "Epoch 76/150, Step 28825: Train Loss=0.1718, Val Loss=0.8132, LR=4.08e-05, Tokens=3,689,600, GradNorm=8.890\n",
            "Epoch 76/150, Step 28850: Train Loss=0.1744, Val Loss=0.8047, LR=4.08e-05, Tokens=3,692,800, GradNorm=3.524\n",
            "Epoch 76/150, Step 28875: Train Loss=0.1106, Val Loss=0.8055, LR=4.07e-05, Tokens=3,696,000, GradNorm=10.266\n",
            "Epoch 76/150, Step 28900: Train Loss=0.1060, Val Loss=0.8150, LR=4.07e-05, Tokens=3,699,200, GradNorm=7.097\n",
            "Epoch 76/150, Step 28925: Train Loss=0.1666, Val Loss=0.8232, LR=4.06e-05, Tokens=3,702,400, GradNorm=4.342\n",
            "Epoch 76/150, Step 28950: Train Loss=0.1852, Val Loss=0.8146, LR=4.06e-05, Tokens=3,705,600, GradNorm=9.170\n",
            "Epoch 76/150, Step 28975: Train Loss=0.1020, Val Loss=0.8028, LR=4.05e-05, Tokens=3,708,800, GradNorm=4.887\n",
            "Epoch 76/150, Step 29000: Train Loss=0.1670, Val Loss=0.8099, LR=4.04e-05, Tokens=3,712,000, GradNorm=3.993\n",
            "Epoch 76/150, Step 29025: Train Loss=0.1047, Val Loss=0.8051, LR=4.04e-05, Tokens=3,715,200, GradNorm=3.659\n",
            "Epoch 76/150, Step 29050: Train Loss=0.2466, Val Loss=0.8254, LR=4.03e-05, Tokens=3,718,400, GradNorm=4.287\n",
            "Epoch 76/150, Step 29075: Train Loss=0.1340, Val Loss=0.7825, LR=4.03e-05, Tokens=3,721,600, GradNorm=5.297\n",
            "Epoch 76/150, Step 29100: Train Loss=0.1986, Val Loss=0.7999, LR=4.02e-05, Tokens=3,724,800, GradNorm=6.420\n",
            "Epoch 76/150, Step 29125: Train Loss=0.1291, Val Loss=0.7682, LR=4.02e-05, Tokens=3,728,000, GradNorm=6.194\n",
            "Epoch 76/150, Step 29150: Train Loss=0.0615, Val Loss=0.7472, LR=4.01e-05, Tokens=3,731,200, GradNorm=4.536\n",
            "New best model saved! Val Loss: 0.7472 (improvement: 0.0142)\n",
            "Epoch 76/150, Step 29175: Train Loss=0.2106, Val Loss=0.7743, LR=4.01e-05, Tokens=3,734,400, GradNorm=8.770\n",
            "Epoch 76 completed. Average loss: 0.2143\n",
            "Epoch 77/150, Step 29200: Train Loss=0.0893, Val Loss=0.7584, LR=4.00e-05, Tokens=3,737,600, GradNorm=4.933\n",
            "Epoch 77/150, Step 29225: Train Loss=0.1016, Val Loss=0.7550, LR=3.99e-05, Tokens=3,740,800, GradNorm=4.892\n",
            "Epoch 77/150, Step 29250: Train Loss=0.2409, Val Loss=0.7459, LR=3.99e-05, Tokens=3,744,000, GradNorm=8.145\n",
            "New best model saved! Val Loss: 0.7459 (improvement: 0.0014)\n",
            "Epoch 77/150, Step 29275: Train Loss=0.1206, Val Loss=0.7662, LR=3.98e-05, Tokens=3,747,200, GradNorm=4.511\n",
            "Epoch 77/150, Step 29300: Train Loss=0.0857, Val Loss=0.7455, LR=3.98e-05, Tokens=3,750,400, GradNorm=7.537\n",
            "New best model saved! Val Loss: 0.7455 (improvement: 0.0004)\n",
            "Epoch 77/150, Step 29325: Train Loss=0.1978, Val Loss=0.7595, LR=3.97e-05, Tokens=3,753,600, GradNorm=6.272\n",
            "Epoch 77/150, Step 29350: Train Loss=0.0596, Val Loss=0.7715, LR=3.97e-05, Tokens=3,756,800, GradNorm=9.580\n",
            "Epoch 77/150, Step 29375: Train Loss=0.1355, Val Loss=0.7729, LR=3.96e-05, Tokens=3,760,000, GradNorm=8.700\n",
            "Epoch 77/150, Step 29400: Train Loss=0.2021, Val Loss=0.7971, LR=3.96e-05, Tokens=3,763,200, GradNorm=5.399\n",
            "Epoch 77/150, Step 29425: Train Loss=0.1829, Val Loss=0.7775, LR=3.95e-05, Tokens=3,766,400, GradNorm=5.842\n",
            "Epoch 77/150, Step 29450: Train Loss=0.1057, Val Loss=0.7752, LR=3.94e-05, Tokens=3,769,600, GradNorm=7.113\n",
            "Epoch 77/150, Step 29475: Train Loss=0.1326, Val Loss=0.7892, LR=3.94e-05, Tokens=3,772,800, GradNorm=2.734\n",
            "Epoch 77/150, Step 29500: Train Loss=0.1627, Val Loss=0.8010, LR=3.93e-05, Tokens=3,776,000, GradNorm=7.678\n",
            "Epoch 77/150, Step 29525: Train Loss=0.1183, Val Loss=0.8144, LR=3.93e-05, Tokens=3,779,200, GradNorm=4.445\n",
            "Epoch 77/150, Step 29550: Train Loss=0.1784, Val Loss=0.8065, LR=3.92e-05, Tokens=3,782,400, GradNorm=3.172\n",
            "Epoch 77 completed. Average loss: 0.2089\n",
            "Epoch 78/150, Step 29575: Train Loss=0.1403, Val Loss=0.7655, LR=3.92e-05, Tokens=3,785,600, GradNorm=3.445\n",
            "Epoch 78/150, Step 29600: Train Loss=0.1325, Val Loss=0.7886, LR=3.91e-05, Tokens=3,788,800, GradNorm=15.471\n",
            "Epoch 78/150, Step 29625: Train Loss=0.1938, Val Loss=0.7708, LR=3.91e-05, Tokens=3,792,000, GradNorm=2.452\n",
            "Epoch 78/150, Step 29650: Train Loss=0.2633, Val Loss=0.8043, LR=3.90e-05, Tokens=3,795,200, GradNorm=8.107\n",
            "Epoch 78/150, Step 29675: Train Loss=0.0670, Val Loss=0.7904, LR=3.89e-05, Tokens=3,798,400, GradNorm=6.968\n",
            "Epoch 78/150, Step 29700: Train Loss=0.1380, Val Loss=0.7751, LR=3.89e-05, Tokens=3,801,600, GradNorm=6.955\n",
            "Epoch 78/150, Step 29725: Train Loss=0.1426, Val Loss=0.8027, LR=3.88e-05, Tokens=3,804,800, GradNorm=9.610\n",
            "Epoch 78/150, Step 29750: Train Loss=0.1674, Val Loss=0.8002, LR=3.88e-05, Tokens=3,808,000, GradNorm=9.335\n",
            "Epoch 78/150, Step 29775: Train Loss=0.2141, Val Loss=0.7933, LR=3.87e-05, Tokens=3,811,200, GradNorm=6.507\n",
            "Epoch 78/150, Step 29800: Train Loss=0.0716, Val Loss=0.7982, LR=3.87e-05, Tokens=3,814,400, GradNorm=3.388\n",
            "Epoch 78/150, Step 29825: Train Loss=0.1732, Val Loss=0.7710, LR=3.86e-05, Tokens=3,817,600, GradNorm=7.111\n",
            "Epoch 78/150, Step 29850: Train Loss=0.1935, Val Loss=0.7944, LR=3.86e-05, Tokens=3,820,800, GradNorm=7.788\n",
            "Epoch 78/150, Step 29875: Train Loss=0.1672, Val Loss=0.7970, LR=3.85e-05, Tokens=3,824,000, GradNorm=4.692\n",
            "Epoch 78/150, Step 29900: Train Loss=0.0654, Val Loss=0.7806, LR=3.85e-05, Tokens=3,827,200, GradNorm=9.602\n",
            "Epoch 78/150, Step 29925: Train Loss=0.0785, Val Loss=0.7698, LR=3.84e-05, Tokens=3,830,400, GradNorm=5.904\n",
            "Epoch 78/150, Step 29950: Train Loss=0.1641, Val Loss=0.7766, LR=3.83e-05, Tokens=3,833,600, GradNorm=9.761\n",
            "Epoch 78 completed. Average loss: 0.2015\n",
            "Epoch 79/150, Step 29975: Train Loss=0.1072, Val Loss=0.7690, LR=3.83e-05, Tokens=3,836,800, GradNorm=6.950\n",
            "Epoch 79/150, Step 30000: Train Loss=0.0593, Val Loss=0.7499, LR=3.82e-05, Tokens=3,840,000, GradNorm=7.477\n",
            "Epoch 79/150, Step 30025: Train Loss=0.1304, Val Loss=0.7583, LR=3.82e-05, Tokens=3,843,200, GradNorm=3.459\n",
            "Epoch 79/150, Step 30050: Train Loss=0.2150, Val Loss=0.7405, LR=3.81e-05, Tokens=3,846,400, GradNorm=5.142\n",
            "New best model saved! Val Loss: 0.7405 (improvement: 0.0050)\n",
            "Epoch 79/150, Step 30075: Train Loss=0.2102, Val Loss=0.7381, LR=3.81e-05, Tokens=3,849,600, GradNorm=4.961\n",
            "New best model saved! Val Loss: 0.7381 (improvement: 0.0024)\n",
            "Epoch 79/150, Step 30100: Train Loss=0.1521, Val Loss=0.7315, LR=3.80e-05, Tokens=3,852,800, GradNorm=5.947\n",
            "New best model saved! Val Loss: 0.7315 (improvement: 0.0067)\n",
            "Epoch 79/150, Step 30125: Train Loss=0.1784, Val Loss=0.7500, LR=3.80e-05, Tokens=3,856,000, GradNorm=8.672\n",
            "Epoch 79/150, Step 30150: Train Loss=0.1536, Val Loss=0.7331, LR=3.79e-05, Tokens=3,859,200, GradNorm=3.630\n",
            "Epoch 79/150, Step 30175: Train Loss=0.1677, Val Loss=0.7356, LR=3.78e-05, Tokens=3,862,400, GradNorm=8.217\n",
            "Epoch 79/150, Step 30200: Train Loss=0.1274, Val Loss=0.7060, LR=3.78e-05, Tokens=3,865,600, GradNorm=2.814\n",
            "New best model saved! Val Loss: 0.7060 (improvement: 0.0254)\n",
            "Epoch 79/150, Step 30225: Train Loss=0.0974, Val Loss=0.6830, LR=3.77e-05, Tokens=3,868,800, GradNorm=5.078\n",
            "New best model saved! Val Loss: 0.6830 (improvement: 0.0230)\n",
            "Epoch 79/150, Step 30250: Train Loss=0.0587, Val Loss=0.7170, LR=3.77e-05, Tokens=3,872,000, GradNorm=4.276\n",
            "Epoch 79/150, Step 30275: Train Loss=0.1510, Val Loss=0.7394, LR=3.76e-05, Tokens=3,875,200, GradNorm=6.638\n",
            "Epoch 79/150, Step 30300: Train Loss=0.1006, Val Loss=0.7391, LR=3.76e-05, Tokens=3,878,400, GradNorm=8.303\n",
            "Epoch 79/150, Step 30325: Train Loss=0.1653, Val Loss=0.7223, LR=3.75e-05, Tokens=3,881,600, GradNorm=4.270\n",
            "Epoch 79 completed. Average loss: 0.1946\n",
            "Epoch 80/150, Step 30350: Train Loss=0.0834, Val Loss=0.7469, LR=3.75e-05, Tokens=3,884,800, GradNorm=6.856\n",
            "Epoch 80/150, Step 30375: Train Loss=0.0680, Val Loss=0.7458, LR=3.74e-05, Tokens=3,888,000, GradNorm=6.837\n",
            "Epoch 80/150, Step 30400: Train Loss=0.1139, Val Loss=0.7333, LR=3.73e-05, Tokens=3,891,200, GradNorm=3.541\n",
            "Epoch 80/150, Step 30425: Train Loss=0.1181, Val Loss=0.7300, LR=3.73e-05, Tokens=3,894,400, GradNorm=7.615\n",
            "Epoch 80/150, Step 30450: Train Loss=0.1563, Val Loss=0.7533, LR=3.72e-05, Tokens=3,897,600, GradNorm=6.623\n",
            "Epoch 80/150, Step 30475: Train Loss=0.1459, Val Loss=0.7514, LR=3.72e-05, Tokens=3,900,800, GradNorm=4.941\n",
            "Epoch 80/150, Step 30500: Train Loss=0.1671, Val Loss=0.7407, LR=3.71e-05, Tokens=3,904,000, GradNorm=7.349\n",
            "Epoch 80/150, Step 30525: Train Loss=0.1005, Val Loss=0.7398, LR=3.71e-05, Tokens=3,907,200, GradNorm=4.988\n",
            "Epoch 80/150, Step 30550: Train Loss=0.1758, Val Loss=0.7337, LR=3.70e-05, Tokens=3,910,400, GradNorm=9.138\n",
            "Epoch 80/150, Step 30575: Train Loss=0.1248, Val Loss=0.7267, LR=3.70e-05, Tokens=3,913,600, GradNorm=8.684\n",
            "Epoch 80/150, Step 30600: Train Loss=0.0698, Val Loss=0.7246, LR=3.69e-05, Tokens=3,916,800, GradNorm=7.039\n",
            "Epoch 80/150, Step 30625: Train Loss=0.3051, Val Loss=0.7349, LR=3.69e-05, Tokens=3,920,000, GradNorm=6.468\n",
            "Epoch 80/150, Step 30650: Train Loss=0.1500, Val Loss=0.7169, LR=3.68e-05, Tokens=3,923,200, GradNorm=3.399\n",
            "Epoch 80/150, Step 30675: Train Loss=0.1325, Val Loss=0.7179, LR=3.67e-05, Tokens=3,926,400, GradNorm=10.378\n",
            "Epoch 80/150, Step 30700: Train Loss=0.0433, Val Loss=0.7056, LR=3.67e-05, Tokens=3,929,600, GradNorm=11.452\n",
            "Epoch 80 completed. Average loss: 0.1887\n",
            "Sample generation after epoch 80:\n",
            "This movie will or you And if you go there in the burning end of who top of who e sleeping but you In off Were not so that IMay all your think that I pay for \" never work, oh-oh-oh, do you every oh-oh-oh-ooh-oh-oh oh-oh One this crazy world, ohoh, oh-oh\n",
            "------------------------------------------------------------\n",
            "Epoch 81/150, Step 30725: Train Loss=0.1230, Val Loss=0.7237, LR=3.66e-05, Tokens=3,932,800, GradNorm=5.800\n",
            "Epoch 81/150, Step 30750: Train Loss=0.0619, Val Loss=0.7284, LR=3.66e-05, Tokens=3,936,000, GradNorm=6.316\n",
            "Epoch 81/150, Step 30775: Train Loss=0.1218, Val Loss=0.7336, LR=3.65e-05, Tokens=3,939,200, GradNorm=2.436\n",
            "Epoch 81/150, Step 30800: Train Loss=0.1777, Val Loss=0.7448, LR=3.65e-05, Tokens=3,942,400, GradNorm=6.024\n",
            "Epoch 81/150, Step 30825: Train Loss=0.0730, Val Loss=0.7470, LR=3.64e-05, Tokens=3,945,600, GradNorm=8.533\n",
            "Epoch 81/150, Step 30850: Train Loss=0.2261, Val Loss=0.7605, LR=3.64e-05, Tokens=3,948,800, GradNorm=4.291\n",
            "Epoch 81/150, Step 30875: Train Loss=0.1189, Val Loss=0.7620, LR=3.63e-05, Tokens=3,952,000, GradNorm=9.619\n",
            "Epoch 81/150, Step 30900: Train Loss=0.1509, Val Loss=0.7257, LR=3.62e-05, Tokens=3,955,200, GradNorm=5.496\n",
            "Epoch 81/150, Step 30925: Train Loss=0.0696, Val Loss=0.7094, LR=3.62e-05, Tokens=3,958,400, GradNorm=5.879\n",
            "Epoch 81/150, Step 30950: Train Loss=0.0991, Val Loss=0.7096, LR=3.61e-05, Tokens=3,961,600, GradNorm=8.330\n",
            "Epoch 81/150, Step 30975: Train Loss=0.0511, Val Loss=0.7116, LR=3.61e-05, Tokens=3,964,800, GradNorm=4.538\n",
            "Epoch 81/150, Step 31000: Train Loss=0.1384, Val Loss=0.7506, LR=3.60e-05, Tokens=3,968,000, GradNorm=8.621\n",
            "Epoch 81/150, Step 31025: Train Loss=0.1954, Val Loss=0.7035, LR=3.60e-05, Tokens=3,971,200, GradNorm=5.419\n",
            "Epoch 81/150, Step 31050: Train Loss=0.1655, Val Loss=0.7060, LR=3.59e-05, Tokens=3,974,400, GradNorm=6.922\n",
            "Epoch 81/150, Step 31075: Train Loss=0.1187, Val Loss=0.7205, LR=3.59e-05, Tokens=3,977,600, GradNorm=12.708\n",
            "Epoch 81/150, Step 31100: Train Loss=0.2352, Val Loss=0.7108, LR=3.58e-05, Tokens=3,980,800, GradNorm=11.660\n",
            "Early stopping triggered at epoch 81, step 31100\n",
            "Best validation loss: 0.6830\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.show()\n",
        "\n",
        "# Assuming train_losses, val_losses, epochs_seen, and tokens_seen are returned from the training function\n",
        "# Plot the losses\n",
        "plot_losses(training_results[\"epochs_seen\"], training_results[\"tokens_seen\"], training_results[\"train_losses\"], training_results[\"val_losses\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "2zTYajCBrXSx",
        "outputId": "1f9afc97-e666-4e72-c9c0-0e3b74260cb8"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetune On ColdPlay"
      ],
      "metadata": {
        "id": "ZS69sHtS-6hT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and validation dataloaders for finetuning on Coldplay lyrics\n",
        "finetune_train_dataloader = create_encoded_dataloader(\n",
        "    joined_train_lyrics,\n",
        "    tokenizer=tokenizer,\n",
        "    batch_size=8,  # Slightly larger batch size for stability\n",
        "    max_length=context_length,\n",
        "    stride=context_length // 2,  # More overlap for better learning\n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "finetune_test_dataloader = create_encoded_dataloader(\n",
        "    joined_test_lyrics,\n",
        "    tokenizer=tokenizer,\n",
        "    batch_size=8,\n",
        "    max_length=context_length,\n",
        "    stride=context_length // 2,\n",
        "    shuffle=False,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "# Define finetuning settings (optimized for finetuning)\n",
        "settings = {\n",
        "    \"learning_rate\": 3e-4,       # Faster convergence\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"num_epochs\": 30,            # Much fewer epochs\n",
        "    \"batch_size\": 2,              # Small batch for more updates\n",
        "    \"warmup_steps\": 50,           # Minimal warmup\n",
        "    \"max_lr\": 5e-4,               # Modest peak LR\n",
        "    \"min_lr\": 1e-5,               # Higher floor to avoid very slow training\n",
        "    \"eval_freq\": 1,               # Evaluate every epoch\n",
        "    \"eval_iter\": -1,              # All validation data\n",
        "    \"gradient_clip\": 0.6,\n",
        "    \"patience\": 5,                # Stop if no improvement\n",
        "    \"min_improvement\": 1e-4\n",
        "}\n",
        "\n",
        "def finetune_model_advanced(model, train_loader, val_loader, device, settings, tokenizer, context_length, start_context=\"Yellow lights\"):\n",
        "    \"\"\"\n",
        "    Advanced finetuning function with learning rate scheduling and warmup\n",
        "    Specifically designed for finetuning pretrained models\n",
        "    \"\"\"\n",
        "    # Initialize optimizer with lower learning rate for finetuning\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=settings[\"learning_rate\"],\n",
        "        weight_decay=settings[\"weight_decay\"],\n",
        "        betas=(0.9, 0.95)  # Better betas for transformer training\n",
        "    )\n",
        "\n",
        "    # Calculate total training steps\n",
        "    total_steps = len(train_loader) * settings[\"num_epochs\"]\n",
        "\n",
        "    # Initialize scheduler with warmup (shorter for finetuning)\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=settings[\"warmup_steps\"],\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Training tracking\n",
        "    train_losses, val_losses = [], []\n",
        "    epochs_seen, tokens_seen = [], []\n",
        "    learning_rates = []\n",
        "    total_tokens = 0\n",
        "    global_step = 0\n",
        "\n",
        "    # Early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(f\"Starting finetuning for {settings['num_epochs']} epochs\")\n",
        "    print(f\"Training batches: {len(train_loader)}\")\n",
        "    print(f\"Validation batches: {len(val_loader)}\")\n",
        "    print(f\"Total training steps: {total_steps}\")\n",
        "    print(f\"Warmup steps: {settings['warmup_steps']}\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(settings[\"num_epochs\"]):\n",
        "        epoch_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for i, (input_batch, target_batch) in enumerate(train_loader):\n",
        "            global_step += 1\n",
        "            total_tokens += input_batch.numel()\n",
        "\n",
        "            # Forward pass\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping (lower for finetuning)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), settings[\"gradient_clip\"])\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # Log current learning rate\n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "            learning_rates.append(current_lr)\n",
        "\n",
        "            # Evaluation and logging\n",
        "            if global_step % settings[\"eval_freq\"] == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, settings[\"eval_iter\"]\n",
        "                )\n",
        "\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                epochs_seen.append(epoch + (i + 1) / len(train_loader))\n",
        "                tokens_seen.append(total_tokens)\n",
        "\n",
        "                print(f\"Epoch {epoch+1}/{settings['num_epochs']}, Step {global_step}: \"\n",
        "                      f\"Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, \"\n",
        "                      f\"LR={current_lr:.2e}, Tokens={total_tokens:,}\")\n",
        "\n",
        "                # Early stopping check\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    patience_counter = 0\n",
        "                    # Save best finetuned model\n",
        "                    torch.save({\n",
        "                        'epoch': epoch,\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'optimizer_state_dict': optimizer.state_dict(),\n",
        "                        'scheduler_state_dict': scheduler.state_dict(),\n",
        "                        'best_val_loss': best_val_loss,\n",
        "                        'global_step': global_step,\n",
        "                        'finetune_settings': settings\n",
        "                    }, 'best_coldplay_finetuned_model.pth')\n",
        "                    print(f\"New best finetuned model saved! Val Loss: {val_loss:.4f}\")\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience_counter >= settings[\"patience\"]:\n",
        "                        print(f\"Early stopping triggered at epoch {epoch+1}, step {global_step}\")\n",
        "                        print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "                        return {\n",
        "                            'train_losses': train_losses,\n",
        "                            'val_losses': val_losses,\n",
        "                            'epochs_seen': epochs_seen,\n",
        "                            'tokens_seen': tokens_seen,\n",
        "                            'learning_rates': learning_rates,\n",
        "                            'best_val_loss': best_val_loss\n",
        "                        }\n",
        "\n",
        "        # End of epoch logging\n",
        "        avg_epoch_loss = epoch_loss / num_batches\n",
        "        print(f\"Epoch {epoch+1} completed. Average loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "        # Generate sample text at end of epoch (every 3 epochs for finetuning)\n",
        "        if (epoch + 1) % 3 == 0:\n",
        "            print(f\"Sample generation after epoch {epoch+1}:\")\n",
        "            generate_and_print_sample(\n",
        "                model, tokenizer, device, start_context, context_length, num_chars=80\n",
        "            )\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    print(f\"Finetuning completed! Best validation loss: {best_val_loss:.4f}\")\n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'epochs_seen': epochs_seen,\n",
        "        'tokens_seen': tokens_seen,\n",
        "        'learning_rates': learning_rates,\n",
        "        'best_val_loss': best_val_loss\n",
        "    }\n",
        "\n",
        "print(f\"Starting finetuning on Coldplay lyrics...\")\n",
        "print(f\"Training batches: {len(finetune_train_dataloader)}\")\n",
        "print(f\"Validation batches: {len(finetune_test_dataloader)}\")\n",
        "\n",
        "# Start finetuning with improved training function\n",
        "finetune_results = finetune_model_advanced(\n",
        "    model,\n",
        "    finetune_train_dataloader,\n",
        "    finetune_test_dataloader,\n",
        "    device,\n",
        "    finetune_settings,\n",
        "    tokenizer,\n",
        "    context_length,\n",
        "    start_context=\"Yellow lights\"\n",
        ")\n",
        "\n",
        "print(\"Finetuning completed!\")\n",
        "\n",
        "# Save the final finetuned model\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'finetune_settings': finetune_settings,\n",
        "    'results': finetune_results\n",
        "}, 'coldplay_finetuned_model_final.pth')\n",
        "print(\"Final finetuned model saved as 'coldplay_finetuned_model_final.pth'\")\n",
        "\n",
        "# Test generation with different prompts\n",
        "test_prompts = [\n",
        "    \"Yellow lights\",\n",
        "    \"Look at the stars\",\n",
        "    \"When you try your best\",\n",
        "    \"In the darkness\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TESTING FINETUNED MODEL GENERATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "model.eval()\n",
        "for prompt in test_prompts:\n",
        "    print(f\"\\nPrompt: '{prompt}'\")\n",
        "    print(\"-\" * 30)\n",
        "    try:\n",
        "        context_tensor = text_to_token_ids(prompt, tokenizer, device)\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = generate(\n",
        "                model,\n",
        "                max_new_tokens=80,  # Shorter generation for cleaner output\n",
        "                context=context_tensor,\n",
        "                context_length=context_length,\n",
        "                temperature=0.8,  # Slightly creative but not too random\n",
        "                top_k=50  # Limit to top 50 tokens for better quality\n",
        "            )\n",
        "            generated_text = token_ids_to_text(generated_tokens, tokenizer)\n",
        "            print(generated_text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating text for prompt '{prompt}': {e}\")\n",
        "\n",
        "model.train()\n",
        "\n",
        "# Plot finetuning losses\n",
        "def plot_finetuning_losses(results):\n",
        "    \"\"\"Plot training and validation losses for finetuning\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    epochs_seen = results['epochs_seen']\n",
        "    train_losses = results['train_losses']\n",
        "    val_losses = results['val_losses']\n",
        "    learning_rates = results['learning_rates']\n",
        "\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Plot losses\n",
        "    ax1.plot(epochs_seen, train_losses, label='Training Loss', color='blue', alpha=0.7)\n",
        "    ax1.plot(epochs_seen, val_losses, label='Validation Loss', color='red', alpha=0.7)\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Finetuning Losses')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot loss difference\n",
        "    loss_diff = [val - train for val, train in zip(val_losses, train_losses)]\n",
        "    ax2.plot(epochs_seen, loss_diff, label='Val - Train Loss', color='green', alpha=0.7)\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('Loss Difference')\n",
        "    ax2.set_title('Overfitting Monitor')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Plot learning rate schedule\n",
        "    ax3.plot(learning_rates, color='orange', alpha=0.7)\n",
        "    ax3.set_xlabel('Steps')\n",
        "    ax3.set_ylabel('Learning Rate')\n",
        "    ax3.set_title('Learning Rate Schedule')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    ax3.set_yscale('log')\n",
        "\n",
        "    # Plot tokens seen vs loss\n",
        "    tokens_seen = results['tokens_seen']\n",
        "    ax4.plot(tokens_seen, train_losses, label='Training Loss', color='blue', alpha=0.7)\n",
        "    ax4.plot(tokens_seen, val_losses, label='Validation Loss', color='red', alpha=0.7)\n",
        "    ax4.set_xlabel('Tokens Seen')\n",
        "    ax4.set_ylabel('Loss')\n",
        "    ax4.set_title('Loss vs Tokens Processed')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('finetuning_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Plot the finetuning results\n",
        "if len(finetune_results['train_losses']) > 0:\n",
        "    plot_finetuning_losses(finetune_results)\n",
        "\n",
        "# Comprehensive finetuning summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINETUNING SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Final training loss: {finetune_results['train_losses'][-1]:.4f}\")\n",
        "print(f\"Final validation loss: {finetune_results['val_losses'][-1]:.4f}\")\n",
        "print(f\"Best validation loss: {finetune_results['best_val_loss']:.4f}\")\n",
        "print(f\"Total epochs trained: {len(finetune_results['epochs_seen'])}\")\n",
        "print(f\"Total tokens processed: {finetune_results['tokens_seen'][-1] if finetune_results['tokens_seen'] else 0:,}\")\n",
        "print(f\"Final learning rate: {finetune_results['learning_rates'][-1]:.2e}\")\n",
        "\n",
        "# Calculate improvement metrics\n",
        "if len(finetune_results['train_losses']) > 1:\n",
        "    initial_train_loss = finetune_results['train_losses'][0]\n",
        "    final_train_loss = finetune_results['train_losses'][-1]\n",
        "    train_improvement = ((initial_train_loss - final_train_loss) / initial_train_loss) * 100\n",
        "\n",
        "    initial_val_loss = finetune_results['val_losses'][0]\n",
        "    final_val_loss = finetune_results['val_losses'][-1]\n",
        "    val_improvement = ((initial_val_loss - final_val_loss) / initial_val_loss) * 100\n",
        "\n",
        "    print(f\"Training loss improvement: {train_improvement:.2f}%\")\n",
        "    print(f\"Validation loss improvement: {val_improvement:.2f}%\")"
      ],
      "metadata": {
        "id": "qA5uKsWl-_bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7da42538-d7ec-409b-be7f-4383e9cb1f85"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting finetuning on Coldplay lyrics...\n",
            "Training batches: 23\n",
            "Validation batches: 2\n",
            "Starting finetuning for 30 epochs\n",
            "Training batches: 23\n",
            "Validation batches: 2\n",
            "Total training steps: 690\n",
            "Warmup steps: 100\n",
            "Epoch 1/30, Step 20: Train Loss=4.0421, Val Loss=6.3826, LR=2.00e-06, Tokens=40,960\n",
            "New best finetuned model saved! Val Loss: 6.3826\n",
            "Epoch 1 completed. Average loss: 3.8453\n",
            "Epoch 2/30, Step 40: Train Loss=3.6807, Val Loss=6.3123, LR=4.00e-06, Tokens=81,920\n",
            "New best finetuned model saved! Val Loss: 6.3123\n",
            "Epoch 2 completed. Average loss: 3.7734\n",
            "Epoch 3/30, Step 60: Train Loss=3.5420, Val Loss=6.1779, LR=6.00e-06, Tokens=122,880\n",
            "New best finetuned model saved! Val Loss: 6.1779\n",
            "Epoch 3 completed. Average loss: 3.6261\n",
            "Sample generation after epoch 3:\n",
            "Yellow lights feel my heart my The summer, I had done I feel my heartest heart beating We could thirst We couldnt change me We could change We couldnt change, no again We always been in a oh oh, change We And we So I was're start So I   So I I- how done So I But I more\n",
            "--------------------------------------------------\n",
            "Epoch 4/30, Step 80: Train Loss=3.3419, Val Loss=6.0298, LR=8.00e-06, Tokens=163,840\n",
            "New best finetuned model saved! Val Loss: 6.0298\n",
            "Epoch 4 completed. Average loss: 3.4651\n",
            "Epoch 5/30, Step 100: Train Loss=3.1257, Val Loss=5.8634, LR=1.00e-05, Tokens=204,800\n",
            "New best finetuned model saved! Val Loss: 5.8634\n",
            "Epoch 5 completed. Average loss: 3.2608\n",
            "Epoch 6/30, Step 120: Train Loss=3.0381, Val Loss=5.7322, LR=9.97e-06, Tokens=245,760\n",
            "New best finetuned model saved! Val Loss: 5.7322\n",
            "Epoch 6 completed. Average loss: 3.0560\n",
            "Sample generation after epoch 6:\n",
            "Yellow lights kids gonna Ill stop the world Cause say Cause Im on my way to me Look and riching the work, Rosa, thats consortium shine wears off And fire at the trees waiting to say YouveMy song is gonna leave at this fire  Petro in Ive heard you said, why turn that it all were way my time mying you change me And Iaba\n",
            "--------------------------------------------------\n",
            "Epoch 7/30, Step 140: Train Loss=3.0877, Val Loss=5.6447, LR=9.89e-06, Tokens=286,720\n",
            "New best finetuned model saved! Val Loss: 5.6447\n",
            "Epoch 7/30, Step 160: Train Loss=2.5704, Val Loss=5.5422, LR=9.75e-06, Tokens=327,680\n",
            "New best finetuned model saved! Val Loss: 5.5422\n",
            "Epoch 7 completed. Average loss: 2.9247\n",
            "Epoch 8/30, Step 180: Train Loss=2.5928, Val Loss=5.4764, LR=9.55e-06, Tokens=368,640\n",
            "New best finetuned model saved! Val Loss: 5.4764\n",
            "Epoch 8 completed. Average loss: 2.8042\n",
            "Epoch 9/30, Step 200: Train Loss=2.5877, Val Loss=5.4348, LR=9.31e-06, Tokens=409,600\n",
            "New best finetuned model saved! Val Loss: 5.4348\n",
            "Epoch 9 completed. Average loss: 2.6896\n",
            "Sample generation after epoch 9:\n",
            "Yellow lights say We dont not say,P secret And in dont need done And it was all around  accessing a say we could get a beautiful world, me  I do I could see up at the world, cries  rain  fall from St police, wasteland And the beam of the water, innocence standing in When they saying  risk All its no movement All\n",
            "--------------------------------------------------\n",
            "Epoch 10/30, Step 220: Train Loss=2.2764, Val Loss=5.3655, LR=9.01e-06, Tokens=450,560\n",
            "New best finetuned model saved! Val Loss: 5.3655\n",
            "Epoch 10 completed. Average loss: 2.5924\n",
            "Epoch 11/30, Step 240: Train Loss=2.2212, Val Loss=5.3297, LR=8.67e-06, Tokens=491,520\n",
            "New best finetuned model saved! Val Loss: 5.3297\n",
            "Epoch 11 completed. Average loss: 2.5040\n",
            "Epoch 12/30, Step 260: Train Loss=2.3499, Val Loss=5.3199, LR=8.29e-06, Tokens=532,480\n",
            "New best finetuned model saved! Val Loss: 5.3199\n",
            "Epoch 12 completed. Average loss: 2.4230\n",
            "Sample generation after epoch 12:\n",
            "Yellow lights say We dont need not say Ah-la,-la, concrete of birds Now theyre dont need said So come Hold in your tears come,Sir from Yellow We know when we go Ive got a crowd  yeah, what in aad learn So I long Oh no on aKnow you learn a concrete ofAll that youve been day \n",
            "--------------------------------------------------\n",
            "Epoch 13/30, Step 280: Train Loss=2.1144, Val Loss=5.2769, LR=7.87e-06, Tokens=573,440\n",
            "New best finetuned model saved! Val Loss: 5.2769\n",
            "Epoch 13 completed. Average loss: 2.3579\n",
            "Epoch 14/30, Step 300: Train Loss=2.2251, Val Loss=5.2634, LR=7.42e-06, Tokens=614,400\n",
            "New best finetuned model saved! Val Loss: 5.2634\n",
            "Epoch 14/30, Step 320: Train Loss=1.9650, Val Loss=5.2425, LR=6.94e-06, Tokens=655,360\n",
            "New best finetuned model saved! Val Loss: 5.2425\n",
            "Epoch 14 completed. Average loss: 2.3041\n",
            "Epoch 15/30, Step 340: Train Loss=2.1494, Val Loss=5.2293, LR=6.44e-06, Tokens=696,320\n",
            "New best finetuned model saved! Val Loss: 5.2293\n",
            "Epoch 15 completed. Average loss: 2.2416\n",
            "Sample generation after epoch 15:\n",
            "Yellow lights upon for the way, when you work to work out from the world cl he was mould.  photographs had moving up the married, I always wanted to be  do Where solo, when I I was all nobodyThat you should you I wouldnt see someone who would again, though you shiver  cantitives,  Youre all Queen. You full of his head Ill\n",
            "--------------------------------------------------\n",
            "Epoch 16/30, Step 360: Train Loss=2.0384, Val Loss=5.2156, LR=5.93e-06, Tokens=737,280\n",
            "New best finetuned model saved! Val Loss: 5.2156\n",
            "Epoch 16 completed. Average loss: 2.2016\n",
            "Epoch 17/30, Step 380: Train Loss=1.9086, Val Loss=5.2046, LR=5.40e-06, Tokens=778,240\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2591220852.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;31m# Start finetuning with improved training function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m finetune_results = finetune_model_advanced(\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mfinetune_train_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2591220852.py\u001b[0m in \u001b[0;36mfinetune_model_advanced\u001b[0;34m(model, train_loader, val_loader, device, settings, tokenizer, context_length, start_context)\u001b[0m\n\u001b[1;32m    126\u001b[0m                     \u001b[0mpatience_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                     \u001b[0;31m# Save best finetuned model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                     torch.save({\n\u001b[0m\u001b[1;32m    129\u001b[0m                         \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                         \u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text using the trained model and the generate_text function\n",
        "start_context = \"I want something\"\n",
        "num_chars_to_generate = 500 # You can adjust this number\n",
        "generated_tokens = generate(\n",
        "    model=model,\n",
        "    max_new_tokens=num_chars_to_generate,\n",
        "    context=text_to_token_ids(start_context, tokenizer, device),\n",
        "    context_length=context_length,\n",
        "    temperature=0.8, # Adjust temperature for creativity\n",
        "    top_k=None # Or specify a top_k value\n",
        ")\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "decoded_text = token_ids_to_text(generated_tokens, tokenizer)\n",
        "\n",
        "print(decoded_text.replace(\"\\n\", \" \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkvCQYFOCEDs",
        "outputId": "3d4d30f5-0faf-470c-b928-95dd0b50a960"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "... Oh, I was so hard Oh-ooh-oh-ooh, dont said, I remember But it waiting, I come at the sky, always have short I said,  Oh, oh-oh-ooh-ooh-ooh-ooh I said It the suna full of blood Now the sky full of blood Can the sky full of blood  hypoh-ooh-ooh-ooh-ooh-ooh, I curiosity it for theDaddy, youre such a Grail of blood One I said, cause the start, cause in the ground Oh, cause in the Kenya the sky full of purple  sound of escape theoh-ooh-ooh-ooh-ooh I fall I think the lights, I saw the sky, there bad the same view I know the ground I think  Nobodycrow the chaos all the ground I know the sky puppet on my heart flow I cry, I know theOne alight, youre the best of the... The deepestable it gets what the good is all theAnother But my heart The fear How all the streets I can stay until the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def debug_model_and_data(model, train_dataloader, tokenizer, device):\n",
        "    \"\"\"Comprehensive debugging of model and data\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"DEBUGGING MODEL AND DATA\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 1. Check data\n",
        "    print(\"\\n1. DATA ANALYSIS:\")\n",
        "    for batch_idx, (input_batch, target_batch) in enumerate(train_dataloader):\n",
        "        if batch_idx == 0:  # Check first batch\n",
        "            print(f\"Input shape: {input_batch.shape}\")\n",
        "            print(f\"Target shape: {target_batch.shape}\")\n",
        "            print(f\"Input sample: {input_batch[0][:20]}\")  # First 20 tokens\n",
        "            print(f\"Target sample: {target_batch[0][:20]}\")\n",
        "\n",
        "            # Decode to check if data makes sense\n",
        "            sample_text = tokenizer.decode(input_batch[0][:50].tolist())\n",
        "            print(f\"Decoded sample: '{sample_text}'\")\n",
        "\n",
        "            # Check for data issues\n",
        "            print(f\"Input min/max: {input_batch.min()}/{input_batch.max()}\")\n",
        "            print(f\"Vocab size: {tokenizer.n_vocab}\")\n",
        "            if input_batch.max() >= tokenizer.n_vocab:\n",
        "                print(\"❌ ERROR: Token IDs exceed vocab size!\")\n",
        "                return False\n",
        "            break\n",
        "\n",
        "    # 2. Check model architecture\n",
        "    print(\"\\n2. MODEL ARCHITECTURE:\")\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # 3. Check model forward pass\n",
        "    print(\"\\n3. FORWARD PASS CHECK:\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sample_input = input_batch[:1].to(device)  # Single sample\n",
        "        try:\n",
        "            output = model(sample_input)\n",
        "            print(f\"Model output shape: {output.shape}\")\n",
        "            print(f\"Output range: {output.min():.4f} to {output.max():.4f}\")\n",
        "\n",
        "            # Check for NaN/Inf\n",
        "            if torch.isnan(output).any():\n",
        "                print(\"❌ ERROR: Model output contains NaN!\")\n",
        "                return False\n",
        "            if torch.isinf(output).any():\n",
        "                print(\"❌ ERROR: Model output contains Inf!\")\n",
        "                return False\n",
        "            print(\"✅ Forward pass successful\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ ERROR in forward pass: {e}\")\n",
        "            return False\n",
        "\n",
        "    # 4. Check loss calculation\n",
        "    print(\"\\n4. LOSS CALCULATION:\")\n",
        "    model.train()\n",
        "    sample_input, sample_target = input_batch[:1].to(device), target_batch[:1].to(device)\n",
        "\n",
        "    # Manual loss calculation\n",
        "    logits = model(sample_input)\n",
        "    loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), sample_target.view(-1))\n",
        "    print(f\"Sample loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Check what random guessing would give\n",
        "    random_loss = np.log(tokenizer.n_vocab)\n",
        "    print(f\"Random guessing loss: {random_loss:.4f}\")\n",
        "\n",
        "    if loss.item() > random_loss + 1:\n",
        "        print(\"⚠️  WARNING: Loss much higher than random guessing!\")\n",
        "        print(\"This suggests the model isn't learning properly\")\n",
        "\n",
        "    # 5. Check gradients\n",
        "    print(\"\\n5. GRADIENT CHECK:\")\n",
        "    loss.backward()\n",
        "\n",
        "    grad_norms = []\n",
        "    zero_grads = 0\n",
        "    total_grads = 0\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None:\n",
        "            grad_norm = param.grad.norm().item()\n",
        "            grad_norms.append(grad_norm)\n",
        "            if grad_norm == 0:\n",
        "                zero_grads += 1\n",
        "            total_grads += 1\n",
        "\n",
        "    if grad_norms:\n",
        "        print(f\"Average gradient norm: {np.mean(grad_norms):.6f}\")\n",
        "        print(f\"Max gradient norm: {np.max(grad_norms):.6f}\")\n",
        "        print(f\"Min gradient norm: {np.min(grad_norms):.6f}\")\n",
        "        print(f\"Zero gradients: {zero_grads}/{total_grads}\")\n",
        "\n",
        "        if np.mean(grad_norms) < 1e-6:\n",
        "            print(\"❌ ERROR: Gradients too small (vanishing gradient problem)\")\n",
        "            return False\n",
        "        elif np.max(grad_norms) > 100:\n",
        "            print(\"❌ ERROR: Gradients too large (exploding gradient problem)\")\n",
        "            return False\n",
        "\n",
        "    print(\"✅ All checks passed!\")\n",
        "    return True"
      ],
      "metadata": {
        "id": "0vWFs8ZIWyS9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debug_success = debug_model_and_data(model, train_dataloader, tokenizer, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Kd5V0QlW2pD",
        "outputId": "eea505ca-c23b-48cc-80aa-55dcb4da696d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "DEBUGGING MODEL AND DATA\n",
            "============================================================\n",
            "\n",
            "1. DATA ANALYSIS:\n",
            "Input shape: torch.Size([2, 64])\n",
            "Target shape: torch.Size([2, 64])\n",
            "Input sample: tensor([ 2612,  4369,    11,   523,   326,   612,   550,   587,   645, 15223,\n",
            "          670,   286,  8166,   438, 14363,  1986,   373,  1598,   290, 36519])\n",
            "Target sample: tensor([ 4369,    11,   523,   326,   612,   550,   587,   645, 15223,   670,\n",
            "          286,  8166,   438, 14363,  1986,   373,  1598,   290, 36519,    13])\n",
            "Decoded sample: ' heart disease, so that there had been no preliminary work of destruction--his face was clear and untouched. I had met him once or twice, years before, and thought him insignificant and dingy. Now I saw that he was superb.\n",
            "\n",
            "'\n",
            "Input min/max: 1/44852\n",
            "Vocab size: 50257\n",
            "\n",
            "2. MODEL ARCHITECTURE:\n",
            "Total parameters: 74,652,753\n",
            "Trainable parameters: 74,652,753\n",
            "\n",
            "3. FORWARD PASS CHECK:\n",
            "Model output shape: torch.Size([1, 64, 50257])\n",
            "Output range: -1.9403 to 1.9291\n",
            "✅ Forward pass successful\n",
            "\n",
            "4. LOSS CALCULATION:\n",
            "Sample loss: 10.7634\n",
            "Random guessing loss: 10.8249\n",
            "\n",
            "5. GRADIENT CHECK:\n",
            "Average gradient norm: 0.718207\n",
            "Max gradient norm: 5.265074\n",
            "Min gradient norm: 0.001960\n",
            "Zero gradients: 0/262\n",
            "✅ All checks passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKQZJxl-fzYX"
      },
      "source": [
        "## Character Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQX_I2QjfIfl"
      },
      "outputs": [],
      "source": [
        "int_to_char = {}\n",
        "for i, char in enumerate(characters):\n",
        "    int_to_char[i] = char\n",
        "\n",
        "char_to_int = {}\n",
        "\n",
        "for value, char  in int_to_char.items():\n",
        "    char_to_int[char] = value\n",
        "\n",
        "print(int_to_char,char_to_int,end=\"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFfWrHPBgeJf"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "encoded_train = [char_to_int[c] for c in joined_train_lyrics if c in char_to_int]\n",
        "\n",
        "encoded_test = [char_to_int[c] for c in joined_test_lyrics if c in char_to_int]\n",
        "\n",
        "print(len(encoded_train),len(encoded_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUeuvxJSdT_E"
      },
      "outputs": [],
      "source": [
        "\n",
        "# context_length=3\n",
        "# encoded= [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]\n",
        "\n",
        "def create_dataset(encoded, context_length=128):\n",
        "  inputs, targets = [], []\n",
        "  for i in range(len(encoded) - context_length):\n",
        "    inputs.append(encoded[i:i+context_length])\n",
        "    targets.append(encoded[i+1:i+context_length+1])\n",
        "  return torch.tensor(inputs), torch.tensor(targets)\n",
        "\n",
        "train_inputs, train_targets = create_dataset(encoded_train)\n",
        "test_inputs, test_targets = create_dataset(encoded_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7bAmKGNliIP"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "def train(model):\n",
        "  vocab_size = len(char_to_int)\n",
        "  context_length = 128\n",
        "  embed_dim = 128\n",
        "  attention_dim = 128\n",
        "  num_heads = 4\n",
        "  num_blocks = 6\n",
        "  epochs = 40\n",
        "  lr = 0.001\n",
        "  patience = 3\n",
        "  batch_size = 32\n",
        "\n",
        "\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
        "  best_loss = float('inf')\n",
        "  patience_counter = 0\n",
        "  losses, accuracies = [],[]\n",
        "\n",
        "  train_loader = DataLoader(TensorDataset(train_inputs, train_targets), batch_size=batch_size, shuffle=True)\n",
        "  test_loader = DataLoader(TensorDataset(test_inputs, test_targets), batch_size=batch_size)\n",
        "\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for X, y in train_loader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      pred = model(X)\n",
        "      pred_flat = pred.view(-1, vocab_size)\n",
        "      y_flat = y.view(-1)\n",
        "      loss = loss_func(pred_flat, y_flat)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "      predictions = pred_flat.argmax(dim=1)\n",
        "      correct += (predictions == y_flat).sum().item()\n",
        "      total += y_flat.size(0)\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = correct / total\n",
        "    losses.append(avg_loss)\n",
        "    accuracies.append(accuracy)\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    with torch.no_grad():\n",
        "      for X, y in test_loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        pred_flat = pred.view(-1, vocab_size)\n",
        "        y_flat = y.view(-1)\n",
        "        loss = loss_func(pred_flat, y_flat)\n",
        "        eval_loss += loss.item()\n",
        "    eval_loss /= len(test_loader)\n",
        "    print(f\"Epoch {epoch+1}: Train Loss={avg_loss:.4f}, Accuracy={accuracy:.4f}, Eval Loss={eval_loss:.4f}\")\n",
        "    if avg_loss < best_loss:\n",
        "      best_loss = avg_loss\n",
        "      patience_counter = 0\n",
        "      best_model_state = model.state_dict()\n",
        "    else:\n",
        "      patience_counter += 1\n",
        "      if patience_counter >= patience:\n",
        "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "        model.load_state_dict(best_model_state)\n",
        "        break\n",
        "  return model, epoch+1, losses, accuracies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eis7CPSEpjVI"
      },
      "outputs": [],
      "source": [
        "vocab_size = 83\n",
        "context_length = 128\n",
        "embed_dim = 128\n",
        "attention_dim = 128\n",
        "num_heads = 4\n",
        "num_blocks = 6\n",
        "\n",
        "model = GPT(num_heads, vocab_size, embed_dim, attention_dim, num_blocks, context_length).to(device)\n",
        "#model.load_state_dict(torch.load(\"/content/trained_model-128-85.pth\", weights_only=False))\n",
        "\n",
        "model,epochs,losses,accuracies = train(model)\n",
        "# Save the trained model\n",
        "model_path = \"trained_model.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "files.download('trained_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross Entropy\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "lQqCVUrjawhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(range(1, epochs+1), losses, marker='o')\n",
        "plt.title('Epoch vs Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(range(1, epochs+1), accuracies, marker='o', color='green')\n",
        "plt.title('Epoch vs Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aABbJvxFiJ3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l_xaF0I6n68l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e1d7c7d"
      },
      "source": [
        "# Training parameters\n",
        "num_epochs = 10\n",
        "eval_freq = 50  # Evaluate every 50 batches\n",
        "eval_iter = 5   # Evaluate on 5 batches for training and validation loss calculation\n",
        "learning_rate = 0.001\n",
        "start_context = \"The\" # Starting text for generation\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Start training\n",
        "train_losses, val_losses = train_model(\n",
        "    model, train_dataloader, test_dataloader, optimizer, device,\n",
        "    num_epochs, eval_freq, eval_iter, start_context, tokenizer, context_length\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}